[{"body":"","ref":"https://arcogroup.bitbucket.io/","summary":"","title":"Arco Research \u0026 Documentation"},{"body":"Introducción Phyxio es una aplicación distribuida formada por dos componentes clave: el servicio web y los módulos del runner. Estos últimos se encargan de utilizar el hardware de la máquina (GPU, TPU, cámaras, etc) para ofrecer servicios de reconocimiento de un ejercicio en concreto.\nEste manual explica el proceso de desarrollo de un módulo de Phyxio.\nRequisitos Para poder seguir con este manual, se asume que los siguientes paquetes Debian están correctamente instalados:\nphyxio-module-runner: se encarga de lanzar/parar los módulos que Phyxio necesite para la realización de cierto ejercicio. Será necesario para probar el módulo que estemos desarrollando.\nphyxio-module-creator: herramienta de gestión para la creación de módulos Phyxio. Si estás leyendo esta documentación, es posible que ya tengas ese paquete instalado.\nArquitectura de los módulos Un módulo es básicamente una aplicación que corre dentro del hardware del Totem, y que se comunica con Phyxio por medio de una o varias interfaces Ice. Puede ser un programa binario escrito en C++, C#, un script en Python, o cualquier otro lenguaje para el que existan bindings de ZeroC Ice. La herramienta phyxio-module-creator y, por ende, este manual asumen que el módulo será escrito en Python.\nUn módulo se compone básicamente de las siguientes partes:\nBinario: este es el programa principal que se ejecutará cuando Phyxio lo indique. Se encarga de procesar el vídeo, de la lectura del hardware, las comunicaciones con el runner y con phyx.io, etc.\nInterfaz ECI: forma la interfaz web que se mostrará durante la realización del ejercicio. La ECI ( Embedded Controller Interface) está formada por un fichero HTML, que cargará una guía de estilos en CSS y una aplicación en Javascript.\nLa herramienta phyxio-module-creator genera la jerarquía y el código mínimo necesario para un módulo básico. A partir de ahí, el desarrollador podrá aportar la funcionalidad que necesite.\nCreación de un módulo Nota Este manual explicará el proceso de creación de un módulo que analizará la posición de las manos del usuario y detectará cuando coloca una de ellas en un punto determinado de la pantalla. Se empleará OpenCV y/o mediapipe, aunque los detalles técnicos del proceso de detección no se discutirán aquí. Para crear un módulo de Phyxio, llamado move-to-point, ejecuta el siguiente comando en una terminal:\nconsole phyxio-mod-admin -c move-to-point El anterior comando generará un directorio, del mismo nombre que el módulo, con la estructura básica necesaria. Algo similar a lo siguiente:\nconsole $ tree move-to-point ├── Makefile ├── module.json └── src ├── bin │ └── move-to-point.py ├── eci │ ├── calibration.html │ ├── css │ │ └── style.css │ ├── index.html │ └── js │ └── app.js │ └── calibration.js ├── module.conf └── move-to-point.ice La función de cada uno de esos archivos se detalla a continuación:\nEl fichero Makefile contiene las reglas necesarias para algunas tareas rutinarias. En particular, permite compilar las interfaces Slice a Javascript, para ser usadas dentro de la aplicación JS. También tiene algunas otras reglas específicas para la creación de paquetes Debian y de artefactos de mender, pero no se usarán en este manual. No es necesario editar este fichero.\nmodule.json sirve para especificar la configuración básica del módulo. Se indica el nombre del módulo, name; su nombre en clave, o codename, cuyo valor se empleará al registrar el módulo dentro de Phyxio; la ruta relativa (module_dir) donde se encuentran el binario y la interfaz ECI; el nombre del binario (exec); y una serie de parámetros de configuración para la ECI (en especial, el proxy que deberá usar para comunicarse con el binario). Los valores por defecto suelen ser adecuados, y no es necesario cambiar nada (al menos de momento).\nsrc/bin/move-to-point.py es el binario del módulo, que será lo que se ejecute cuando Phyxio lo indique. Este fichero contiene el código mínimo necesario para un módulo Python, pero es posible reescribirlo por completo, usando cualquier otro lenguaje compatible. Más adelante comentaremos su contenido paso a paso.\nEl directorio src/eci contiene la interfaz web que se empotrará dentro de phyx.io cuando se esté realizando un ejercicio que use este módulo, o cuando se realize el proceso de calibración. También hablaremos después de su contenido más detalladamente.\nsrc/module.conf especifica la configuración de ZeroC Ice que usará el binario para crear su Communicator y el adaptador de objectos. Los valores especificados por defecto son suficiente, por lo que tampoco es necesario cambiar nada (salvo requisitos muy concretos).\nsrc/move-to-point.ice define la interfaz ZeroC Ice que se empleará en la comunicación entre el binario y la interfaz ECI (aplicación web). Por defecto, la interfaz hereda de ECIClient, pero está pensada para ser extendida con nuevos métodos, según sea necesario. Comentaremos su uso más adelante.\nInstalación Para que el módulo recién creado pueda usarse, debe instalarse en el sistema. La aplicación phyxio-mod-admin también se encarga de ello. Basta con indicarle el nombre del módulo (desde el directorio en dónde esté), con la opción -i. Esto copiará el módulo completo a su ruta correspondiente. A veces, sin embargo, y sobre todo durante el desarrollo, es conveniente tener el módulo instalado (para probarlo) y a su vez, poder seguir realizando modificaciones en él. A tal efecto existe la opción -s, que le indica a phyxio-mod-admin que debe realizar un enlace simbólico (en vez de una copia completa).\n¡Atención! Si usas la opción -s, asegúrate de que otros usuarios pueden acceder a la carpeta donde tienes el módulo (y a todos sus ancestros). De otra forma, el servicio no podrá cargar el módulo correctamente. Además, dado que se crea un enlace simbólico a la ruta local, recuerda que no debes cambiarle el nombre a esa carpeta, o moverla de sitio, pues eso romperá el enlace simbólico, y dejará de funcionar. Por tanto, si queremos instalar nuestro módulo, y seguir trabajando con él, ejecutaremos el siguiente comando:\n¡Importante! Es necesario ejecutarlo con sudo, pues la aplicación modificará rutas del sistema. console sudo phyxio-mod-admin -i -s move-to-point Si deseas desinstalar, usa la opción -u de la misma forma:\nconsole sudo phyxio-mod-admin -u move-to-point Al instalar un nuevo módulo (o desinstalarlo), es necesario reiniciar el servicio que lo usa (para que actualice sus registros). Así pues, ejecuta el siguiente comando después de la (des)instalación:\nconsole sudo systemctl restart phyxio-module-runner.service Y, para comprobar que la instalación (y la carga) se han realizado satisfactoriamente, puedes ver el log del servicio con el siguiente comando:\nconsole journalctl -f -u phyxio-module-runner.service ... INFO:ModuleRunner: Loading module /usr/share/phyxio-module-runner/modules/move-to-point/module.json INFO:ModuleRunner: - module named \u0026#39;move-to-point\u0026#39; correctly loaded! ... Esquema general de funcionamiento Cuando el usuario visita la web de phyx.io, en la vista de totem, entra en una rutina y pulsa sobre Comenzar, se abre una página de carga para el primer ejercicio de la rutina. Esta página corre una aplicación en JavaScript que contacta con un servicio llamado module-runner (usando ZeroC Ice), y le indica que ejecute el binario del módulo específico para ese ejercicio.\nEl module-runner, al iniciar, registra todos los módulos instalados (leyendo el fichero module.json de cada uno). Cuando la página de carga le indica el nombre del módulo (usando su codename), este lo busca en su registro, obtiene la ruta al binario, y lo ejecuta. Además, lanza un servidor web con la interfaz ECI del módulo, y notifica a la página de carga que está disponible.\nLa página de carga (también llamada cliente), al recibir la notificación del runner, carga la web ECI del módulo y se queda esperando más eventos desde el runner. A partir de este momento, el control lo toma el módulo del ejercicio, tanto la aplicación que corre en la ECI, como el binario que ha ejecutado el runner.\nFlujo de ejecución del binario Nota Si bien es cierto que llamamos binario a la aplicación principal del módulo, no tiene porqué ser un binario como tal (algo compilado). De hecho, la mayoría de las veces, hablamos de un script en Python. No obstante, seguiremos usando esta notación en aras de la simplicidad. El binario del módulo que se ha generado (el fichero src/bin/move-to-point.py) consta de una aplicación Ice (Ice.Application), con su communicator correspondiente, y un adaptador de objetos:\nsnippet.python class MoveToPoint(Ice.Application): def run(self, args): if not self._check_args(args): return -1 # create communicator and adapter ic = self.communicator() adapter = ic.createObjectAdapter(\u0026#34;MoveToPoint.Adapter\u0026#34;) adapter.activate() Además, se crea un proxy a un objeto que reside en el runner y que sirve para notificarle cambios en el estado del módulo:\nsnippet.python # get proxy to runner service runner = Phyxio.ModuleHandlerPrx.checkedCast( ic.stringToProxy(self.args.runner_prx)) En concreto, se usará el método ready() para indicar al runner que el módulo ha arrancado correctamente y está activo, y finish() para notificar la finalización del ejercicio, y mandar un reporte de resultados.\nPor otro lado, también se crea un sirviente que será registrado en el adaptador de objetos, y estará a disposición de la ECI. La interfaz de ese sirviente es Phyxio.ECIObserver, y tiene como objetivo permitir la comunicación desde la ECI con el módulo. En particular, la interfaz enviará al módulo el proxy de otro observador, que se empleará en la comunicación entre el módulo y la ECI. De esta forma, se establece un canal de comunicación bidireccional entre ambas partes. La interfaz de ese observador está definida en el fichero slice que se ha generado (en este caso, move-to-point.ice) y se puede modificar según sea necesario.\nEn la librería de componentes, hay una implementación genérica del ECIObserver, llamada PhyxioECIObserverI, que es la que se usa en el código generado por phyxio-mod-admin:\nsnippet.python # add observer servant, to receive client proxy servant = PhyxioECIObserverI(self.args, runner) prx = adapter.add(servant, Ice.stringToIdentity(\u0026#34;MoveToPoint\u0026#34;)) log.info(f\u0026#34; Module proxy: {prx}\u0026#34;) Por último, la aplicación notifica su estado y ejecuta el bucle de eventos del communicator. De ser necesario, es posible reemplazar este por otro bucle de eventos (lo veremos más tarde). El código relacionado es el siguiente:\nsnippet.python # notify runner that we are ready, and wait events runner.readyAsync() self.callbackOnInterrupt() ic.waitForShutdown() ic.destroy() Todo el código discutido hasta ahora ha sido generado por la herramienta phyxio-mod-admin, y no es necesario realizar modificaciones en él (salvo que se indique lo contrario). También existen algunos puntos, marcados con sendos comentarios, donde es probable que necesites realizar cambios. Según progresemos en el ejemplo, los iremos viendo.\nArgumentos del ejercicio La clase principal del binario que estamos analizando contiene un método llamado _check_args(), que se encarga de parsear los argumentos de la línea de órdenes. Estos argumentos (que los aporta el runner) se obtienen de dos sitios diferentes: 1) la configuración interna del runner (parámetros comunes a todos los módulos) y 2) las opciones de configuración del ejercicio (determinada por el trainer al crearlo).\nUtilizando la función get_global_parser() de la librería tools, se crea un parser (en concreto, un ArgumentParser) que ya tiene configuradas las opciones comunes. En el método _check_args() puedes añadir cualquier opción particular que tu módulo necesite. A modo de ejemplo, vamos a hacer que move-to-point acepte un argumento opcional para indicar si limitamos el punto dónde el usuario debe mover la mano a una parte del espacio: derecha, izquierda, arriba o abajo. Para ello, añadimos un argumento al parser, --limit-area, de la siguiente forma:\nsnippet.python def _check_args(self, args): parser = get_global_parser() parser.add_argument(\u0026#34;--limit-area\u0026#34;, choices=[\u0026#34;left\u0026#34;, \u0026#34;right\u0026#34;, \u0026#34;top\u0026#34;, \u0026#34;bottom\u0026#34;], help=\u0026#34;limit area to specified section\u0026#34;) A partir de ese momento, la aplicación podrá acceder al valor de ese parámetro opcional desde args.limit_area.\nTracker de manos El módulo que genera phyxio-mod-admin por defecto está orientado al análisis de vídeo mediante técnicas de visión por computador. Esto implica que está pensado para ser usado con un componente de vídeo que procese los frames de entrada, realice las operaciones pertinentes, y escriba los frames resultantes a un sumidero de vídeo. Y esa es precisamente la funcionalidad que aporta la clase VideoComponent de la librería de componentes.\nVideoComponent se encarga de inicializar los dispositivos de vídeo, tanto de entrada como de salida. Además, implementa un bucle de eventos específico, que realiza las siguientes acciones:\nCaptura un frame del dispositivo de vídeo de entrada. Lo invierte horizontalmente (para obtener una imagen especular). Llama al método on_frame(), pasando el frame como argumento. Escribe en el dispositivo de vídeo de salida el frame retornado por on_frame(). Nota VideoComponent también implementa otros métodos de soporte para notificar cambios a la ECI, detener el bucle de eventos, etc. Los iremos viendo según sea necesario. Para usar el VideoComponent, es necesario definir un nuevo componente que herede de él. Por ejemplo, para el módulo move-to-point, definiremos la clase HandPositionDetector, en cuyo constructor crearemos los objetos necesarios para realizar el tracking de las manos (usando MediaPipe Hands):\nsnippet.python import mediapipe as mp mp_draw = mp.solutions.drawing_utils mp_draw_styles = mp.solutions.drawing_styles mp_hands = mp.solutions.hands from libs.components import VideoComponent class HandPositionDetector(VideoComponent): def __init__(self, args, servant): super().__init__(args, observer=servant) self._target_pos = TargetPositionIterator(args) self._hands = mp_hands.Hands( max_num_hands=1, model_complexity=0, min_detection_confidence=0.5, min_tracking_confidence=0.5) Como parámetros del constructor, tenemos los argumentos que procesó el parser de la línea de órdenes (args), y el sirviente que creamos en la aplicación principal (la instancia de PhyxioObserver, servant). El constructor de la clase VideoComponent acepta un parámetro opcional más llamado has_video_out que indica si deseamos proporcionar feedback de vídeo al usuario o no. Por defecto es True, pero si le pasamos False, no se creará el dispositivo de vídeo de salida (lo cual podría ser útil para módulos que procesan el vídeo pero no necesitan mostrarlo al usuario).\nEl método más importante que debemos sobreescribir de la clase VideoComponent es on_frame. Acepta un único parámetro, un frame de vídeo, y debe devolver el mismo frame (quizá modificado), o uno similar, con las mismas características (tamaño, bpp, etc.).\n¡Importante! Este método lo invocará el bucle de eventos con cada frame del dispositivo de entrada de vídeo que se capture, por lo que el procesamiento debe ser el mínimo imprescindible, para evitar latencia o frame dropping. En el ejemplo que estamos desarrollando, este método realizará lo siguiente:\nDibujar un círculo en la posición objetivo Hacer el tracking de una mano y obtener su centro geométrico Comparar ambos (posición objetivo y centro geométrico), y si hay superposición, actualizar el objetivo a una nueva posición, e incrementar (y notificar a la ECI) el número de iteraciones realizadas. Su implementación sería la siguiente:\nsnippet.python def on_frame(self, image): # draw target position self._target_pos.draw(image) # detect hands, and exit if no hand detected # NOTE: to improve performance, mark the image as not writeable (pass by reference) image.flags.writeable = False image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) results = self._hands.process(image) image.flags.writeable = True image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) if results.multi_hand_landmarks is None: return image # get hand center, and draw \u0026#34;a ball\u0026#34; hand_center = self._get_hand_center(image, results.multi_hand_landmarks[0]) cv2.circle(image, hand_center, 32, ORANGE, cv2.FILLED, cv2.LINE_AA) # check if center is inside target if self._target_pos.match(hand_center): self._target_pos.next() self.iter_add() # draw the hand annotations on the image (only for debug) if self._is_debug: self._draw_hand_skel(image, results) return image También he definido en la misma clase un par de métodos auxiliares, que se emplean en on_frame. Uno es el encargado de obtener el centro geométrico de la mano:\nsnippet.python def _get_hand_center(self, image, landmarks): # https://google.github.io/mediapipe/solutions/hands.html#hand-landmark-model x = (landmarks.landmark[5].x \u0026#43; landmarks.landmark[17].x \u0026#43; landmarks.landmark[0].x) / 3 y = (landmarks.landmark[5].y \u0026#43; landmarks.landmark[17].y \u0026#43; landmarks.landmark[0].y) / 3 # landmark positions are normalized, return global positions return int(x * image.shape[1]), int(y * image.shape[0]) Y el otro se encarga de dibujar los landmarks que el módulo Hands ha detectado (es decir, el \u0026ldquo;esqueleto\u0026rdquo; de la mano). Esto se usa sólo a modo de depuración, y no se llamará a menos que, al definir el ejercicio en phyx.io se incluya el flag --debug-info:\nsnippet.python def _draw_hand_skel(self, image, results): for hand_landmarks in results.multi_hand_landmarks: mp_draw.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS, mp_draw_styles.get_default_hand_landmarks_style(), mp_draw_styles.get_default_hand_connections_style()) En la clase anterior, también he instanciado un objeto encargado de generar al azar la posición objetivo a la que el usuario tendrá que mover la mano (el TargetPositionIterator). Por razones de completitud, he aquí su código:\nsnippet.python import random import cv2 ORANGE = (0, 95, 253) # BGR GREEN = (50, 194, 134) # BGR GRAY = (100, 100, 100) # BGR WHITE = (255, 255, 255) # BGR class TargetPositionIterator: def __init__(self, args, radius=64): self._w = args.video_width self._h = args.video_height self._r = radius self._limit = args.limit_area self._current = (0, 0) self.next() def draw(self, image): x, y = self._current cv2.circle(image, (x\u0026#43;1, y\u0026#43;1), self._r, GRAY, 3, cv2.LINE_AA) cv2.circle(image, (x, y), self._r, WHITE, 3, cv2.LINE_AA) def match(self, position): dx = abs(position[0] - self._current[0]) dy = abs(position[1] - self._current[1]) return dx \u0026lt; self._r and dy \u0026lt; self._r def next(self): # (0,0) is at top-left corner x1, y1 = self._r, self._r x2, y2 = self._w - self._r, self._h - self._r if self._limit == \u0026#34;top\u0026#34;: y2 = self._h / 2 elif self._limit == \u0026#34;bottom\u0026#34;: y1 = self._h / 2 elif self._limit == \u0026#34;left\u0026#34;: x2 = self._w / 2 elif self._limit == \u0026#34;right\u0026#34;: x1 = self._w / 2 x = random.randrange(x1, x2) y = random.randrange(y1, y2) self._current = (x, y) Modificación del bucle de eventos Hasta ahora, nuestro módulo utilizaba el bucle de eventos de Ice para mantenerse activo. Sin embargo, al implementar un VideoComponent, es necesario reemplazarlo por el que trae el componente de vídeo. La modificación se debe hacer en el método run de la aplicación principal (en el caso que nos ocupa, la clase MoveToPoint que hereda de Ice.Application). Los pasos para realizar esta modificación son:\nCrear una instancia del VideoComponent: snippet.python self._detector = HandPositionDetector(self.args, servant) Reemplazar el bucle de eventos de Ice por el suyo: snippet.python # ic.waitForShutdown() self._detector.run() Detener el detector cuando se produce una interrupción. Básicamente es llamar al método stop cuando el runner solicita que nuestro módulo debe detenerse (por ejemplo, porque el usuario ha pulsado sobre el botón stop). Este código debe ir en el método interruptCallback: snippet.python def interruptCallback(self, sig): self._detector.stop() Interfaz de usuario (ECI) La interfaz empotrada de control (Embedded Control Interface) es la parte que phyx.io cargará en el cliente web, y supone la parte visual y de interacción con el usuario del módulo. Consta inicialmente de un fichero en HTML (src/eci/index.html) con la plantilla que necesitamos para funcionar, y una aplicación JavaScript (src/eci/js/app.js) que implementa la interfaz Ice que hayamos definido, y la inicialización mínima necesaria.\nLa interfaz que genera la herramienta phyxio-mod-admin está compuesta de:\nFeedback de vídeo empotrado a pantalla completa. Este es el vídeo de salida que genera el módulo. Contador de tiempo e iteraciones (realizadas y objetivo). Vídeo del modelo de prueba del ejercicio, para indicar al usuario qué debe hacer. Panel de información sobre la tarea a realizar. Si existe algún componente que no queramos mostrar, simplemente borramos el código HTML correspondiente. Por ejemplo, en move-to-point no tendremos vídeo de muestra del ejercicio, por ello, eliminamos las siguientes líneas de código del fichero HTML (no es necesario cambiar nada de la aplicación en JavaScript).\nsnippet.html \u0026lt;!-- ----------------------------------------------------------- video from trainer model sample --\u0026gt; \u0026lt;div id=\u0026#34;model-video\u0026#34;\u0026gt; \u0026lt;video id=\u0026#34;mv-elem\u0026#34; poster=\u0026#34;/shared/images/model-poster.png\u0026#34; autoplay loop muted\u0026gt;\u0026lt;/video\u0026gt; \u0026lt;/div\u0026gt; Compilando y probando Antes de poder probar el módulo recién creado, y siempre que realicemos modificaciones en la interfaz Ice, será necesario compilar el slice. Para ello, ejecuta el siguiente comando:\nconsole make compile-slices Esto generará un fichero .js con los skeletons/stubs para nuestro cliente JavaScript. Y dado que en un paso anterior ya hicimos la instalación del módulo, y reiniciamos el module-runner, no es necesario hacer nada más para poder usarlo. Simplemente, registra el módulo en phyx.io (usando su condename), y crea un ejercicio que lo use, añádelo a una rutina y asignala a un usuario de prueba. Accede a la página del totem, y ¡prueba tu módulo!\nNota Recuerda que necesitas permiso para acceder a la cámara. Por ello, si usas una ventana de incógnito o es la primera vez que lo usas, el navegador te mostrará un mensaje solicitando acceso. Por otro lado, también necsitarás registrar los certificados TLS que usa el runner, por lo que también habrás tenido que ejecutsar phyxio-install en tu usuario local. Soporte para calibración Algunos ejercicios están ligados a ciertos parámetros que son específicos de cada usuario que los realiza. Esta información se ha de obtener previamente, en un proceso de calibración. En esta etapa, el ejercicio se ejecuta de forma similar a como lo haría normalmente, pero la lógica se centra en obtener del usuario los parámetros pertinentes, notificándoselos a Phyxio para que los proporcione en próximas ejecuciones.\nNo todos los módulos requieren de calibración, y de hecho está desactivada por defecto. Para habilitarla, o bien al crear un ejercicio, o bien desde la vista de edición, se ha de marcar la casilla Calibración:\nUna vez marcada, nuestro módulo podrá recibir dos argumentos relacionados:\n--calibrate, que le indicará que debe arrancar el módulo en modo calibración. --calibration-data, que proporciona los datos de calibración almacenados para este usuario. No es necesario parsear esta opción, pues ya lo hace el componente de vídeo. En su lugar, tendremos una propiedad llamada _calibration con los parámetros correctos. Nota Los datos de calibración son específicos por cada para ejercicio / usuario, y por tanto, están vinculados a las opciones de configuración del ejercicio. No son únicos para cada módulo. Componente de vídeo para calibración Cuando el binario recibe el argumento --calibrate, debe ejecutarse en modo calibración, con el objetivo de obtener del usuario los parámetros requeridos. Queda bajo el criterio del desarrollador implementar los mecanismos necesarios para conseguirlo. Por ejemplo, podría simplemente mostrar una interfaz web (sin vídeo), con varios controles que el usuario debe ajustar (con el ratón, o el touchscreen). Una vez ajustados, al pulsar sobre un botón \u0026ldquo;guardar\u0026rdquo; se notifica a Phyxio que la calibración ha concluido.\nOtra forma de calibrar un ejercicio concreto (especialmente los que usan vídeo), es indicar al usuario que haga las mismas acciones que haría normalmente, pero en vez de esperar a que llegue a un umbral específico, lo que hace el módulo es registrar los rangos máximos a los que el usuario es capaz de llegar.\nUtilizaremos una aproximación mixta para añadir soporte de calibración al ejercicio del ejemplo move-to-point. El parámetro que vamos a calibrar es el radio del círculo que se pinta como objetivo. Y para ello, diseñamos un nuevo componente de vídeo, HandPositionCalibrator, que heredará de HandPositionDetector para reutilizar algunos métodos ya implementados.\nsnippet.python class HandPositionCalibrator(HandPositionDetector): def __init__(self, args, servant): super().__init__(args, servant) self._target_pos = TargetPositionCalibrator(args) # set initial values of calibration self._update_calibration() def on_frame(self, image): # draw target position self._target_pos.draw(image) # detect hands, and exit if no hand detected # NOTE: to improve performance, mark the image as not writeable (pass by reference) image.flags.writeable = False image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) results = self._hands.process(image) image.flags.writeable = True image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) if results.multi_hand_landmarks is None: return image # get hand center, and draw \u0026#34;a ball\u0026#34; hand_center = self._get_hand_center(image, results.multi_hand_landmarks[0]) cv2.circle(image, hand_center, 32, ORANGE, cv2.FILLED, cv2.LINE_AA) # update target radius if self._target_pos.update(hand_center) and self._event_observer is not None: self._update_calibration() # draw the hand annotations on the image (only for debug) if self._is_debug: self._draw_hand_skel(image, results) return image def _update_calibration(self): self._event_observer.record_calibration(target_radius=self._target_pos.radius) self._event_observer.sync_calibration() La implementación es similar al tracker que antes definimos. Los métodos interesantes aquí son los relacionados con la calibración, que están en el objeto _event_observer.\nrecord_calibration(), encargado de notificar al sirviente los nuevos parámetros que deseamos almacenar como datos de calibración. Se utiliza un diseño tipo clave-valor, usando los nombres de los parámetros de la función como clave. En el ejemplo, los datos almacenados serán: {\u0026quot;target_radius\u0026quot;: x}, siendo x el valor actual del radio (que lo tenemos en self._target_pos.radius). No es necesario indicar todos los parámetros al mismo tiempo; podríamos, por ejemplo, llamar de nuevo a la función con otro dato (algo como record_calibration(date=now)), y ambos registros se conservarían, tanto target_radius como date.\nsync_calibration(), que envía a la ECI todos los datos de calibración almacenados hasta ahora. Queda por cuenta del desarrollador cómo se representarán en la interfaz.\nPor cuestiones de completitud, he aquí de nuevo la implementación de la clase que gestiona el target position, llamada TargetPositionCalibrator:\nsnippet.python class TargetPositionCalibrator: def __init__(self, args, min_radius=64): self._w = args.video_width self._h = args.video_height self._min_r = min_radius self.radius = min_radius self._handler_r = 16 self._pos = (self._w // 2 - self.radius, self._h // 2 - self.radius) def draw(self, image): # outter target circle x, y = self._pos cv2.circle(image, (x \u0026#43; 1, y \u0026#43; 1), self.radius, GRAY, 3, cv2.LINE_AA) cv2.circle(image, self._pos, self.radius, WHITE, 3, cv2.LINE_AA) # handler point cv2.circle(image, (x \u0026#43; self.radius, y), self._handler_r, GREEN, cv2.FILLED, cv2.LINE_AA) def update(self, position): \u0026#34;\u0026#34;\u0026#34;Returns whether registered calibration values have changed or not.\u0026#34;\u0026#34;\u0026#34; dx = abs(position[0] - (self._pos[0] \u0026#43; self.radius)) dy = abs(position[1] - self._pos[1]) if dx \u0026lt; self._handler_r * 2 and dy \u0026lt; self._handler_r * 2: self.radius = max(self._min_r, abs(position[0] - self._pos[0])) return True return False Por último, un paso importante es instanciar el nuevo componente de vídeo. Como la aplicación principal es la misma, y queremos seguir dando soporte al ejercicio que teníamos hecho, debemos crear un componente u otro en función del argumento --calibrate:\nsnippet.python class MoveToPoint(Ice.Application): def run(self, args): ... # create our hand detector if self.args.calibrate: self._detector = HandPositionCalibrator(self.args, servant) else: self._detector = HandPositionDetector(self.args, servant) Otra posible implementación habría supuesto utilizar el mismo componente de vídeo, pero implementar internamente la lógica que hiciera una función u otra, en función del valor de args.calibrate.\nInterfaz ECI de calibración Cuando se ejecuta el módulo en modo de calibración, el fichero HTML que se cargará no será index.html, como hasta ahora, sino calibration.html, que reside en la misma carpeta. Esta interfaz no contie los contadores de tiempo o de iteraciones, sino que muestra al usuario los valores actuales de calibración. También hay un botón que permite determinar cuando la calibración ha concluido, e indicar al backend que deben almacenarse los valores recogidos.\nAl igual que la interfaz principal, la de calibración también cuenta con feedback de vídeo, y un placeholder para los vídeos modelo del ejercicio, por si fueran necesarios para la calibración. En el ejemplo que estamos desarrollando no hay modelos, por lo que elimino esta sección de la plantilla:\nsnippet.html \u0026lt;!-- ----------------------------------------------------------- video from trainer model sample --\u0026gt; \u0026lt;div id=\u0026#34;model-video\u0026#34;\u0026gt; \u0026lt;video id=\u0026#34;mv-elem\u0026#34; poster=\u0026#34;/shared/images/model-poster.png\u0026#34; autoplay loop muted\u0026gt;\u0026lt;/video\u0026gt; \u0026lt;/div\u0026gt; Por otro lado, sí deseo mostrar el valor actual del parámetro que quiero almacenar en la calibración (a saber, el radio del círculo objetivo). Para ello existe la sección stats. Esta formada por una lista de pares, nombre-valor, donde el primer elemento (\u0026lt;span\u0026gt;) es el nombre, y el segundo (\u0026lt;h1\u0026gt;) es el valor. Para el ejemplo, la modificamos de la siguiente forma:\nsnippet.html \u0026lt;!-- ----------------------------------------------------------- current calibration parameters (modify to suit your needs) --\u0026gt; \u0026lt;div class=\u0026#34;stats\u0026#34;\u0026gt; \u0026lt;span\u0026gt;Target Radius:\u0026lt;/span\u0026gt; \u0026lt;h1 id=\u0026#34;target-radius\u0026#34;\u0026gt;0\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; Ahora será necesario actualizar ese elemento, #target-radius, con el valor que nos proporciona el módulo al sincronizar. Para ello, necesitamos modificar el fichero JavaScript que contiene el código relacionado con la interfaz ECI, que es diferente del principal (app.js). En este caso, se llama calibration.js, y el método que nos interesa es syncCalibration, que recibe un objeto con todos los datos de calibración almacenados para este ejercicio y este usuario, según lo hayamos definido en el módulo con record_calibration. Así, en nuestro ejemplo, tendremos disponible directamente el valor usando la clave target_radius:\nsnippet.js // this is an RMI invocation (common iface) syncCalibration(data) { data = JSON.parse(data); $(\u0026#39;#target-radius\u0026#39;).text(data.target_radius); } ¡Y listo! Aunque hay más cosas que podemos modificar, para este ejemplo, tenemos todo lo necesario para iniciar el proceso de calibración de nuestro módulo.\nUso de datos de calibración Una vez que el ejercicio está correctamente calibrado, es necesario realizar una última modificación en el componente principal (no el de calibración) para que use los datos convenientemente. La calibración está disponible en el atributo _calibration de la clase VideoComponent y sus derivadas. Es un diccionario con las claves y valores que hayamos registrado.\nPara el ejemplo que nos atañe, he modificado la creación de la instancia TargetPositionIterator para pasarle el radio que se indica en la calibración. Dado que la clase se reutiliza, y que en modo calibración no hay _calibration, el código es un poco más complejo, pero podría haber sido simplemente TargetPositionIterator(args, self._calibration[\u0026quot;target_radius\u0026quot;]):\nsnippet.python kwargs = {} if self._calibration is not None: kwargs[\u0026#34;radius\u0026#34;] = self._calibration[\u0026#34;target_radius\u0026#34;] self._target_pos = TargetPositionIterator(args, **kwargs) Con ese cambio, ya deberías poder cambiar el tamaño del círculo objetivo en el módulo move-to-point.\n","ref":"https://arcogroup.bitbucket.io/shapes/creating_phyxio_modules/","summary":"Manual de usuario para la creación de módulos para Phyxio","title":"Creación de Módulos para Phyxio"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/development/","summary":"","title":"development"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/module/","summary":"","title":"module"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/phyxio/","summary":"","title":"phyxio"},{"body":"","ref":"https://arcogroup.bitbucket.io/shapes/","summary":"Technological solutions which seeks to facilitate long-term healthy, active ageing and high-quality standard of life.","title":"Shapes"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/","summary":"","title":"Tags"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/magic-mirror/","summary":"","title":"magic mirror"},{"body":"Overview A Smart Mirror can be overloaded with multiple services and utilities to serve different functionalities, but, like any other software product, this have to be shaped into its final user needs. For that, the smartmirror-console comes in handy, as it provides a high-level interface to configure all Smart Mirror packages and services.\nIngredients The mandatory ingredients to follow this recipe are:\nA PC with a browser installed. A Raspberry Pi 4, or RPi4, either connected to the same network as the PC, or with a public ID address. The package smartmirror-console installed on your RPi4, which is available in ARCO package repository. Be sure it has the version 0.20210510+ds.1 or higher. Start the environment To access the console, open the PC browser and write the RPi4 address, its IP address or a public one if it has any. It is required to specifiy the port to access to, which is 5000. This is done by preciding the port number with a colon.\nFor example, if the RPi4 has the address my-smartmirror.com, the console address will be http://my-smartmirror.com:5000.\nThen, a login page will be shown. Credentials will be admin for both username and password for the first time using the application, followed by other form asking to change the default password. So, from that point, username will remain as admin, but it will be used the new setted password, so be sure to remember that password!\nAfter logged in, it will appear the main page, where all available and configurable modules will be shown. There is only one module available right now, which is the Mi Band 4 Data Collector. This one, and further added modules, will be explained in the following sections.\nModules In this section it will be explained how to configure each one of the Smart Mirror modules.\nMi Band 4 Data Collector This section explains how to configure the Mi Band 4 Data Collector module, that refers to miband-dc debian package.\nIt can also be managed using its low-level interface, through configuration files. To know more about this, read miband-dc manual page or Configuring miband-dc section at Integrating Xiaomi Mi Band 4 devices with Smart Mirror.\nIngredients The specific ingredients to complete the steps detailed in this section are:\nIf Mi Band 4 Data Collector is not installed, do it through smartmirror-console interface. A PostgreSQL database with remote connections allowed, either by using a provider on the cloud, or by doing a manual installation. If the latter is chosen, please read Appendix 1. Mi Fit application installed on a mobile device (Android or iOS). One or more Mi Smart Band 4 devices, or MB4. Settings Go to the settings page of the Mi Band 4 Data Collector, in which there is a side menu displaying settings sections. Also, at the top of the page we can see the version of the package and several buttons which change the package status: start or stop collecting, install or uninstall the package, and inspect the service (show logs).\nLet\u0026rsquo;s review each one of the settings sections.\nDatabase parameters The data collector stores all information collected in a database. In order to use it, please complete the form with its connection parameters and click \u0026ldquo;Save\u0026rdquo; button.\nWarning! This configuration is mandatory for the Mi Band 4 Data Collector to run properly. Also, the console does not test the connection to the database. Notifications The service miband-dc, the data collector, can notify the user, if the user wants to, about some internal information. This can be done by setting an e-mail address and a time interval. This data will be stored by the console, but, if selected, used by the data collector. To enable a notification, select the desired notification types and click \u0026ldquo;Save\u0026rdquo;.\nHooks This service can be linked with Magic Mirror, magic-mirror-2 package, through its modules. To do so, make sure you have installed the aforementioned package and the required modules for each hook. To enable a hook, just select it and click \u0026ldquo;Save\u0026rdquo;.\nCharging station To ease the data updating process, the data collector can perform, if configured, as a charging station. This mode pretends to simulate an updating station, where users leave their devices charging while data is being stored, resulting in a less aggressive communication. To enable it, select the option and click \u0026ldquo;Save\u0026rdquo;.\nDevices In order to pair your MB4 devices with the data collector, you must link them to a Mi Fit account. To do so, follow the steps of Appendix 2.\nTo add all linked MB4, go to the Getting tokens section. Fill the form with your credentials and click \u0026ldquo;Request tokens\u0026rdquo;. After some time, a list containing your devices data will appear. Finally, click \u0026ldquo;Add all and save\u0026rdquo;.\nAlso, you can manually add a device to the data collector providing a MAC address and an authentication token. Click on the \u0026ldquo;Add\u0026rdquo; button, with a \u0026ldquo;plus\u0026rdquo; icon, to add a new device. After providing the required information, click \u0026ldquo;Save\u0026rdquo;.\nWarning! The console does not check the validity of a band token. To know if communication is correct, see data collector logs by clicking the \u0026ldquo;Inspect\u0026rdquo; button. Resources and useful links Integrating Xiaomi Mi Band 4 devices with Smart Mirror Magic Mirror miband-dc manual page Mi Fit application (Android or iOS) Mi Smart Band 4 PostgreSQL Appendix 1. Manual installation of PostgreSQL Install PostgreSQL following its instructions. Then, you need to modify two configuration files: postgresql.conf and pg_hba.conf.\nThe first one, postgresql.conf, can be found by executing SHOW config_file with the PostgreSQL client psql. Once found, modify the parameter listen_addresses to *. If the line containing this parameter is preceded by a hashtag, remember to delete this character. For example, in a Linux environment:\nconsole user@linux:~/$ sudo su postgres -c \u0026#34;psql -c \u0026#39;SHOW config_file\u0026#39;\u0026#34; config_file ----------------------------------------- /etc/postgresql/12/main/postgresql.conf (1 row) user@linux:~/$ [ ... modify file ... ] user@linux:~/$ sudo grep -n listen_addresses /etc/postgresql/12/main/postgresql.conf 59:listen_addresses = \u0026#39;*\u0026#39; The second configuration file, pg_hba.conf, can be found searching for the parameter hba_file in the first one, postgresql.conf. Once found, and following this file format, add the following lines:\nsnippet.conf # TYPE DATABASE USER ADDRESS METHOD host database user samenet md5 Replace database and user with its respective values for the data collector. Both values are miband-dc in this recipe, section Database parameters. Also, replace samenet with your PostgreSQL host public IP address if it is not running on the same network as the RPi4 with the data collector.\nAppendix 2. Configure Mi Band 4 devices with Mi Fit application First, create an account for Mi Fit application and log in.\nWarning! It is required a \u0026ldquo;recent\u0026rdquo; Mi Fit account to follow further steps. If you are not sure about when your account was created, it is highly recommended that you create a new one. Link the devices following the manufacturer instructions. If you add multiple devices, you will be asked to \u0026ldquo;deactivate\u0026rdquo; your previous band, as there cannot be two active devices of the same type. Ignore this warning, as it is only important to link the device with the account, not the \u0026ldquo;active\u0026rdquo; state.\nEach time you add a new device, go to Profile \u0026gt; My devices \u0026gt; Active band \u0026gt; Heart rate monitoring and set:\nMonitoring method to Automatic heart rate monitoring and sleep assistant. Detection frequency to less or equal than 10 minutes. Enable Activity monitoring. Once done, it is important to turn off the Bluetooth of the mobile phone you used to activate the band, as it will interfere with the communication between the data collector and the active band. If you need to enable the Bluetooth, just make sure that you place the MB4 far away, if possible.\n","ref":"https://arcogroup.bitbucket.io/shapes/smartmirror_console_manual/","summary":"Learn how to configure and manage an ARCO Smart Mirror with the package smartmirror-console.","title":"Manage your Smart Mirror with smartmirror-console"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/miband4/","summary":"","title":"miband4"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/xiaomi/","summary":"","title":"xiaomi"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/deepspeech/","summary":"","title":"deepspeech"},{"body":"Overview DeepSpeech is a speech recognition engine developed by Mozilla that uses a model trained by machine learning techniques. This recipe will explain how to make use of this tool for speech recognition.\nIngredients Linux environment (tested on Ubuntu 20.04 LTS x64) APT dependencies: docker (\u0026gt;= 20.10.2), python3 (\u0026gt;= 3.7), venv The speech recognition repository available in this link Setup First, clone the repository provided in the Ingredients section. Once cloned you must copy the audios, in .wav format, that you want to convert from speech to text into a folder called original. In the repository you can find some sample audios in the Downloads section.\nconsole user@pc:~/$ tree original original └── es ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_1_001.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_1_002.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_1_003.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_1_004.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_2_001.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_2_002.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_2_003.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_3_001.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_3_002.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_3_003.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_4_001.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_4_002.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_4_003.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_5_001.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_5_002.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_6_001.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_6_002.wav ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_7_001.wav └── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_7_002.wav 1 directory, 19 files Set the commands In the name of the audios an id is specified. This id has to match the content of the commands.csv file, which specifies the audio id and the audio transcript. With the example audios our commands.csv file would look like this: console user@pc:~/$ cat commands.csv id,es 1,ok espejo 2,hola espejo 3,espejo 4,vale espejo 5,llama a mi hija 6,espejo llama a mi hijo 7,espejo llama a mi nuera Usage to train a model In order to install all dependencies, you must execute from the repository root directory make dependencies. Then, execute make volumes and finally make, which executes build and run targets. The audio will be processed and stored in a new folder calles data (this folder will be copied into a docker volume to be used by the container), and will be generated as data/train.csv, registry.csv, Dockerfile, and so on. Also, then container will start running detached. For checking its status execute make log.\nIf you conciusly want to stop the container, use make stop. This stopping the container and stores last finished DeepSpeech epoch so, if you start again the container continues from there. If you want to restart the container just execute make run.\nIn case the training of the model has finished and you want to restart it anyway, change in the file configuration.env the variables CURRENT_EPOCH to 0 or MAX_EPOCH to a value greater than the current one.\nIf you manually added or changed something keep in mind that DeepSpeech could no longer work, as it tries to restore from a checkpoint that has invalid values. To start a new training, rebuild the trainer image executing make rebuild NEW_DATA=yes LOAD_FROM_CHECKPOINT=no. This copies the new and modified data contents to docker volume and tells DeepSpeech to not load from any checkpoint, so there is no errors.\nMaking a mmap-able model The shapes-mm-speechrecognition.pb model file generated in the above step will be loaded in memory to be dealt with when running inference. This will results in extra loading time and memory consumption. One way to avoid this is to directly read data from the disk.\nTensorflow has tooling to achieve this, so clone DeepSpeech repository with the command git clone --branch v0.9.3 https://github.com/mozilla/DeepSpeech. To use the tools in this repository you must have python version 3.6. To do this you can create a virtual environment with that version with the command python3 -m virtualenv -p /usr/bin/python3.6 \u0026lt;name_virtual_environment\u0026gt;. Then use util/taskcluster.py tool to download: console user@user-pc:~$ python3 util/taskcluster.py --source tensorflow --artifact convert_graphdef_memmapped_format --branch r1.15 --target . Then, from the directory where you will use the model execute the next command: console user@user-pc:~$ ./convert_graphdef_memmapped_format --in_graph=/var/lib/docker/volumes/ds_model/_data/shapes-mm-speechrecognition.pb --out_graph=shapes-mm-speechrecognition.pbmm Run the model First, you will have to install the DeepSpeech wheel. To perform the installation, just use pip3 as such pip3 install deepspeech. After installation has finished, you should be able to call deepspeech from the next command-line:\nconsole user@user-pc:~$ deepspeech --model shapes-mm-speechrecognition.pbmm --audio data/es/\u0026lt;name_file_audio\u0026gt; The output of this command with one of the example audios would look like this: console user@user-pc:~$ deepspeech --model shapes-mm-speechrecognition.pbmm --audio data/es/shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_1_001.wav Loading model from file shapes-mm-speechrecognition.pbmm TensorFlow: v2.3.0-6-g23ad988 DeepSpeech: v0.9.3-0-gf2e9c85 2021-04-28 09:32:21.749436: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. Loaded model in 0.0949s. Running inference. ok espejo Inference took 1.570s for 2.400s audio file. ","ref":"https://arcogroup.bitbucket.io/shapes/speech_recognition/","summary":"A recipe to learn how train a speech recognition model and how to use that with DeepSpeech","title":"Speech Recognition with DeepSpeech"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/speech-to-text/","summary":"","title":"speech to text"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/voice-assistant/","summary":"","title":"voice assistant"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/grafana/","summary":"","title":"grafana"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/influxdb/","summary":"","title":"influxdb"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/monitoring/","summary":"","title":"monitoring"},{"body":"Overview Zigbee2mqtt is an open-source project that enables the interconnection of different-brand devices with Zigbee connectivity. It can also be integrated with Home Assistant thanks to the bidirectional message relay from the network to MQTT. Because neither our Raspberry Pi nor our laptop has a ZigBee interface, we additionally need a zigbee adapter, this is the zig-a-zig-ah! (zzh!).\nIngredients In order to follow this recipe you will need:\nA suported Zigbee adapter (zzh!). One or more suported Zigbee device/s. A Raspberry Pi 4 or RPi4. The shapes-monitoring-service debian package, which will configure ZigBee2MQTT, Mosquitto, InfluxDB, Telegraf and Grafana, available in ARCO package repository. Grafana APT repository, the latest OSS release, available in download link. Configuring zzh! Drivers for CH341 First step in getting the zzh! set up is to ensure that the host computer has the right drivers for the CH341 installed. Plug your device and ensure that zzh is detected with the command dmesg. If detected, you will see something like:\nconsole [ 2514.373835] usbcore: registered new interface driver ch341 [ 2514.373867] usbserial: USB Serial support registered for ch341-uart [ 2514.373903] ch341 1-1:1.0: ch341-uart converter detected [ 2514.374745] usb 1-1: ch341-uart converter now attached to ttyUSB0 Download cc2538-bsl To run cc2538-bsl.py you need to have python3 and pip3 installed on your system. If you don\u0026rsquo;t have them installed running the following commands should work: sudo apt update \u0026amp;\u0026amp; sudo apt-get install python3-pip\nTo download and extract cc2538-bsl run wget -O cc2538-bsl.zip https://codeload.github.com/JelmerT/cc2538-bsl/zip/master \u0026amp;\u0026amp; unzip cc2538-bsl.zip in your terminal and to install the required dependencies run sudo pip3 install pyserial intelhex.\nConfiguring ZigBee2MQTT Download the Z-Stack coordinator firmware from https://github.com/Koenkk/Z-Stack-firmware. The firmware you need can be found under coordinator/Z-Stack_3.x.0/bin/CC26X2R_coordinator_\u0026lt;date\u0026gt;.zip. Download and extract this.\nTo burn this on your zzh! go to the directory cc2538-bsl-master and run: console pi@raspberrypi:~$ ./cc2538-bsl.py -p /dev/\u0026lt;port\u0026gt; -evw \u0026lt;firmware_zigbee\u0026gt;.hex The port, in my case, is ttyUSB0, as shown in section Drivers for CH341, so do not forget to verify the port you are using on your machine and change it, accordingly, if needed.\nIn case you want to erase the flash, run: console pi@raspberrypi:~$ ./cc2538-bsl.py -p /dev/\u0026lt;port\u0026gt; -e Warning! Before running, all you have to do is to look if your port is different from ttyUSB0, in this case you have to change it in the docker-compose.yml file in the /etc/compose directory.\nConfiguring shapes-monitoring-service The monitoring system, or shapes-monitoring-service, is a service running always in our device with systemd, we can see its status with the following command: console pi@raspberrypi:~$ systemctl status shapes-monitoring.service ● shapes-monitoring.service - Service for monitoring smart sensors Loaded: loaded (/lib/systemd/system/shapes-monitoring.service; enabled; vendor preset: enabled) Active: active (exited) since Thu 2021-05-13 10:55:14 CEST; 21min ago Process: 20484 ExecStart=/usr/bin/docker-compose up -d (code=exited, status=0/SUCCESS) Main PID: 20484 (code=exited, status=0/SUCCESS) may 13 10:54:56 maria-GP63-Leopard-8RD systemd[1]: Starting Service for monitoring smart sensors... may 13 10:54:56 maria-GP63-Leopard-8RD docker-compose[20484]: Creating network \u0026#34;compose_default\u0026#34; with the default driver may 13 10:54:56 maria-GP63-Leopard-8RD docker-compose[20484]: Creating mosquitto ... may 13 10:54:56 maria-GP63-Leopard-8RD docker-compose[20484]: Creating zigbee2mqtt ... may 13 10:54:56 maria-GP63-Leopard-8RD docker-compose[20484]: Creating influxdb ... may 13 10:55:07 maria-GP63-Leopard-8RD docker-compose[20484]: [153B blob data] may 13 10:55:14 maria-GP63-Leopard-8RD docker-compose[20484]: [43B blob data] may 13 10:55:14 maria-GP63-Leopard-8RD systemd[1]: Finished Service for monitoring smart sensors. This service runs ZigBee2MQTT, Mosquitto, InfluxDB and Telegraf with docker. If you want to stop the service run the command systemctl stop shapes-monitoring-service.\nOptional. When synchronizing the sensors you can rename the devices in the configuration file in the zigbee2mqtt-data folder, which is located in the /etc/compose directory. An example would be as follows: zigbee2mqtt-data/configuration.yaml homeassistant: true permit_join: true mqtt: base_topic: zigbee2mqtt server: \u0026#39;mqtt://localhost\u0026#39; serial: port: /dev/ttyUSB0 advanced: rtscts: false devices: \u0026#39;0x00158d00047b5824\u0026#39;: friendly_name: temperature_humidity_living-room/SENSOR \u0026#39;0x00158d0002520b26\u0026#39;: friendly_name: door_window_living-room/SENSOR \u0026#39;0x00158d0002567d32\u0026#39;: friendly_name: motion_living-room/SENSOR \u0026#39;0x00158d0003cc4e70\u0026#39;: friendly_name: motion_bedroom/SENSOR \u0026#39;0x00158d000395b3a6\u0026#39;: friendly_name: motion_kitchen/SENSOR \u0026#39;0x00158d00047b835a\u0026#39;: friendly_name: temperature_humidity_kitchen/SENSOR \u0026#39;0x00158d000290b6d5\u0026#39;: friendly_name: door_window_bedroom/SENSOR In addition, this package automates the provision of Grafana with data stored in InfluxDB. To create the correct files, shapes-monitoring-service must know the database connection parameters, so we must edit its configuration file with sudo shapes-monitoring-service -e. A configuration can be: configuration.conf DBNAME=zigbee2mqtt DBUSER=telegraf DBPASSWORD=telegraf DBHOST=localhost DBPORT=8086 Then, execute the following command to create the database in InfluxDB: console pi@raspberrypi:~$ sudo shapes-monitoring -u Connected to http://localhost:8086 version 1.8.5 InfluxDB shell version: 1.6.4 \u0026gt; create database zigbee2mqtt \u0026gt; use zigbee2mqtt Using database zigbee2mqtt \u0026gt; create user telegraf with password \u0026#39;telegraf\u0026#39; with all privileges \u0026gt; grant all on zigbee2mqtt to telegraf \u0026gt; exit Database checked! Done updating! This will provision our local Grafana instance. If you check http://localhost:3000 you will see the Grafana UI, and following on the side menu Dashboards \u0026gt; Manage we can see our provisioned dashboards: To see what files where provisioned and the generated dashboards information use: console pi@raspberrypi:~$ sudo shapes-monitoring -s INFO: Datasource provider file at /etc/grafana/provisioning/datasources/shapes-monitoring-datasource.yaml INFO: Dashboard provider file at /etc/grafana/provisioning/dashboards/shapes-monitoring-dashboard.yaml INFO: Found dashboard definition at /var/lib/grafana/dashboards/shapes-monitoring-dashboard.json: INFO: Name: smart-sensors INFO: UID: fFBmCCrMz INFO: Available panels: INFO: ID: 1\tDescription: Bathroom sensors INFO: ID: 2\tDescription: Living room sensors INFO: ID: 3\tDescription: Bedroom sensors INFO: ID: 4\tDescription: Kitchen sensors NOTE: If you want grafana to be started on boot, execute sudo systemctl enable grafana-server.service.\nOnce the service shapes-monitoring-service is started and the database has been configured, you have to run the shapes-monitoring command. This command executes an mqtt client that transforms the values true and false into an 1 and 0 for the motion and door and window sensors before storing them in the database. The output of this command would look like this: console pi@raspberrypi:~$ shapes-monitoring-client Connected to MQTT Broker localhost:1883 with return code 0! Received {\u0026#34;battery\u0026#34;:100,\u0026#34;contact\u0026#34;:true,\u0026#34;linkquality\u0026#34;:162,\u0026#34;voltage\u0026#34;:3005} from zigbee2mqtt/door_window_living-room/SENSOR topic! Sensor data: {\u0026#39;battery\u0026#39;: 100, \u0026#39;contact\u0026#39;: 1, \u0026#39;linkquality\u0026#39;: 162, \u0026#39;voltage\u0026#39;: 3005} Sensor data published to zigbee2mqtt/smart-sensors/living-room topic! Received {\u0026#34;battery\u0026#34;:100,\u0026#34;contact\u0026#34;:true,\u0026#34;linkquality\u0026#34;:158,\u0026#34;voltage\u0026#34;:3005} from zigbee2mqtt/door_window_bedroom/SENSOR topic! Sensor data: {\u0026#39;battery\u0026#39;: 100, \u0026#39;contact\u0026#39;: 0, \u0026#39;linkquality\u0026#39;: 158, \u0026#39;voltage\u0026#39;: 3005} Sensor data published to zigbee2mqtt/smart-sensors/bedroom topic! Received {\u0026#34;battery\u0026#34;:100,\u0026#34;humidity\u0026#34;:52.09,\u0026#34;linkquality\u0026#34;:129,\u0026#34;pressure\u0026#34;:947.6,\u0026#34;temperature\u0026#34;:25.24, \u0026#34;voltage\u0026#34;:3015} from zigbee2mqtt/temperature_humidity_living-room/SENSOR topic! Sensor data published to zigbee2mqtt/smart-sensors/living-room topic! To view the data transmitted by the sensors go to the Smart Sensors dashboard and you will see something similar to the following image:\nBathroom sensors Living-room sensors Bedroom sensors Kitchen sensors ","ref":"https://arcogroup.bitbucket.io/shapes/zigbee2mqtt/","summary":"Explanation of the installation and execution of the monitoring system.","title":"Monitoring System"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/smart-sensors/","summary":"","title":"smart sensors"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/telegraf/","summary":"","title":"telegraf"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/zigbee2mqtt/","summary":"","title":"zigbee2mqtt"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/zzh/","summary":"","title":"zzh!"},{"body":"Overview You can set up your own Google Assistant with just a Raspberry Pi and an Adafruit Voice Bonnet. You through setting up the Google Assistant API you can install a few library, enable permissions and get the Google Assistant running on the RPi. Now you can ask Google what you want with the simple push of a button.\nIngredients In order to follow this recipe you will need:\nA Raspberry Pi 4 or RPi4 A Adafruit Voice Bonnet Two Mono Enclosed Speaker or headphones A Google account Raspberry Pi Setup The first step is perform an update/upgrade and install the cross-platform package manager pip:\nconsole pi@raspberrypi:~ $ sudo apt-get update pi@raspberrypi:~ $ sudo apt-get -y upgrade pi@raspberrypi:~ $ sudo apt-get install -y python3-pip pi@raspberrypi:~ $ sudo pip3 install --upgrade setuptools Blinka Setup Blinka is a CirciutPython library compatibility layer. It requires just a few commands to run:\nconsole pi@raspberrypi:~ $ cd ~ pi@raspberrypi:~ $ sudo pip3 install --upgrade adafruit-python-shell pi@raspberrypi:~ $ wget https://raw.githubusercontent.com/adafruit/Raspberry-Pi-Installer-Scripts/master/raspi-blinka.py pi@raspberrypi:~ $ sudo python3 raspi-blinka.py When it finishes, it will ask you if you would like to reboot. Choose yes\nYou install DotStart library to controlling the 3 on-board DotStar LEDs:\nconsole pi@raspberrypi:~ $ pip3 install --upgrade adafruit-circuitpython-dotstar adafruit-circuitpython-bmp280 Check I2C and SPI To verify that the script had enable I2C and SPI, run the following command:\nconsole pi@raspberrypi:~ $ ls /dev/i2c* /dev/spi* You sould see the next response:\nIf you need another hardware SPI port, you can enable it by adding the line dtoverlay=spi1-3cs to the bottom of boot/config.txt and rebooting.\nAudio Setup Install Voicecard software You run sudo i2cdetect -y 1 and you should see an entry under 1a. If you already installed software the number appear as UU, as show in the image. At the commands like run: console pi@raspberrypi:~ $ sudo apt-get install -y git pi@raspberrypi:~ $ git clone https://github.com/HinTak/seeed-voicecard pi@raspberrypi:~ $ cd seeed-voicecard pi@raspberrypi:~ $ git checkout v5.5 pi@raspberrypi:~ $ sudo ./install.sh When it finishes reboot and on reboot run sudo aplay -l to list all sound cards. You should see it at the bottom: You can user alsamixer to adjust the volume. Select the card with F6 Headphone/Speaker Test Make sure the audio on/off switch is set to on. Run speaker-test -c2 and you will hear white noise.\nMicrophone Test Run sudo arecord -f cd -Dhw:2 | aplay -Dhw:2. Then speak to hear yourself echoed.\nGoogle Setup Project Creation Start by going to https://console.actions.google.com/ and create New Project.\nEnter a project name such as Google Assistant BrainCraft and click on Create Project.\nAt the bottom of the page, click on the link to go to device registration. Device Registration Click the register model button. Fill in the fields with the requested information. For device type, choose Speaker. Click the register moddel button. Click Download OAuth 2.0 credentials and save the JSON file as client_secret.json. You will upload this to your Raspberry Pi in a later step. Click Next. Under traits, click All 7 traits and then click Save Traits.\nGoogle Assistant API Setup Go to the Google Developers Console to enable the API at https://console.developers.google.com/apis/api/embeddedassistant.googleapis.com/overview. Click on the project selector at the top. Then, select the project. Click on the Enable button. Once it is enabled, you will be taken to the Overview Screen. Click on the Credentials tab on the left and then click the Configure Consent Screen button on the right. For user type, select External and click create.\nThen, enter an application name, this is is the name that will appear on the permissions screen. Select a support email, this is the email that will appear when you click on the application name on the permissions screen. Finally, click save.\nEnabling Permissions Go to activity controls at https://myaccount.google.com/activitycontrols. Make sure web and app activity is on. Also make sure to select the include chrome history and activity from sites, apps, and devices that use google services; this is the first checkbox. An additional dialog may pop up asking you to confirm, click the Turn on button.\nDevice Setup Upload the client_secret.json credentials file and place it in your home directory on RPi. Next, you are going to check that you have the required packages and setup a virtual environment\nconsole pi@raspberrypi:~ $ cd ~ pi@raspberrypi:~ $ sudo apt-get update pi@raspberrypi:~ $ sudo apt-get install python3-dev python3-venv pi@raspberrypi:~ $ python3 -m venv env pi@raspberrypi:~ $ env/bin/python -m pip install --upgrade pip setuptools wheel pi@raspberrypi:~ $ source env/bin/activate Install the Authorization library: console pi@raspberrypi:~ $ sudo apt-get install portaudio19-dev libffi-dev libssl-dev pi@raspberrypi:~ $ python -m pip install --upgrade google-assistant-sdk[samples] pi@raspberrypi:~ $ python -m pip install --upgrade google-auth-oauthlib[tool] Generating an OAuth Token Run the following command:\nconsole pi@raspberrypi:~ $ google-oauthlib-tool --scope https://www.googleapis.com/auth/assistant-sdk-prototype --save --headless --client-secrets ~/client_secret.json The script should provide a URL to visit to generate the token. Go ahead and click on the Advanced link. After that, it will come up with a confirmation dialog, go ahead and click on Allow. Finally, you will be given an Authorization Code; click on the Copy icon and paste the authorization on your terminal. Code back into the script and the token will be generated and saved.\nUsage First make sure you\u0026rsquo;re in the virtual environment. If you aren\u0026rsquo;t run source env/bin/activate.\nNext, run the following command\nconsole pi@raspberrypi:~ $ wget https://raw.githubusercontent.com/adafruit/Adafruit_Learning_System_Guides/master/BrainCraft_Google_Assistant/gv_buttontotalk.py And run the script, type python3 gv_buttontotalk.py. At the beggining the leds are red so click the button and the leds should turn green, meaning it\u0026rsquo;s waiting for you to give it ask a question.\n","ref":"https://arcogroup.bitbucket.io/shapes/google_assistant_on_voice_bonnet/","summary":"A recipe to learn how to create a Google Assistant with a Raspberry Pi 4 and Adafruit Voice Bonnet.","title":"Google Assistant on the Voice Bonnet"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/bluetooth/","summary":"","title":"bluetooth"},{"body":"Overview Mi Band 4 is the most popular and best-selling smart band of the famous IT company Xiaomi. It provides several information about our physical activity like number of steps or heart rate. This device is linked with our smartphone, which access to its data using Bluetooth LE. Using this mechanism, and with a Raspberry Pi transformed into a smart mirror, we can monitorize this information and present it with graphs and other intuitive forms of displaying its progression through time.\nIngredients In order to follow this recipe you will need:\nA Xiaomi Mi Band 4 device or MB4. A Raspberry Pi 4 or RPi4. A PostgreSQL database with remote connections allowed. It could be located on the cloud or run locally. See Appendix 1: Configuration for local database. The miband-dc package on your RPi4, which is our data collector, available in ARCO package repository. The miband-grafana package on your PC, which will configure Grafana, also available in ARCO package repository. Basic knowledge of linux. Configuring miband-dc The data collector, or miband-dc, is a service running always in our device with systemd, we can see its status with the following command:\nconsole pi@raspberry:~/$ systemctl status miband-dc.service\r● miband-dc.service - miband-dc service\rLoaded: loaded (/lib/systemd/system/miband-dc.service; enabled; vendor preset: enabled)\rActive: active (running) since Wed 2020-12-02 19:08:34 CET; 4s ago\rMain PID: 22377 (miband-dc)\rTasks: 3 (limit: 9362)\rMemory: 12.6M\rCGroup: /system.slice/miband-dc.service\r└─22377 /usr/bin/python3 /usr/bin/miband-dc -d devices.csv -c settings.json\rdic 02 19:08:34 raspberry systemd[1]: Started miband-dc service.\rBut it needs configuration to work properly.\nDatabase The service needs a database to store the collected data. It reads the connection parameters from the file /etc/miband-dc/settings.json. An example is:\nsettings.json { \u0026#34;db_name\u0026#34;: \u0026#34;user-stadistics\u0026#34;, \u0026#34;db_user\u0026#34;: \u0026#34;miband-dc\u0026#34;, \u0026#34;db_passwd\u0026#34;: \u0026#34;mysecret\u0026#34;, \u0026#34;db_host\u0026#34;: \u0026#34;postgresql.on.cloud.com\u0026#34;, \u0026#34;db_port\u0026#34;: 5432 } It is really important that the database user OWNS the database, so it has privileges to create a new table, if needed, and can perform any operation in it.\nDevices First, the MB4 has to be linked to an account at MiFit App (available on Google Play Store and Apple Store). Register the MB4 following the MiFit instructions. While you are there, enable continous heart rate monitoring:\nGo to link device settings and select \u0026lsquo;heart rate monitoring\u0026rsquo;. Set monitoring method to \u0026lsquo;Automatic heart rate monitoring and sleep assistant\u0026rsquo;. Enable activity monitoring. When finished, turn off your smartphone Bluetooth and do not turn it back while miband-dc is running, as the service will not work properly. Then, store the MB4 information in /etc/miband-dc/devices.csv like:\ndevices.csv # id | mac | token 1 | E3:49:01:ED:00:A6 | abc123DEF456ghi789JKL3mnsOPQrst2 The device token can be obtained by using Huami token script with the used account in MiFit credentials. This works for Amazfit accounts, so if you are not sure what type of account you have, we encourage you to create a new one from within MiFit app. An example is: console pi@raspberry:~/$ python3 huami_token.py -m amazfit -e pi@raspberry.com -p mypassword -b\rGetting access token with amazfit login method...\rToken: [\u0026#39;abcDEFghiJKLmnsOPQrst\u0026#39;]\rLogging in...\rLogged in! User id: 0123456789\rGetting linked wearables...\r\u0026#43;------------------------------------------------------------------------\u0026#43;\r| MAC | auth_key |\r|-------------------\u0026#43;----------------------------------------------------|\r| E3:49:01:ED:00:A6 | 0xabc123DEF456ghi789JKL3mnsOPQrst2 | \u0026lt;------ Do not copy the \u0026#39;0x\u0026#39;\r\u0026#43;------------------------------------------------------------------------\u0026#43;\rLogged out.\rIt is important to not reset nor unlink the MB4 from your account. If so, just repeat the steps of this section.\nRestart service After all wanted modifications, restart the service and check its status:\nconsole pi@raspberry:~/$ sudo systemctl restart miband-dc.service\rpi@raspberry:~/$ systemctl status miband-dc.service\r● miband-dc.service - miband-dc service\rLoaded: loaded (/lib/systemd/system/miband-dc.service; enabled; vendor preset: enabled)\rActive: active (running) since Wed 2020-12-02 19:14:47 CET; 7s ago\rMain PID: 23417 (miband-dc)\rTasks: 3 (limit: 9362)\rMemory: 12.6M\rCGroup: /system.slice/miband-dc.service\r└─23417 /usr/bin/python3 /usr/bin/miband-dc -d devices.csv -c settings.json\rdic 02 19:14:47 raspberry bash[23417]: INFO:mi band 1: init dev, mac: E3:49:01:ED:00:A6, static-id: None\rdic 02 19:14:47 raspberry bash[23417]: DEBUG:TaskRunner: remaining tasks: 0\rdic 02 19:14:47 raspberry bash[23417]: INFO:TaskRunner: run task: \u0026lt;Task: DiscoverTask\u0026gt;\rdic 02 19:14:47 raspberry bash[23417]: INFO:DiscoverTask: start discovery process...\rdic 02 19:14:54 raspberry bash[23417]: INFO:DiscoverTask: - discovered 1 devices\rdic 02 19:14:54 raspberry bash[23417]: INFO:DiscoverTask: - got device: \u0026lt;MiBandDev, mac: E3:49:01:ED:00:A6, status: online\u0026gt;\rdic 02 19:14:54 raspberry bash[23417]: DEBUG:TaskRunner: remaining tasks: 0\rdic 02 19:14:55 raspberry bash[23417]: INFO:TaskRunner: run task: \u0026lt;Task: RunFunctionTask-update_static\u0026gt;\rdic 02 19:14:55 raspberry bash[23417]: INFO:mi band 1: connecting to \u0026#39;E3:49:01:ED:00:A6\u0026#39;...\rWith the service running, the daemon is storing the MB4 data on the database provided.\nConfiguring miband-grafana This package exists to automate the Grafana provision with the data stored by miband-dc. In order to create the correct files, miband-grafana must know the connection parameters of the database, so we must edit its configuration file with sudo miband-grafana -e. A configuration can be:\nconfiguration.conf DBNAME=user-stadistics DBUSER=grafana DBPASSWORD=mysecret DBHOST=postgresql.on.cloud.com DBPORT=5432 It is really important that the database user has only CONNECT and SELECT privileges, because the queries executed by Grafana cannot be filtered so any user logged on Grafana UI with admin rights could manipulate the database.\nThen execute:\nconsole user@pc:~/$ sudo miband-grafana -u\rThis will provision our local Grafana instance. If we check http://localhost:3000 we will see Grafana UI, and following on the side menu Dashboards \u0026gt; Manage we can see our provisioned dashboards:\nTo see what files where provisioned and the generated dashboards information use: console user@pc:~/$ sudo miband-grafana -s\rINFO: Datasource provider file at /etc/grafana/provisioning/datasources/miband-grafana.yaml\rINFO: Dashboard provider file at /etc/grafana/provisioning/dashboards/miband-grafana.yaml\rINFO: Found dashboard definition at /var/lib/grafana/dashboards/miband-grafana_dev1.json:\rINFO: Name: mi-band-data-user-1\rINFO: UID: c9b38467\rINFO: Available panels:\rINFO: ID: 11 Description: Steps per day\rINFO: ID: 12 Description: Steps today\rINFO: ID: 21 Description: Calories per day\rINFO: ID: 22 Description: Calories today\rINFO: ID: 31 Description: Heart rate\rNOTE: If you want grafana to be started on boot, execute sudo systemctl enable grafana-server.service.\nSetting up the smart mirror To transform our RPi4 in a smart mirror we will use Magic Mirror. Following its installation instructions, execute:\nconsole pi@raspberry:~/$ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\rpi@raspberry:~/$ sudo apt install -y nodejs\rpi@raspberry:~/$ git clone https://github.com/MichMich/MagicMirror\rpi@raspberry:~/$ cd MagicMirror\rpi@raspberry:~/MagicMirror/$ npm install\rpi@raspberry:~/MagicMirror/$ cp config/config.js.sample config/config.js\rOnce this is done, it is time to add our Grafana module for Magic Mirror, called MMM-GrafanaEmbed:\nconsole pi@raspberry:~/MagicMirror/$ cd modules\rpi@raspberry:~/MagicMirror/modules/$ git clone https://bitbucket.org/arco_group/mmm-grafanaembed.git MMM-GrafanaEmbed\rIt is really important that the final folder is called MMM-GrafanaEmbed, respecting the capital letters.\nFinally, we change Magic Mirror configuration /MagicMirror/config/config.js adding MMM-GrafanaEmbed parameters on modules list. For example:\nsnippet.js modules: [\r// Rest of modules configuration\r{\rmodule: \u0026#39;MMM-GrafanaEmbed\u0026#39;,\rposition: \u0026#39;bottom_center\u0026#39;,\rconfig: {\rhost: \u0026#34;192.168.0.22\u0026#34;, // Our PC IP address\rdash_id: \u0026#39;c9b38467\u0026#39;, // Dashboard uid gotten from miband-grafana -s\rdash_name: \u0026#39;mi-band-data-user-1\u0026#39;, // Dashboard name gotten from miband-grafana -s\rpanels: [11] // Panel ids for number of steps gotten from miband-grafana -s\r}\r}\r]\rThen we run the following command and we will bring our Grafana panel to Magic Mirror:\nconsole pi@raspberry:~/MagicMirror/$ npm run start\rTroubleshooting Grafana is not installable Using the apt program, it can occur that Grafana package is not installable. This is because apt cannot find the source package within its sources lists. To solve this problem, execute the following commands:\nconsole user@pc:~/$ sudo apt-get install -y apt-transport-https software-properties-common wget\ruser@pc:~/$ wget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add -\ruser@pc:~/$ echo \u0026#34;deb https://packages.grafana.com/oss/deb stable main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/grafana.list\ruser@pc:~/$ sudo apt-get update\rNow you can install miband-grafana normally, as apt can find Grafana package now.\nGrafana service is not running after provisioning Sometimes, after we run miband-grafana -u, the Grafana service, grafana-server, stops running and its log does not offer any relevant information about what went wrong with the provision. Do not modify any of the generated files by miband-grafana, they are properly tested, so the problem is with Grafana instance. For the time being, reinstalling Grafana works. So execute:\nconsole user@pc:~/$ sudo apt purge grafana // Warning: This uninstalls miband-grafana package\ruser@pc:~/$ sudo rm -drf /etc/grafana /var/lib/grafana // Dpkg do not remove those if there were custom files on them, so do it manually\ruser@pc:~/$ sudo apt install miband-grafana // Reinstall miband-grafana and grafana\ruser@pc:~/$ sudo miband-grafana -u // Try again!\rmiband-dc: No detection of devices There are some issues with the Bluetooth service that we are not capable of locate yet. miband-dc is programmed to auto-restart the RPi4 Bluetooth service when it does not detect any configured device (the ones specified at devices.csv) within 15 minutes.\nIf it still does not detect your device, you must restart the Bluetooth adaptor. You can do it using the UI, selecting the proper option at the Bluetooth icon up to the right of your screen, or via command-line with sudo hciconfig hci0 reset.\nmiband-dc: \u0026lsquo;Are you root?\u0026rsquo; error miband-dc is running with admin privileges by systemctl, so it is running as root. The problem seems to be related with the Bluetooth adaptor, so a temporary solution is to restart the Bluetooth adaptor, just as is explained in the previous section.\nReferences Grafana Magic Mirror miband-dc(1) miband-grafana(1) MMM-GrafanaEmbed Appendix 1. Configuration for local database If we have installed PostgreSQL locally, either in our RPi4 or in our PC, we must ensure they are in the same network or both hosts have public IP addresses. In these scenarios, either Grafana instance, in our PC, or miband-dc, in our RPI4, will have to access the database, so we must change PostgreSQL configuration to allow remote connections.\nFirst, we will change the value of listen_addresses to \u0026rsquo;*\u0026rsquo; in the file postgresql.conf. If we execute SHOW config_file in our PostgreSQL instance we will know its location. For example:\nconsole user@pc:~/$ sudo su postgres -c \u0026#34;psql -c \u0026#39;SHOW config_file\u0026#39;\u0026#34;\rconfig_file -----------------------------------------\r/etc/postgresql/12/main/postgresql.conf\r(1 row)\ruser@pc:~/$ sudo grep -n listen_addresses /etc/postgresql/12/main/postgresql.conf 59:#listen_addresses = \u0026#39;localhost\u0026#39; # what IP address(es) to listen on;\rWe will change, in the line 59, \u0026rsquo;localhost\u0026rsquo; for \u0026lsquo;*\u0026rsquo; and uncomment the line.\nNext we will open the file pg_hba.conf, whose path is specified in postgresql.conf with the hba_file parameter), and add the next line:\npg_hba.conf # ORIGINAL CONFIGURATION - DO NOT DELETE # [...] # TYPE DATABASE USER ADDRESS METHOD host database user samenet md5 Replace the parameter database with the name of the database, user-stadistics in this recipe.\nIn case your database is hosted by your RPi4, the remote user will be the one used by the Grafana instance, grafana in this recipe. If this host is not in the same net as your RPi4, replace samenet parameter with the public IP address of the Grafana host.\nIf that was not the case and the database is located in your PC, the remote user will be the one used by miband-dc program running on the RPi4, miband-dc in this recipe. If this host is not in the same net as your PC, replace samenet parameter with the public IP address of the miband-dc host.\n","ref":"https://arcogroup.bitbucket.io/shapes/integrating_miband_with_smart_mirror/","summary":"A recipe to learn how to collect data from Xiaomi Mi Band 4 devices and visualize it in a smart mirror.","title":"Integrating Xiaomi Mi Band 4 devices with smart mirror"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/python/","summary":"","title":"python"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/active-ageing/","summary":"","title":"active ageing"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/ble/","summary":"","title":"BLE"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/fall-detection/","summary":"","title":"fall detection"},{"body":"Overview In ARCO Research Group, in the context of the European project H2020 Shapes, we are working in a set of solutions to encourage the active ageing and improve the health condition at an advanced age. One of these solutions is a fall detection system that will allow us to automatically detect when a person has fell, in order to provide a prompt response. The system here presented could be a very powerful and useful tool, taking into account that falls are the leading cause of accidental death.\nWarning! Fundamentals In order to build this system an Intertial Measure Unit (IMU) will be used, specifically a MetamotionR sensor, which was already presented in our preovious recipe, where we explained how to use our library to extract different data from it. This IMU can give us information about the movement with two 3-AXIS sensor, an accelerometer and a gyroscope, but the raw data by itself does not give meaningful information and for this reason we have to preprocess it to get the intended information. This sensor also counts with a magnetometer, that can be useful to calculate the orientation of the sensor, considering the earth\u0026rsquo;s magnetic field. In our case we have decided to place the sensor in the user\u0026rsquo;s waist, because it is the center of gravity of a person, and it is sensitive to abrupt changes in the measurements of the IMU when a fall occurs. You can see underneath the sensor and where it has been placed.\nWe have developed an algorithm which is a mixture of the two types of algorithms used for fall detection, Threshold algorithm and Machine Learning Algorithm. The thresholds algorithm are mainly used in low performance devices. Our threshold algorithm is a Finite State Machine that check if the inertial measures (mainly acceleration) exceed a threshold under certain circumstances, if this requisite is achieved, then a series of features is extracted from the raw data in a window os 1 second. With the previous features, a model is trained to detect whether this can be considered a fall (in our case we are using SVM), so introducing the features extracted to our model we can know if they correspond to a fall. Why do we need a model? Well, this is a good question, and all is about the Activities of Daily Living (ADL).\nAlong the day, we carry out many different activites that can be tracked by our imus, if we only use a threshold algorithm it is possible that a lot of activites that do not involve a fall can overpass the selected threshold, leading to a false positive. To avoid this scenarios, a model previously trained with labeled data can help us to reduce this false-positive case. At the same time, the threshold algoritm can act as a filter, in order to avoid a frequent extraction of features and evaluations by the model.\nIn the data collection, the most important ADL identified that can cause a false positive are the following:\nHitting the sensor Jumping Running Sitting in a chair or a sofa To train the model, we have collected data from the previous ADLs and the following falls:\nBackward fall Left-side fall Right-side fall Front fall All the falls have been made using a mattress, you may think that using a mattress could alter the data, but not using it would also alter it, because pretend falls would be performed with fear. We concluded that the best approach was to use a mattress because the data is less altered due to more natural falls.\nTraining model At this point, we are using only one subject for both training and testing. The environment where the data has being collected was the ARCO Research Group\u0026rsquo;s lab, located in the Technology and System Information Institute (ITSI). Following you can see the mattress used for the falls.\nThe activities carried out in the collection are these:\n10x Hitting the sensor 10x Jumping 10x Running 10x Sitting on a chair or a sofa 5x Backward fall 5x Left-side fall 5x Right-side fall 5x Front fall With this data, we have trained the SVM model that is going to be used for testing purpose.\nTesting algorithm For testing purpose, training activities have been repeated in the following way:\n3x Hitting the sensor 3x Jumping 3x Running 3x Sitting on a chair or a sofa 3x Backward fall 3x Left-side fall 3x Right-side fall 3x Front fall In the next table you can see the results:\nExercise True Positive True Negative False Positive False Negative Hitting the sensor 3 Jumping 3 Running 3 Sitting on a chair 3 Front fall 3 Left-side fall 3 Right-side fall 3 Backward fall 2 1 As you can see, almost all the results are the expected, with the exception of one backward fall, which produce a wrong result. It has to be noted that the tests were made with the same subject used for training the model. To summarize, our system with the tests made has a reliability of 95,83%. In future works, we are going to collect data from more subjects, create a more complete battery of tests, and finally deploy this solutions in a real environment, monitoring the residents from the nursing home El Salvador in order to test our system.\n","ref":"https://arcogroup.bitbucket.io/shapes/fall_detection_system/","summary":"Explanation of the basics about the fall detection system that is in development","title":"Fall Detection System"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/imu/","summary":"","title":"imu"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/machine-learning/","summary":"","title":"machine learning"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/mbientlab/","summary":"","title":"mbientlab"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/sensor/","summary":"","title":"sensor"},{"body":"","ref":"https://arcogroup.bitbucket.io/api/","summary":"\u003cb\u003eAPIs\u003c/b\u003e and other \u003cb\u003etechnical information\u003c/b\u003e about libraries software applications and other projects.","title":"APIs"},{"body":"Library for the control of a MetaMotionR device class index: Accelerometer Color ConnectionError Gyroscope MetaMotion MetaMotionDeviceNotFound Mode WrongMetaDeviceModel metamotion API Documentation Class Accelerometer Class for represent an Accelerometer sensor that is part of a MetaMotionR device This class enable the user to request all the data that is provided by this accelerometer sensor\n__init__ def __init__(self, board, freq, motion_samples, motion_threshold, tap_threshold) Method for initialice the accelerometer sensor with the desired parameters. Also initializing the data, this method subscribe for all the accelerometer signal and active the sensor\nParameters:\nboard [mandatory] this is the MetaMotionR Board where the sensor is located. freq [optional] Frequency at which the acceleration is updated. By default, the frequency is 25Hz what means that will be sent 25 acceleration signales per second. motion_samples [optional] Number of samples that must overpass the motion_threshold in order to consider a motion. motion_threshold [optional] Difference between the samples indicated in motion_samples to consider a motion. tap_threshold [optional] Difference between samples acceleration to consider a tap. active def active(self, acc_freq, motion_samples, motion_threshold, tap_threshold) Method to configure, active and enable all the accelerometer sensors of the device\nParameters:\nacc_freq [optional] Frequency at which the acceleration is updated. By default, the frequency is 25Hz what means that will be sent 25 acceleration signales per second. motion_samples [optional] Number of samples that must overpass the motion_threshold in order to consider a motion. motion_threshold [optional] Difference between the samples indicated in motion_samples to consider a motion. tap_threshold [optional] Difference between samples acceleration to consider a tap. get_callback def get_callback(self, attr, handler) Generic method to generate a callback that wil be executed when a signal from a sensor is received\nParameter:\nattr [mandatory] this is the class atribute where the data recived by the signal is going to be saved. handler [mandatory] a user handler to process the data once received. init_measures def init_measures(self) Subscribe the signal of the accelerometers measures in order to update the data\non_acceleration def on_acceleration(self, func=None) Add a handler to process the data received by the signal. If there is no handler, the data is saved in the attribute\nParameter:\nfunc [optional] a handler to process the data. on_motion def on_motion(self, func=None) Subscribe to motion detector data and attach a handler that receives the motion as a parameter.\nParameter:\nfunc [optional] a handler to process the data. on_tap def on_tap(self, func=None) Subscribe tap data and attach a handler that receives tap as a parameter\nParameter:\nfunc [optional] a handler to process the data. read_acceleration def read_acceleration(self) Return the last acceleration detected by the sensor. The acceleration is formed by the following fields:\nx: Acceleration in X axis in Gs units. y: Acceleration in Y axis in Gs units. z: Acceleration in Z axis in Gs units. read_motion def read_motion(self) Return the last motion detected by the sensor. The motion is formed by the following fields:\nsign: Axis direction where the sensor is moving. x_axis_active: 1 if the motion was produced in the X axis. y_axis_active: 1 if the motion was produced in the Y axis. z_axis_active: 1 if the motion was produced in the Z axis. read_step_counter def read_step_counter(self) Return the number of steps that the user has made\nread_tap def read_tap(self) Return the last tap detected by the sensor. The tap is formed by the following fields:\nsign: Axis direction where the tap is detected. type: 2 if only 1 tap is detected, 1 if 2 taps. setup_logger def setup_logger(self) Create and initialize an accelerometer logger\nsubscribe_logged_data def subscribe_logged_data(self) Subscribe to the accelerometer logger in order to receive the acceleration data when the download action is performed\nsubscribe_signal def subscribe_signal(self, attr, signal_subscriber, func=None) Method to subscribe a client to a signal, indicating the attribute where the data received must be saved, the method to subscribe to the signal and the user function that will process the data. If no function is indicated, the data will be saved only\nParameter:\nattr [mandatory] this is the class atribute where the data recived by the signal is going to be saved. signal_subscriber [mandatory] is the metawear function that enable the user to receive data from a signal. func [optional] a user handler to process the data once received. wait_until_download_completed def wait_until_download_completed(self) Wait until the acceleration data is fully downloaded\nClass Color Class to save constants about color LEDS. The three constants are GREEN, RED, and BLUE\nClass ConnectionError A exception to inform that a connection with a concrete sensor failed\nClass Gyroscope Class for represent a Gyroscope of a MetaMotionR device, this class enables the user to gather the rotation data from the gyroscope sensor\n__init__ def __init__(self, board, freq) Method to initalize the gyroscope client. The initialization include the activation of the gyroscope and subscription in order to update the attributes\nactive def active(self, freq) Active and configure the gyroscope sensor with default configuration\nget_callback def get_callback(self, attr, handler) Generic method to generate a callback that wil be executed when a signal from a sensor is received\nParameter:\nattr [mandatory] this is the class atribute where the data recived by the signal is going to be saved. handler [mandatory] a user handler to process the data once received. init_measures def init_measures(self) Subcribe to rotation data in order to update the attribute saving the data\non_rotation def on_rotation(self, func=None) Add a handler to process the data received by the signal. If there is no handler, the data is saved in the attribute self.gyro\nParameter:\nfunc [optional] a handler to process the data. read_rotation def read_rotation(self) Return the last rotation detected by the sensor. The rotation is formed by the following attributes:\nx: Rotation in the X axis in degrees per second. y: Rotation in the Y axis in degrees per second. z: Rotation in the Z axis in degrees per second. setup_logger def setup_logger(self) Create and initialize a rotation logger\nsubscribe_logged_data def subscribe_logged_data(self) Subscribe to the rotation logger in order to receive the rotation data when the download action is performed\nsubscribe_signal def subscribe_signal(self, attr, signal_subscriber, func=None) Method to subscribe a client to a signal, indicating the attribute where the data received must be saved, the method to subscribe to the signal and the user function that will process the data. If no function is indicated, the data will be saved only\nParameter:\nattr [mandatory] this is the class atribute where the data recived by the signal is going to be saved. signal_subscriber [mandatory] is the metawear function that enable the user to receive data from a signal. func [optional] a user handler to process the data once received. translate_frequency def translate_frequency(self, desired_freq) Translate a numeric frequency in the corresponding macro\nwait_until_download_completed def wait_until_download_completed(self) Wait until the rotation data is fully downloaded\nClass MetaMotion This class is used for request and add handlers for MetaMotionR sensor This is an abstraction of the Mbientlab Metawear API. This class has two main attributes that represents the sensors that formed the device:\naccelerometer: Instance of the Accelerometer class gyroscope: Instance of the Gyroscope class __init__ def __init__(self, mac_address=None) MetaMotion constructor. This method is the start point of the library and must be invoked in order to use the library characteristics\nParameters:\nmac_address, [optional] the MAC addres of the MetaMotion sensor that will be used in order to connect it by blutooth. check_model def check_model(self, model) Check if the model of the connected device is the MetaMotionR\nParameters:\nmodel, [mandatory] the model of the device connected. connect def connect(self) Method to make the bluetooth connection with the MetaMotion sensor. It includes some necessary initialization for the library, like create an instance for the sensors and subscribe to some events. Also, check that the model of the sensor that we are connecting is the properly (MetaMotionR)\ndisconnect def disconnect(self) Disables all the sensors of the MetaMotionR device, and clean and disconnect it.\ndownload_logs def download_logs(self) Download the data saved in the logs previosuly created in a MetaMotion device\nget_device_info def get_device_info(self) Returns a string with the basic information of the device\nis_connected def is_connected(self) Method to check if the MetaMotion sensor is correctly connected. If the sensor is correctly connected return True, else return False\nread_battery def read_battery(self) Return information about the battery of the device. It has two fields:\nvoltage: Voltage that feed the device. charge: Percentage of battery that the sensor has. scan_metamotion_sensor def scan_metamotion_sensor(cls) Method of the class that enable te user to scan MetaMotionR sensor. After the scanner this method return a list with the address of all the sensors found. If the list is empty there is no MetaMotionR devices nearby\nsetup_accelerometer def setup_accelerometer(self, freq=25, motion_samples=4, motion_threshold=0.7, tap_threshold=1) Initialize the accelerometer sensors that is part of the MetaMotionR device. Is mandatory to invoke this method before use any funcionalities of the accelerometer.\nParameters:\nfreq [optional] Frequency at which the acceleration is updatea. By default, the frequency is 25Hz what means that will be sent 25 acceleration signales per second. motion_samples [optional] Number of samples that must overpass the motion_threshold in order to consider a motion. motion_threshold [optional] Difference between the samples indicated in motion_samples to consider a motion. tap_threshold [optional] Difference between samples acceleration to consider a tap. setup_gyroscope def setup_gyroscope(self, freq=25) Initialize the gyroscope sensor that is part of the MetaMotionR device. Is mandatory to invoke this method before use any funcionalities of the gyroscope.\nParameters:\nfreq [optional] Frequency at which the rotation is updated. By default the frequency is 25Hz. The library will try to assign the frequency closest to that indicated by the user and allowed by the sensor. start_logging def start_logging(self) Start to logging the data\nstop_logging def stop_logging(self) Stop data logging.\nsubscribe_battery def subscribe_battery(self) Subscribe for battery signal in order tu update the battery level attribute\nturn_off_led def turn_off_led(self) Turn off the led of the sensor\nturn_on_led def turn_on_led(self, mode, color) Turn on the led of the sensor. It has tow parameters\nmode: Mode in which the led is going to turn on. There are 3 different modes Mode.BLINK, Mode.PULSE and Mode.SOLID color: Color of the led, which could be Color.RED, Color.GREEN and Color.BLUE Class MetaMotionDeviceNotFound After a scanner, MetaMotion device was not found\nClass Mode Class to save constants about the led modes. There are three modes, BLINK, PULSE and SOLID\nClass WrongMetaDeviceModel Exception which indicates that the sensor connecte is not a MetaMotionR device\n","ref":"https://arcogroup.bitbucket.io/api/bosch-sensor/","summary":"Python library to connect with the MetaMotion chip","title":"Library for the control of a MetaMotionR device"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/bosch/","summary":"","title":"bosch"},{"body":"Overview MbientLab is a manufacture of different wearable devices, an example of these are the Meta sensors family, which is formed by different wearable devices like MetaTracker, MetaMotionC and MetaMotionR. The Meta family devices are formed by different sensors, like Accelerometers, Gyroscopes, Barometes, etc\u0026hellip; In this case, we are going to see how to use a Python library for the MetaMotionR, which enable the user to read the sensors of this device. The measures that can be read with the Library are the acceleration, motion, tap, step counter by an Accelerometer and the rotation by a Gyroscope.\nIngredients In order to follow the next recipe you will need the this ingredients:\nA MetaMotionR device The linux packages bluez, build-essential, libboost-all-dev, libbluetooth-dev which enables the Bluetooth communication and provides the necessary compilers The metawear package, a library created for MbientLab to read sensors measures The bleak library in order to discover BLE devices The metamotion library available in ARCO\u0026rsquo;s package repository Basic knwoledge of Python3 and Linux In order to install al the dependencies you must execute the next commands\nconsole sudo apt install bluez build-essential libboost-all-dev libbluetooth-dev pip3 install metawear --upgrade pip3 install bleak sudo apt install metamotion In the following sections, we are going to explain the main funcionalities of the library, read a value from a sensor, attach a handler to a value produced by the sensor and create logs of data that will be downloaded lately. Almost every measure of the sensor can be readed, but not all the measures support the use of a handler.\nConnecting and Reading data from the device In this section, we are going to explain how you can read the battery from a MetaMotionR board. The first step is import the metamotion library in the following way.\nsnippet.py #!/usr/bin/python3 from metamotion import MetaMotion Warning! Once you import the library you must instantiate your chip. In order to do that you must call the constructor MetaMotion with the MAC address of the chip, which can be observer in the label of the device. Once the sensor is instantiated, call the connect() method in order to establish the Bluetooth connection\nsnippet.py sensor = MetaMotion(\u0026#34;E9:75:41:AF:11:AE\u0026#34;) sensor.connect() Finally you can read any value of the chip, in this case we are going to read the remanent percentage battery invoking the method read_battery of the class. This method returns a struct that is formed by two fields, voltage, which represents the voltage that receives the chip, and battery which is the percentage of battery that the device has.\nsnippet.py measure = sensor.read_battery() print(measure.battery) Before the end of the program is important to invoke the disconnect method, in order to close the communication.\nsnippet.py sensor.disconnect() The full code is listed here below\nsnippet.py #!/usr/bin/python3 from metamotion import MetaMotion sensor = MetaMotion(\u0026#34;E9:75:41:AF:11:AE\u0026#34;) sensor.connect() measure = sensor.read_battery() print(measure.battery) sensor.disconnect() Warning! Read a measure from a sensor Like it was said before, the MetaMotionR device is formed by several sensors. The most important sensors, and those to which the library gives access to, are the Acceleromter and the Gyro sensor. In order to access to the measures of this sensors, a MetaMotion instance has two attributes representing it, accelerometer and gyroscope. Following, we are going to see an example in which we are going to read the acceleration.\nIn order to read the acceleration, the first step is to active and initialize the accelerometer. For this purpose you must invoke the method setup_accelerometer with the aim of active and initialize the accelerometer sensor. If you don\u0026rsquo;t do this, and you try to access the accelerometer sensor, an exception will be raised.\nsnippet.py sensor.setup_accelerometer() Once initialized the sensor, you can read the acceleration invoking the method form the accelerometer read_acceleration, which returns the acceleration calculated by the sensor in G units.\nsnippet.py sensor.accelerometer.read_acceleration() The full code is listed below\nsnippet.py #!/usr/bin/python3 from metamotion import MetaMotion sensor = MetaMotion(\u0026#34;E9:75:41:AF:11:AE\u0026#34;) sensor.connect() sensor.setup_accelerometer() measure = sensor.accelerometer.read_acceleration() print(measure) sensor.disconnect() Add a handler to a measure With the MetaMotion library you can read any values produced by the sensor, but this library also provides the funcionality to subscribe to a measure and attach a handler. In this way, if the sensor which we are subscribed send a value, the handler will be executed and will process the data with the user function.\nIn this section we are going to see an example where we add a handler to an accelertion sample.\nThe first step is to declare the handler that will process the data, in this case the handler is going to print the data.\nsnippet.py def handler(data): print(f\u0026#34;The acceleration is: {data.x} {data.y} {data.z}\u0026#34;) Once the handler is defined, you must attach it to the acceleration produced by the sensor, so that when an acceleration measure is received, the user function will be executed. The acceleration is produced by an accelerometer in the MetaMotionR device, for this reason, to add the handler you must call the on_acceleration method of the accelerometer attribute in your instance of the MetaMotion class. To avoid the end of the program, the method wait_until_break must be called. The full program can be seen below.\nsnippet.py #!/usr/bin/python3 from metamotion import MetaMotion def handler(data): print(f\u0026#34;The acceleration is: {data.x} {data.y} {data.z}\u0026#34;) sensor = MetaMotion(\u0026#34;E9:75:41:AF:11:AE\u0026#34;) sensor.connect() measure = sensor.accelerometer.on_acceleration(handler) sensor.wait_until_break() sensor.disconnect() Using a logger An interesting funcionality of the MetaMotionR devices is the posibility to create loggers. A logges make the users ables to register a set of data from a sensor in the device, and lately download all this data, avoiding maintaining a continuous streaming. This is very useful if you don\u0026rsquo;t want to communicate with the sensor in each measure. In the following example, we are going to create a logger for a rotation measure\nAs in all previous examples, the first step is to made the connection with the sensor. Once the connection is made, and beacuse we are going to log a measure from the Gyro, we must invoke the method setup_gyroscope and setup_logger from the gyroscope attribute to initialize the sensor and the logger. After that, invoke the method start_logging in order to start to save the data. Following, you can disconnect from the sensor until you want to download the data.\nsnippet.py sensor.gyroscope.setup_gyroscope() sensor.gyroscope.setup_logger() sensor.start_logging() sensor.disconnect() Once you want to download data, connect to the sensor again and invoke the methods stop_logging in order to stop registering data.\nsnippet.py sensor.stop_logging() At this point, there is a problem, our instance doesn\u0026rsquo;t know which is the status of the sensor, which loggers were created, and which signal were registering. To solve this problem, can be used the method sync_host_with_device, that synchronize our instance with the actual status of our MetamotionR device. Now our instance knows which logger exists, but doesn\u0026rsquo;t knows which signals were subscribed, for this reason, and to download all data, you must invoke the method subscribe_anonymous_datasignals. Once our instance is prepared, download the data is as simple as call the method download_logs to get all the data from the device, and wait_until_download_completed, to avoid the finish of the program until all the data is downloaded. Finally, the downloaded data is in a list, in this case in the attribute rotation_log from the gyroscope.\nsnippet.py sensor.stop_logging() sensor.gyroscope.subscribe_logged_data() sensor.download_logs() sensor.gyroscope.wait_until_download_completed() print(\u0026#34;The rotation is \u0026#34;, sensor.gyrocope.rotation_log) Finally, is needed to clean the logger created in the device before disconnection.\nsnippet.py sensor.clean() sensor.disconnect() The full code is listed bellow\nsnippet.py #!/usr/bin/python3 import sys import time from metamotion import MetaMotion sensor = MetaMotion(\u0026#34;E9:75:41:AF:11:AE\u0026#34;) sensor.connect() sensor.gyroscope.setup_gyroscope() sensor.gyroscope.setup_logger() sensor.start_logging() sensor.disconnect() time.sleep(4) #Wait 4 seconds to let the sensor log data sensor = MetaMotion(\u0026#34;E9:75:41:AF:11:AE\u0026#34;) sensor.connect() sensor.stop_logging() sensor.sync_host_with_device() sensor.subscribe_anonymous_datasignals() sensor.download_logs() sensor.wait_until_download_complete() print(\u0026#34;The rotation is \u0026#34;, sensor.gyroscope.rotation_log) sensor.clean() sensor.disconnect() You can use as many logs as you want, but you must follow the next workflow:\nConnect the sensor Setup the necessary logger in their respective sensor from the chip Start the logging Wait a desired amount of time Connect with the sensor again Stop the logging Sync the client with the device status Subscribe to anonymous datasignals Download the logs Wait for each sensor to download its log data Clean and disconnect from the sensor MetaMotion API For more information you can see the API reference where you can find how to read all the measures available and the different methods to attach handlers:\nMetaMotion API Reference ","ref":"https://arcogroup.bitbucket.io/shapes/integrating_metamotionr_with_python/","summary":"A recipe to learn how to use the MetaMotion library, and read data from the sensors of the MetaMotionR board usin Python","title":"Integrating MbientLab MetaMotionR sensors with Python"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/amazfit/","summary":"","title":"amazfit"},{"body":"Overview Under the Amazit branch name, there are a lot of devices. Among them, Xiaomi sells \u0026lsquo;smart\u0026rsquo; shoes (or sneakers) that have a device inside which measures some variables and also counts the steps. The device could be purchased alone, and is known as Amazfit 2 Smart Chip or Dynamic Core. In this recipe, we will use a Python library to connect to that device, and acquire the provided information.\nIngredients Of course, to follow this recipe you will need the device itself, but also the following packages (assuming that you are on GNU/Linux):\npython3-bluez: generic support for Bluetooth python3-gattlib: specific support for BLE and GATT protocols. More information on bitbucket.org/OscarAcena/pygattlib amazfit2-dynamic-core: specific library for this device The easiest way to install all these packages is using the pike Debian repository. To install it:\nconsole wget -qO- http://pike.esi.uclm.es/add-pike-repo.sh | sudo sh sudo apt update Then, install with apt the needed packages:\nconsole sudo apt install python3-bluez python3-gattlib amazfit2-dynamic-core Discovering and Connecting The first step you would do in order to check if your device is ready for connection (has enough battery, is awake, is near your Bluetooth transceiver, etc.) is a discovery.\nWarning! So, first things first, import the Smart Chip library into your script. There is only one interesting class, called Core2, so:\nsnippet.py #!/usr/bin/python3 from amazfit import Core2 Now, you can use the method find_all() (which is a @classmethod) to perform the discovery. It will try to find all the BLE (Bluetooth Low Energy) devices near you, and then filter them by its vendor ID. Because this is an endless operation, you can specify the maximum time you want to wait for responses (by default is 5 seconds), using the argument timeout.\nfind_all() will return a list with the found devices. A similar operation is find(), which only returns the first result in the list:\nsnippet.py chip = Core2.find(timeout=10) if chip is None: print(\u0026#34;ERROR: no device found!\u0026#34;) Next step is connect to the device. This will create a communication \u0026ldquo;channel\u0026rdquo; between your computer and the chip, used to read sensor values, receive notifications, etc. For this purpose, call the method connect(), which does not have any argument:\nsnippet.py chip.connect() If it returns normally, the connection has been stablished. If something went wrong (maybe the device is asleep\u0026hellip;), it will raise a RuntimeError with a message about the problem.\nAbout pairing After the connection has been stablished, you can run a bonding or pairing process. It will be used to exchange a cyphering key with the device, which is the way to validate client\u0026rsquo;s identity. Also, it will reset the step counter to zero.\nAlthough in many devices this is a critical part (otherwise, the device will disconnect or ignore your requests), on the Smart Chip is not required to read sensor values or receive notifications. If you need it, just use the method pair(), which does not accept any parameter:\nsnippet.py chip.pair() During this stage, the chip will blink a red led to indicate that you must shake the device in that moment. If you don\u0026rsquo;t do it, then the pairing will fail. Otherwise, it will be successful, and you can read the exchanged key in the property key. Store it for later usage:\nsnippet.py print(f\u0026#34;Pairing key: {chip.key}\u0026#34;) Reading Static Information The Smart Chip holds some specific static information that barely changes, like the device address, its name, product id, vendor id, and so on. All this data could be read using the method read_info(). This method will contact with the device and retrieve all those fields. Then, you can access each one using its specific property. For a complete list of the fields, please see the API reference. The following is an example of use:\nsnippet.py chip.read_info() print( f\u0026#34;Device Information:\\n\u0026#34; f\u0026#34;- name: {chip.name}\\n\u0026#34; f\u0026#34;- address: {chip.address}\\n\u0026#34; f\u0026#34;- serial number: {chip.serial_number}\\n\u0026#34; f\u0026#34;- hardware version: {chip.hardware_rev}\\n\u0026#34; f\u0026#34;- software version: {chip.software_rev}\u0026#34; ) Reading Step Count There are some other values that could be read from the device, but are not considered static information because they change along the time: the step counter and the device orientation. Let\u0026rsquo;s review first the steps.\nThe Amazfit library exposes the step counter as a property, steps. Each time is accessed, the library will read it from the device (no caching is done). So, as an example:\nsnippet.py print(f\u0026#34;- steps: {chip.steps}\u0026#34;) Now, you may put the device inside your shoes and walk. Then, run the program again, which should return a different result.\nOf course, it is very cumbersome (and battery draining) to poll the value every few seconds. For this reason, you can use a push-notification system. The library will call a user-configured callback each time the counter changes. To setup this callback, use the method on_steps(), which accepts a single parameter: the callback function with the following signature:\nsnippet.py def on_steps_callback(steps) Of course, this push mechanism is asynchronous, which means that once the callback is set, the on_steps() method will return, allowing you to do other things. If you just want to wait for incoming events, you can create a waiting event loop, or just call the library\u0026rsquo;s builtin with wait_until_break().\nThe following example will print every new update on the step counter:\nsnippet.py def on_steps_callback(steps): print(f\u0026#34;- steps changed, new value: {steps}\u0026#34;) chip.on_steps(on_steps_callback) chip.wait_until_break() Reading Chip Orientation Another dynamic value that could be read from the device is the orientation, which are a pair of values representing the angle of (X, Y) axis (it is given as a tuple). Once again, you retrieve it using a property that will be read from the device each time is used:\nsnippet.py print(f\u0026#34;- axis: {chip.orientation}\u0026#34;) It will take a little to acquire this value, and could not be read very frecuently.\nWarning! Again, you can also configure a callback to be called when this value has changed (using the on_orientation() method), but, you are warned, no new notifications will be received unless someone fires a read request, with the method fire_orientation_read(). See the following example:\nsnippet.py print(f\u0026#34;- axis: {chip.orientation}\u0026#34;) def on_orientation_callback(axis): print(f\u0026#34;- orientation changed, new value: {axis}\u0026#34;) chip.on_orientation(on_orientation_callback) while True: try: chip.fire_orientation_read() time.sleep(2) except KeyboardInterrupt: break References This is a list of useful resources:\nAmazfit 2 Chip API Reference ","ref":"https://arcogroup.bitbucket.io/recipes/amazfit_2_integration_with_python/","summary":"Recipe explaining how to connect and use the Amazfit 2 Smart Chip device using Python.","title":"Amazfit 2 Dynamic Core: Integration with Python"},{"body":"","ref":"https://arcogroup.bitbucket.io/recipes/","summary":"All common recipes that does not belong to any specific project (i.e documentation, hardware, ...).","title":"Recipes"},{"body":"Amazfit 2 Chip API Reference class index: Core2 amazfit API Documentation Class Core2 This class provides access to the Huami Smart Chip (version 2), which could be found on Amazfit or Xiaomi sneakers. Use this class to discover a new device, connect and pair to it, retrieve device information, orientation, steps, etc.\nFor some of the properties, it also supports a push mechanism which allows to receive notifications when the property changes.\nbattery [read-only property] Holds the current battery level, in percentage, of the device. It will retrieve it (synchronously) when used (no caching).\nNOTE: due to a bug in the device firmware, this characteristic may not return any payload. In that case, None will be returned (an error will also be logged).\ncurrent_time [read-only property] It returns the current date and time configuration of the device, in string format. It will retrieve it (synchronously) when used (no caching).\norientation [read-only property] Returns the IMU orientation readings of the device. It is a tuple with two fields, one for each axis.\nFIXME: semantic of these fields is to be determined!\nsteps [read-only property] Provides the current step count of the device. It will retrieve it (synchronously) when used (no caching).\n__init__ def __init__(self, address, name=\u0026#39;\u0026#39;, key=None) Smart Chip (a.k.a Dyanmic Core 2) constructor. This is the basic object you need to instantiate. It could be created directly (providing the device address) or performing a discovery with the .find() method (explained later).\nParameters:\naddress, the Bluetooth MAC address of the device to connect. It should be in the form aa:bb:cc:dd:ee:ff (case insensitive). name, [optional] the name of the device. It also could be obtained later from the device itself. key: [optional] the encription key (AES/ECB) used to pair with the device. If not provided, it will be generated randomly. connect def connect(self) Try to connect to the device, using the provided address. Returns when then connection is established. If the device is not present (before the timeout expires), it will raise an exception. This method must be called before any other operation.\nfind def find(cls, timeout=5) Classmethod to search compatible devices. It will return the first found.\nParameters:\ntimeout: (default: 5), maximum elapsed time to wait for devices to answer discovery. find_all def find_all(cls, timeout=5) Classmethod to search compatible devices. It will return all the devices found.\nParameters:\ntimeout: (default: 5), maximum elapsed time to wait for devices to answer discovery. fire_orientation_read def fire_orientation_read(self) Forces a new read on the orientation characteristic. You should call on_orientation() before, in order to setup a notification callback.\non_orientation def on_orientation(self, cb) Enables notifications on orientation handle, and configures a callback to be called when a new notification arrives.\nParameters:\ncb: the function or method to be called. The signature should be as follows: def callback([self,] axis) NOTE: the orientation characteristic will not issue new notifications when the value has changed. To receive them, you should force a new event with the fire_orientation_read() method.\non_steps def on_steps(self, cb) Enables notifications on step readings, and configures a callback to be called when a new notification arrives.\nParameters:\ncb: the function or method to be called. The signature should be as follows: def callback([self,] steps) pair def pair(self) Bond with this device. It will exchange the private key used to cipher messages with the chip (although it is not necessary for every operation). This procedure also will reset the step counter in the device.\nread_info def read_info(self) Retrieve all the static information provided by the device. After calling this method, you can access the following attributes:\nname: the device name (if provided on constructor, it will be updated) appearance: a number which identifies the type of this device (like \u0026lsquo;Generic Phone\u0026rsquo;, \u0026lsquo;Sports Watch\u0026rsquo; or \u0026lsquo;Heart rate Sensor\u0026rsquo;). privacy_flag: indicates if this device has privacy enabled or disabled. min_connection_int: Preferred connection parameter, BLE minimum connection interval. max_connection_int: Preferred connection parameter, BLE maximum connection interval. slave_latency: Preferred connection parameter, BLE Slave latency. timeout_mult: Preferred connection parameter, BLE connection supervision timeout multiplier. service_changed_start: Start of affected GATT attribute handle range. service_changed_end: End of affected GATT attribute handle range. system_id: This is a 64-bit structure which consists of a 40-bit manufacturer-defined identifier concatenated with a 24 bit unique Organizationally Unique Identifier. serial_number: The serial number of this device. hardware_rev: Hardware revision number. software_rev: Software revision number. vendor_src: Bit which identifies the source of the Vendor ID: Bluetooth SIG or USB implementer. vendor_id: Identifies the product vendor from the namespace in the Vendor ID Source product_id: Manufacturer managed identifier for this product product_ver: Manufacturer managed version for this product wait_until_break def wait_until_break(self) Blocks the calling thread execution until it receives a KeyboardInterrupt (Ctrl+C) Use it (or use instead an event loop) to keep the application running while you wait for incoming events.\n","ref":"https://arcogroup.bitbucket.io/api/amazfit_2_dynamic_core/","summary":"Python library to connect with the Amazfit Smart Core 2","title":"Amazfit 2 Chip API Reference"},{"body":"Ingredientes Esta receta está pensada para ser usada con un demostrador o servicio que está virtualizado usando Vagrant. Partiremos asumiendo que el servicio está correctamente desplegado y funcionando.\nCrear el box de vagrant Para crear el box, solo es necesario ejecutar el siguiente comando:\nconsole $ vagrant package --output my-application.box Esto generará un fichero con el nombre indicado, que podrá ser usado para crear nuevas instancias de este Vagrant. El fichero contiene el sistema entero tal y como esta en este momento, por lo que quizá ocupe más de 1 GiB (dependiendo del contenido de la VM).\nCrea una carpeta en algún sitio, y ponle como nombre la fecha de hoy. Utilizaremos esa carpeta para guardar todo lo relacionado con esta instantánea. Mueve el .box a esa carpeta:\nconsole $ mkdir 20190823 $ mv my-application.box 20190823 Ahora necesitarás incluir el Vagrantfile: cópialo en la carpeta que has creado. Y dado que el despliegue ya está hecho dentro de la VM, es necesario que comentes (o borres) las líneas relacionadas. En mi caso:\nVagrantfile ... # config.vm.provision :ansible, playbook: \u0026#34;provision.yaml\u0026#34; ... También deberás cambiar la configuración relativa al box que vas a emplear. Debes indicarle que será el que hay en esta misma carpeta (y que acabas de crear). Para ello, edita la siguiente línea (usa el nombre de tu .box):\nVagrantfile config.vm.box = \u0026#34;my-application.box\u0026#34; Reunir las dependencias Si bien es cierto que esta máquina no necesita despliegue, es probable que tengas algún fichero compartido en el host. Por ello, asegúrate de copiar al directorio donde está el Vagrantfile todos aquellos ficheros que necesites. Por ejemplo, si mi maquina necesita un fichero llamado Makefile, que asumo que se montará en /vagrant automáticamente, es necesario que lo copie al directorio destino:\nconsole $ cp Makefile 20190823 Por otro lado, verifica que haces lo mismo con aquellas carpetas que se han compartido de forma explícita. Por ejemplo, si tienes un synced_folder \u0026quot;src\u0026quot; en tu Vagrantfile, copia ese directorio también (y si la ruta era relativa y en niveles superiores, actualiza el Vagrantfile como veas apropiado)\nUna vez hecho esto, comprueba que todo funciona correctamente arrancando la máquina desde el directorio que has creado, y verificando que los servicios / dependencias se cumplen.\nIndicar que el servicio tiene snapshots Lo siguiente que debes hacer es guardar el directorio que has creado en algún sitio accesible. De esa forma, estará disponible cuando haga falta. Y por último, debes indicar que este servicio está disponible en una instantánea. Para ello, añade al repositorio el siguiente fichero, y modifícalo para incluir la ruta adecuada:\nSNAPSHOTED This repository (with its deployed services on Vagrant) has been SNAPSHOTED, which means that there are one or more images with an already deployed state. The goal of this is to get an always-running system, despite software updates or source changes. Please, see the following location to get the snapshots: * ssh://arco.esi.uclm.es/users/oscar.acena/time-machine/my-application Referencias Vagrant: Creating a Base Box ","ref":"https://arcogroup.bitbucket.io/recipes/creating_vagrant_snapshots/","summary":"Esta receta explica como crear imágenes de un servicio para mantenerlo funcionando siempre.","title":"Cómo crear snapshots de vagrant"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/devops/","summary":"","title":"devops"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/recipe/","summary":"","title":"recipe"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/vagrant/","summary":"","title":"vagrant"},{"body":"Overview Ian is a very useful tool that facilitates the task of builiding and uploading packages to a repository. Building packages in debian could be a very annoying task, thanks to Ian, you can build your package barely with a few commands.\nSome of the mos relevant commandas that we are going to use are ian create, ian build and ian upload. In this recipe the focus will be put in how to build a package, at the end of the recipe a link you can find a link to a recipe that explain how tu upload a package to the arco repository.\nIngredients In order to follow this recipe, you will need to satisfy the following requirements:\nThe gpg and ssh package. The ian package. A Debian/Ubuntu Linux. Basic knowledge of Linux and Makefiles. In order to install the tool ian you must execute the following command.\nconsole $ sudo apt install ian Configure Ian The first step after the installation of ian is to configure it. This step is as simple as modify the file ~.config/ian/config with the next content. AS you can see, is required a gpg key, to sign the packages built.\nsnippet.config DEBFULLNAME=\u0026#34;Jesús Fernández-Bermejo Ruiz\u0026#34; DEBEMAIL=jesus.fbermejo@gmail.com DEBSIGN_KEYID=\u0026lt;gpg_key\u0026gt; DEBREPO_URL=debrepo@debrepo/shared EDITOR=\u0026#34;emacs -nw\u0026#34; Preparaing the building Now, is time to build your package. The first step is create a directory with the named of your package (for example example-package) where you are going to save the source files of your program. In this example case, we are going to package a python program that consists in a file named example.py.\nOnce you moved your source code, you have to create a Makefile. This Makefile must contains two directives, build and install, build is an optinal directive and contains the actions needed to compile the software, in the other hand, the directive install contains the actions needed to install the executable file of your program.\nDue to python is an interpreted programming language, the compilation is not needed, so only the install directive is mandatory. The Makefile would be the following.\nsnippet.make DESTDIR ?= ~ install: install -vd $(DESTDIR)/usr/bin install -v -m 555 example.py $(DESTDIR)/usr/bin/example The directive install performs the following tasks. Firstly if the directory ~/usr/bin doesn\u0026rsquo;t exist it is created, after that, it copies the python file to the specified directory with the properly permissions (read and execute permissions), converting the word example in a command that execute the example.py program.\nAfter the Makefile is created, the following step is execute the ian create command, this command will create a folder named debian with a default information about the package. In the debian directory is saved all the information required to build the package, and is formed by different files and directories, some of them optionals. The most important files are the following:\ncontrol This file saves all the information that help other tools to manage the package. An example of a control file can be found bellow.\nsnippet. Source: metamotion Section: python Priority: optional Maintainer: Jesús Fernández-Bermejo Ruiz \u0026lt;jesus.fruiz@uclm.es\u0026gt; Build-Depends: debhelper (\u0026gt;= 12), quilt, python3, libbluetooth-dev, dh-python Standards-Version: 4.3.0 Package: metamotion Architecture: all Depends: ${misc:Depends}, python3, ${python3:Depends} Description: Python library to control MetaMotionR devices This package present a useful library that enable the users to request differente data from the MetaMotionR sensor, like the acceleration and rotation of the sensor. In the first seven lines, is added the control information of the package. The firs line indicates the name of the source package, the second the section inside the distribution where the package can be found, some section examples are devel for programming tool, main for open source programs, non-free for propietary code\u0026hellip; etc. The tird line describes the relevance of the package for the user, the fourth the maintainer of the package and the fifth the list of packages needed to build our package.\nIn the second paragraph, the first line is the name of the binary package, the second the architectures where the package can be compile, the third line is a list of packages needed for our package instalation, if some of that packages are not installed, our packaged will not be installed. In the fourth line, is added a short and a long description.\ncopyright This file contains information about the resources, license and copyright of the original sources of the package, in a simple use, isn\u0026rsquo;t necessary to modify this file.\nchangelog This is a mandatory file with a special format, and also a good place where document all the changes on your source code. A simple example of a changelog is the following\nsnippet. gentoo (0.9.12-1) unstable; urgency=low * Initial Release. Closes: #12345 * This is my first Debian package. * Adjusted the Makefile to fix $(DESTDIR) problems. -- Josip Rodin \u0026lt;joy-mg@debian.org\u0026gt; Mon, 22 Mar 2010 00:37:31 \u0026#43;0100 In the first line, and in this order, is presented the name of the package, its version, the distribution where the pacakge can be installed (should be unstable) and the urgency. Following, you can see a log entry with the changes for the new version of the package and at the end, information about the author of the changes and a timestamp.\nrules This is a Makefile that will be executed at the time to build the package.\nBuilding the package Finally and once all the information is added to debian directory, builduing the package is as simple as executing the command ian build -c. After this command, all the files of your package will be created. If you want to know more about ian, the command ian help will tell you all the options that this tool have. If you want to know more about building debian packages I recommend you to read the debian maintainer\u0026rsquo;s guide and specially the fourth chapter.\nUploading the package For the package upload, you can find all the information about how to upload a package to the ARCO\u0026rsquo;s repository here. You need to have a GitHub account with permission.\n","ref":"https://arcogroup.bitbucket.io/recipes/building_packages_with_ian/","summary":"Recipe explaining how to build and upload a debian package to a package repository","title":"Building Packages with Ian"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/debian/","summary":"","title":"debian"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/ian/","summary":"","title":"ian"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/packages/","summary":"","title":"packages"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/c/","summary":"","title":"C"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/icec/","summary":"","title":"icec"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/make/","summary":"","title":"make"},{"body":"Overview IceC is an object oriented communication middleware, written in C/C++, with a low use of the resources. This middleware is thought to small microcontrollers with limited resources, but is compatible with different architectures, in this example you are going to see how to program IceC in the architecture x86.\nIngredients In order to follow the next recipe you will need the following requirements:\nThe icec and smart-transducer packages available at Pike\u0026rsquo;s repository. Basic knowledge of C and the tool make. A Debian/Ubuntu Linux distribution. To install the required packages you have to execute the next command.\nconsole $ sudo apt install icec smart-tranducer An IceC server in x86 The server will consist in an object that implement the IBool interface of the st.ice module, located in the path /usr/share/slice/st/.\nThe first step of the program must be implement the set function of the interface, in this example case, the implementation will be print a message with the new value in the stdout.\nsnippet.c void st_IBoolI_set(st_IBoolPtr self, Ice_Bool v, Ice_String sourceAddr) { printf(\u0026#34;The new value is %s\\n\u0026#34;, v ? \u0026#34;true\u0026#34; : \u0026#34;false\u0026#34;); fflush(NULL); } The work flow in an IceC server will always be the same:\nDeclare an ice communicator, an object adapter and the servants (an object that implements and ice interface) that the server is going to serve. snippet.c Ice_Communicator ic; Ice_ObjectAdapter adapter; st_IBool servant; Initialize the communicator and the endpoints of the communicator (TCPEndpoint or UDPEndpoint). snippet.c Ice_initialize(\u0026amp;ic); TCPEndpoint_init(\u0026amp;ic); Create and activate the object adapter with their respective endpoints (ip address and port where the server is going to be listening). snippet.c Ice_Communicator_createObjectAdapterWithEndpoints(\u0026amp;ic, \u0026#34;Adapter\u0026#34;, endp, \u0026amp;adapter); Ice_ObjectAdapter_activate(\u0026amp;adapter); Initialize the servants with their respective types. snippet.c st_IBool_init(\u0026amp;servant); Add the servants to the object adapter. snippet.c char *servant_identity = \u0026#34;ServantIBool\u0026#34;; Ice_ObjectAdapter_add(\u0026amp;adapter, (Ice_ObjectPtr)\u0026amp;servant, servant_identity); Print the apropiate proxies with the structure \u0026quot;[identity] -e [ice_encoding_version] -[invocation_mode]:[endpoints]\u0026quot;. snippet.c printf(\u0026#34;Proxy ready: \u0026#39;%s -e 1.0 -o:%s\u0026#39;\\n\u0026#34;, servant_identity, endp); Wait for calls to objects snippet.c Ice_Communicator_waitForShutdown(\u0026amp;ic); When you print the proxy is very important to consider some issues. Firstly IceC only support the version 1.0, if you want to communicate an Ice program with another written with IceC both have to work with the 1.0 version. Secondly, if you declare a tcp endpoint you have to print a proxy with a tcp invocation mode, the different invocations mode are -o(oneway) -t(twoway) for TCP and -d(datagram) for UDP.\nAfter organize the code, your program should look like this one:\nexample_x86.c #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;IceC/IceC.h\u0026gt; #include \u0026lt;IceC/platforms/x86/TCPEndpoint.h\u0026gt; #include \u0026#34;st.h\u0026#34; void st_IBoolI_set(st_IBoolPtr self, Ice_Bool v, Ice_String sourceAddr) { printf(\u0026#34;The new value is %s\\n\u0026#34;, v ? \u0026#34;true\u0026#34; : \u0026#34;false\u0026#34;); fflush(NULL); } int main() { Ice_Communicator ic; Ice_ObjectAdapter adapter; st_IBool servant; char *endp = \u0026#34;tcp -h 127.0.0.1 -p 10000\u0026#34;; char *servant_identity = \u0026#34;ServantIBool\u0026#34;; Ice_initialize(\u0026amp;ic); TCPEndpoint_init(\u0026amp;ic); Ice_Communicator_createObjectAdapterWithEndpoints(\u0026amp;ic, \u0026#34;Adapter\u0026#34;, endp, \u0026amp;adapter); Ice_ObjectAdapter_activate(\u0026amp;adapter); st_IBool_init(\u0026amp;servant); Ice_ObjectAdapter_add(\u0026amp;adapter, (Ice_ObjectPtr)\u0026amp;servant, servant_identity); printf(\u0026#34;Proxy ready: \u0026#39;%s -e 1.0 -o:%s\u0026#39;\\n\u0026#34;, servant_identity, endp); fflush(NULL); Ice_Communicator_waitForShutdown(\u0026amp;ic); return 0; } Compile the program with IceC To simplify the task of compiling, it\u0026rsquo;s going to be used a Makefile with the tool make.\nThe first task to do is define the variables that are going to be used in the compilation process, in this case we must define the compiler, some paths and the compilation flags that we will need.\nsnippet. ICEC_SRC = /usr/src/IceC DIRSLICE = /usr/share/slice/st/ TARGET = example_x86 SLICE = st CC = gcc CFLAGS \u0026#43;= -I$(ICEC_SRC) #Flag to include the source code of IceC CFLAGS \u0026#43;= -Wall -Wextra -pedantic -Wno-variadic-macros -Wno-unused-parameter \\ -Wno-unused-function Following we are going to used a powerfull tool of make, vpath. Thanks to it, we can indicate a set of directories where make will search for source files or prerequisites that are missing. In order to use IceC, it\u0026rsquo;s necessary to obtain the object code of certain source files, with vpath the folder where that files are located can be indicated.\nsnippet. vpath %.c $(ICEC_SRC) vpath %.c $(ICEC_SRC)/platforms/x86 Finally, we must include all the rules with their respective prerequisites to compile our program.\nsnippet. all: $(SLICE).h $(TARGET) $(SLICE).h: slice2c $(DIRSLICE)$(SLICE).ice $(TARGET): $(TARGET).c IceC.o IceUtil.o TCPEndpoint.o port.o $(CC) $(CFLAGS) $^ -o $@ run: ./$(DIREXE)$(TARGET) clean: find -name \u0026#34;*~\u0026#34; -print -delete find -name \u0026#34;*.o\u0026#34; -print -delete $(RM) $(TARGET) $(SLICE).h As you can see, the two main files that are produce with this Makefile are the header file (in this case st.h) with the IceC interfaces and the executable. In order to obtain the executable, we have to link our source code with some object files of IceC. Because of the previous vpath, make will see the .o files like a prerequisites and will search the .c files in the paths that we indicated, producing the object files and compiling the program.\nThe entire Makefile will be the following.\nMakefile ICEC_SRC = /usr/src/IceC DIRSLICE = /usr/share/slice/st/ TARGET = example_x86 SLICE = st CC = gcc CFLAGS \u0026#43;= -I$(ICEC_SRC) CFLAGS \u0026#43;= -Wall -Wextra -pedantic -Wno-variadic-macros -Wno-unused-parameter \\ -Wno-unused-function vpath %.c $(ICEC_SRC) vpath %.c $(ICEC_SRC)/platforms/x86 all: $(SLICE).h $(TARGET) $(SLICE).h: slice2c $(DIRSLICE)$(SLICE).ice $(TARGET): $(TARGET).c IceC.o IceUtil.o TCPEndpoint.o port.o $(CC) $(CFLAGS) $^ -o $@ run: ./$(DIREXE)$(TARGET) clean: find -name \u0026#34;*~\u0026#34; -print -delete find -name \u0026#34;*.o\u0026#34; -print -delete $(RM) $(TARGET) $(SLICE).h Executing the server Once developed the Makefile, compile and execute the program is as simple as execute the next commands in your shell.\nconsole $ make $ make run If you want to test your server you can use the st-client of the smart-transducer package.\nconsole $ st-client -t bool -p \u0026#34;ServantIBool -e 1.0 -o:tcp -h 127.0.0.1 -p 10000\u0026#34; 1 If all is correct the server must show the following message.\nconsole Proxy ready: \u0026#39;ServantIBool -e 1.0 -o:tcp -h 127.0.0.1 -p 10000\u0026#39; The new value is true ","ref":"https://arcogroup.bitbucket.io/recipes/programming_icec_in_x86/","summary":"Description, step by step, of how to program and compile an IceC program to the x86 architecture","title":"Programming IceC in X86"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/x86/","summary":"","title":"x86"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/integration/","summary":"","title":"integration"},{"body":"Overview Prego is a library consisting on a set of clases and hamcrest matchers usefull to specify shell command interactions through files, environment variables, network ports.\nThe main concept in prego is the Task(). A task is a set of assertions with three different checkers:\ntask.assert_that, for single shot checking task.wait_that, for polling recurrent checking task.command, to run arbitrary shell command A task also counts with two assertions asociated, runnning() and terminated(), which check if the task is working or has finished respectively.\nPrego also counts with context, an object whose attribute may be aytomatically interpolated in command and filename paths. Some of the attributes are set as default values for command() parameters.\nSet up the environment In this recipe the following elements are needed:\nThe python3-prego package, available at Pike\u0026rsquo;s repository The python3-nose, python3-hamcrest packages A Debian/Ubuntu Linux distribution Basic knowledge of python and shell comands If you don\u0026rsquo;t count with Pike in the repository list of your machine you must add it.\nFollowing, you must install the packages previously mentioned with the next command.\nconsole $ sudo apt install python3-prego python3-nose python3-hamcrest A simple example In order to make a simple example we are going to use a tool named ncat that will permit us to emulate a system with a server and a client. To perform the testing with prego we must define tasks, in this case can be differentiated two different tasks, the server and the client. The purpose of our emulate system will be that the client will send the command echo testing to the server, what means that the server must show in the stdout the message \u0026ldquo;testing\u0026rdquo;. Once the funcionality of the system is clearly is time to make the test.\nFirst of all, the context object can be used to set default values. In this case, a variable port can be defined, representing the port where the server will be listening. To access the value of the variable you have to use the $ character.\nsnippet.py prego.context.port = 2000 The next step is create the server task. As you can see in the snippet of code, first the task is initialized, and in this case we say to prego to launch the task in background (detach=true), if we don\u0026rsquo;t do it, the server will block the execution and the task of the client will never be executed. Once the task is defined, a command is assigned to it. The variable cmd will be usefull to check the stdout of the server.\nsnippet.py server = prego.Task(desc=\u0026#39;ncat server\u0026#39;, detach=True) cmd = server.command(\u0026#39;ncat -l -p $port\u0026#39;) The task of the client will be very similar, the main differences are that the client won\u0026rsquo;t be detached, and obviously the command that will be assigned to the task will be different.\nsnippet.py client = prego.Task(desc=\u0026#39;ncat client\u0026#39;) client.wait_that(server, prego.running()) client.command(\u0026#39;ncat -c \u0026#34;echo testing\u0026#34; localhost $port\u0026#39;) The task of the client make a polling checking with wait_that, thanks to this the task of the client won\u0026rsquo;t be executed until the server is running.\nFinally, is time to check if the server has received the message from the client. Thanks to hamcrest and with the variable cmd previously declared, we can compare the stdout of the server with an arbitrary string.\nsnippet.py server.assert_that(cmd.stdout.content, hamcrest.contains_string(\u0026#39;testing\u0026#39;)) Executing a test The code of the test should look similar to this.\ntest.py import hamcrest import prego class TestNcat(prego.TestCase): def test_server(self): prego.context.port = 2000 server = prego.Task(desc=\u0026#39;ncat server\u0026#39;, detach=True) cmd = server.command(\u0026#39;ncat -l -p $port\u0026#39;) server.assert_that(cmd.stdout.content, hamcrest.contains_string(\u0026#39;testing\u0026#39;)) client = prego.Task(desc=\u0026#39;ncat client\u0026#39;) client.wait_that(server, prego.running()) client.command(\u0026#39;ncat -c \u0026#34;echo testing\u0026#34; localhost $port\u0026#39;)\tExecute the test is as simple as type the following in the shell.\nconsole $ nosetests3 test.py If all is correct and the test pass, your shell must show the message OK.\nconsole . ---------------------------------------------------------------------- Ran 1 test in 0.228s OK ","ref":"https://arcogroup.bitbucket.io/recipes/make_testing_with_prego/","summary":"A simple description of how to install and use prego to make system/integration tests","title":"Make testing with Prego"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/prego/","summary":"","title":"prego"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/testing/","summary":"","title":"testing"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/domotics/","summary":"","title":"domotics"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/smart-home/","summary":"","title":"smart home"},{"body":"Overview Smart-Transducer is a platform for building Smart Home solutions the easy way. It uses very simple interfaces, with a push model, to acquire sensor information and also to change the state of actuators. This recipe will analyze those interfaces and how to use them.\nIngredients In order to follow this recipe, you will need to satisfy the following requirements:\nThe smart-transducer package, available at Pike\u0026rsquo;s repository. Python skills (search in python.org for tutorials if needed). Basic understanding of the ZeroC Ice midleware. Debian/Ubuntu Linux and a terminal. If you already added the Pike repository to your system, then just install smart-transducer:\nconsole $ sudo apt install smart-transducer ST description The ST module provides a set of interfaces, written in ZeroC Slice IDL, specifically designed to be simple, and aimed to build sensors and actuators for a smart environment.\nThe definition of the module is as follows:\nst.ice module st { // basic types interface IBool { void set(bool v, string sourceAddr); }; interface IByte { void set(byte v, string sourceAddr); }; interface IFloat { void set(float v, string sourceAddr); }; interface IString { void set(string v, string sourceAddr); }; // observable pattern interface Linkable { void linkto(string observerAddr); }; }; Note: It has an interface for every basic type (bool, byte, float, string\u0026hellip;), with a single method, set, which accepts two arguments:\nvalue, of a specific type (determined by the interface). sourceAddr, a string with the source address of the caller. These interfaces could be implemented by actuators and used by sensors. For instance, a door controller would implement the IBool interface to lock/unlock the door. Or a temperature sensor could use the IFloat interface to publish its temperature to some subscriber.\nThe last interface, Linkable is used to register an observer to some observable object. For instance, the temperature sensor could implement this interface, so when a service wants to receive the temperature, it could call to linkto() with a proxy (or a valid IDM address) to a servant that implements the same interface that the temperature sensor uses (in the former example, IFloat). The sensor usually will store that proxy, and whenever it wants to publish the temperature, it calls the set() method on it.\nLet\u0026rsquo;s use them on a very simple example.\nA (not-so) minimal service Imagine that you want to create a clock service. It has a method that allows the user to change the current time, and it also publishes the updated time every five seconds.\nThis clock service will implement the interface IString to set the current hour. Also, it implements the Linkable interface, so other services could attach their observers. And it will use the same interface IString to publish the updated time.\nSo, first, let\u0026rsquo;s create a custom interface that inherits IString and Linkable. This way, we may have only one servant. The slice would be:\nclock.ice #include \u0026lt;st.ice\u0026gt; module recipe { interface Clock extends st::IString, st::Linkable {}; }; Now, we can implement the servant. The method IString.set() will be used to set the current time, so the given value must have some format. Let\u0026rsquo;s assume that it will be in the form \u0026ldquo;hh:mm:ss\u0026rdquo;, where hh is the hour in 24 hours format, mm is the minute and ss is the second. The code may be:\nsnippet.py def set(self, value, source=None, current=None): self.seconds = sum( x * int(t) for x, t in zip([3600, 60, 1], value.split(\u0026#34;:\u0026#34;)) ) if source is not None: print(\u0026#34;time set to \u0026#39;{}\u0026#39; by {}\u0026#34;.format(value, source)) On the other hand, the method Linkable.linkto() will provide the (stringfied) proxy to some object that will implement the IString interface, so we store it for later use:\nsnippet.py def linkto(self, observer, current): ic = current.adapter.getCommunicator() prx = ic.stringToProxy(observer) self.observer = st.IStringPrx.uncheckedCast(prx) print(\u0026#34;new observer set: \u0026#39;{}\u0026#39;\u0026#34;.format(str(prx))) The last part is to create the server that will hold the adapter, instantiate this servant, and register it on the adapter. It will also run the event loop: once every five seconds, it will call a method on the servant to publish the time to its observer (if any). The whole clock-server.py code is:\nclock-server.py #!/usr/bin/python3 import sys import Ice import time from datetime import datetime, timedelta Ice.loadSlice(\u0026#34;clock.ice -I. --all\u0026#34;) import st import recipe class ClockI(recipe.Clock): def __init__(self): self.observer = None self.set(time.strftime(\u0026#34;%H:%M:%S\u0026#34;)) def set(self, value, source=None, current=None): self.seconds = sum( x * int(t) for x, t in zip([3600, 60, 1], value.split(\u0026#34;:\u0026#34;)) ) if source is not None: print(\u0026#34;time set to \u0026#39;{}\u0026#39; by {}\u0026#34;.format(value, source)) def linkto(self, observer, current): ic = current.adapter.getCommunicator() prx = ic.stringToProxy(observer) self.observer = st.IStringPrx.uncheckedCast(prx) print(\u0026#34;new observer set: \u0026#39;{}\u0026#39;\u0026#34;.format(str(prx))) def publish_time(self, elapsed): if self.observer is None: return self.seconds \u0026#43;= elapsed dt = datetime(1, 1, 1) \u0026#43; timedelta(seconds=self.seconds) now = dt.strftime(\u0026#34;%H:%M:%S\u0026#34;) self.observer.set(now, \u0026#34;clock\u0026#34;) print(\u0026#34;publish time: \u0026#39;{}\u0026#39;\u0026#34;.format(now)) class ClockServer(Ice.Application): def run(self, args): ic = self.communicator() adapter = ic.createObjectAdapterWithEndpoints( \u0026#34;Adapter\u0026#34;, \u0026#34;tcp -h 127.0.0.1 -p 1234\u0026#34; ) adapter.activate() servant = ClockI() proxy = adapter.add(servant, ic.stringToIdentity(\u0026#34;Clock\u0026#34;)) print(\u0026#34;proxy ready: \u0026#39;{}\u0026#39;\u0026#34;.format(proxy)) last = time.time() while True: try: time.sleep(5) elapsed = int(time.time() - last) last = time.time() servant.publish_time(elapsed) except KeyboardInterrupt: break if __name__ == \u0026#34;__main__\u0026#34;: ClockServer(1).main(sys.argv) Run it and you should see something like this:\nconsole $ ./clock-server.py proxy ready: \u0026#39;Clock -t -e 1.1:tcp -h 127.0.0.1 -p 1234 -t 60000\u0026#39; A minimal client You can also write a custom client, which in this case is very straightforward. Just create the proxy, and call it using the given arguments. It will be used to change the clock time, so you must call it using the clock\u0026rsquo;s proxy and the desired time, in the correct format.\nThe whole client code may be:\nclock-client.py #!/usr/bin/python3 import sys import Ice Ice.loadSlice(\u0026#34;clock.ice -I. --all\u0026#34;) import recipe class ClockClient(Ice.Application): def run(self, args): if len(args) \u0026lt; 3: print(\u0026#34;Usage: {} \u0026lt;proxy\u0026gt; \u0026lt;hh:mm:ss\u0026gt;\u0026#34;.format(args[0])) return ic = self.communicator() clock = ic.stringToProxy(args[1]) clock = recipe.ClockPrx.uncheckedCast(clock) clock.set(args[2], \u0026#34;time-master\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: ClockClient().main(sys.argv) If you run it, you should see a message on the server telling you that it changed its time:\nconsole client $ ./clock-client.py \u0026#39;Clock -t:tcp -h 127.0.0.1 -p 1234\u0026#39; \u0026#34;23:59:46\u0026#34; server output: [...] time set to \u0026#39;23:59:46\u0026#39; by time-master A minimal observer In order to receive time publications from the server, we would need an observer. It is just a common servant of the proper type (in this case IString) that we will register on the server using the Linkable interface. We don\u0026rsquo;t need to do any special thing in the servant, so:\nsnippet.py class ClockObserverI(st.IString): def set(self, value, source, current): print(\u0026#34;time event: \u0026#39;{}\u0026#39;, by {}\u0026#34;.format(value, source)) As always, we need to register this servant on an adapter. We then use the resulting proxy as the argument for the linkto method. The whole observer would be:\nclock-observer.py #!/usr/bin/python3 import sys import Ice import time import sched from datetime import datetime, timedelta Ice.loadSlice(\u0026#34;clock.ice -I. --all\u0026#34;) import st import recipe class ClockObserverI(st.IString): def set(self, value, source, current): print(\u0026#34;time event: \u0026#39;{}\u0026#39;, by {}\u0026#34;.format(value, source)) class ClockObserver(Ice.Application): def run(self, args): if len(args) \u0026lt; 2: print(\u0026#34;Usage: {} \u0026lt;clock-server\u0026gt;\u0026#34;.format(args[0])) return ic = self.communicator() adapter = ic.createObjectAdapterWithEndpoints( \u0026#34;Adapter\u0026#34;, \u0026#34;tcp -h 127.0.0.1 -p 1235\u0026#34; ) adapter.activate() servant = ClockObserverI() proxy = adapter.add(servant, ic.stringToIdentity(\u0026#34;Clock\u0026#34;)) print(\u0026#34;Proxy ready: \u0026#39;{}\u0026#39;\u0026#34;.format(proxy)) server = ic.stringToProxy(args[1]) server = st.LinkablePrx.uncheckedCast(server) server.linkto(str(proxy)) print(\u0026#34;Subscribed to clock, waiting events...\u0026#34;) self.shutdownOnInterrupt() ic.waitForShutdown() if __name__ == \u0026#34;__main__\u0026#34;: ClockObserver().main(sys.argv) Run the observer and it should start receiving new events:\nconsole $ ./clock-observer.py \u0026#39;Clock -t:tcp -h 127.0.0.1 -p 1234\u0026#39; Proxy ready: \u0026#39;Clock -t -e 1.1:tcp -h 127.0.0.1 -p 1235 -t 60000\u0026#39; Subscribed to clock, waiting events... time event: \u0026#39;23:59:51\u0026#39;, by clock time event: \u0026#39;23:59:56\u0026#39;, by clock [...] server output: [...] new observer set: \u0026#39;Clock -t -e 1.1:tcp -h 127.0.0.1 -p 1235 -t 60000\u0026#39; publish time: \u0026#39;23:59:51\u0026#39; publish time: \u0026#39;23:59:56\u0026#39; [...] Using the st-client tool To invoke any st interface, you can use the st-client tool, provided in the smart-transducer package. So, to change the clock time, just call it with the proxy and the desired value (run st-client -h for more options):\nconsole $ st-client -t string -p \u0026#34;Clock -t:tcp -h 127.0.0.1 -p 1234\u0026#34; \u0026#34;12:23:34\u0026#34; ","ref":"https://arcogroup.bitbucket.io/recipes/st_getting_started/","summary":"First steps of using the \u003ccode\u003eST\u003c/code\u003e interfaces and tools.","title":"Smart Transducer: getting started"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/st/","summary":"","title":"st"},{"body":"Introducción Las máquinas de prototipado para Printed Circuit Board (PCB), como las fabricadas por la empresa LPKF, permiten al usuario la impresión física de una placa previamente diseñada, siendo la solución predominante en la creación de prototipos. Estas máquinas imprimen el footprint de un diseño en una placa recubierta de cobre, creando los pads y aislando las pistas del resto del cobre.\nPara realizar el proceso de impresión se debe hacer uso de las siguientes herramientas:\nMáquina de prototipado para PCB LPKF Software CircuitCam Software BoardMaster Herramientas de la máquina LPKF Placa revestida de cobre de la cual se imprimirá y cortará nuestro PCB A lo largo de la sección se verá paso por paso como operar con estas herramientas para obtener un PCB a partir de un diseño desarrollado con programas como KiCad o DesignSpark.\nPreparación del diseño con CircuitCam Antes de empezar a imprimir el circuito, hay que convertir los archivos resultantes del diseño en un formato entendible por el software que controlará la máquina (BoardMaster) y con la información necesaria para su impresión. Este es el objetivo del software CircuitCam, el cual es un sistema de fabricación asistida por ordenador para PCBs y otro tipo de tecnologías. Al abrir el software se nos presentará una interfaz similar a la siguiente.\nInterfaz del software CircuitCam\nA la izquierda se puede observar un recuadro rojo, este recuadro es muy importante ya que contiene seis iconos que se corresponden con los seis pasos ordenados que se deben seguir para preparar los archivos y que se explican a continuación.\nImportar Archivos El primer icono corresponde con la importación de los archivos resultado del diseño. En este paso hay que seleccionar aquellas capas que se van a usar para la impresión, como norma general serán necesarios cuatro archivos Gerber correspondientes a las capas top, bottom, de contorno y de taladrado, resultado del diseño previo de la placa. Estos archivos suelen ser generados como plots del diseño y tienen extensión .gbr, salvo la capa de taladrado que posee extensión drl.\nUna vez importados estos archivos, la aplicación nos preguntará por la correspondencia de cada uno de los archivos importados con las capas de la placa. En la ventana que aparece, en la columna Layer/Template se le tiene que que asignar a cada archivo su capa correspondiente:\nArchivo de la capa top - TopLayer Archivo de la capa bottom - BottomLayer Archivo de la capa del contorno - BoardOutline Archivo de la capa de taladrado - DrillPlated En algunos casos es posible que a la capa de taladrado se le deba cambiar la escala para que coincida con el resto de las capas. Si la escala es la incorrecta, este error será muy visible en el software ya que se podrá como una capa queda más pequeña o más grande que el resto. Para la mejor comprensión de este paso, a continuación se presenta una imagen de ejemplo con la correspondencia entre capas.\nEjemplo de correspondencia de archivos con las capas\nIndicar parámetro del contorno Esta tarea está asociada al segundo icono, y su objetivo es el de señalar al software como queremos que se realice el contorno del PCB al cortarlo de la placa original.\nEn este paso hay que indicar al software que a la hora realizar el corte, la máquina debe dejar cuatro pequeñas franjas sin cortar, esto es debido a que si el corte se realizase completo, cuando la placa fabricada quede totalmente libre, la broca seguiría en contacto con ella pudiendo ocasionar que el PCB salga volando. Para evitarlo se le tiene que decir al software que deje cuatro uniones en los laterales (4 edges), además se fijará la anchura de corte del contorno a 1 mm (Tab Width) aunque es recomendable ponerla a una anchura menor, para una vez finalizada la impresión, extraer el PCB con mayor facilidad.\nSelección del contorno de la placa\nCuando terminemos de configurar los parámetros habra que pulsar sobre el botón Run ya que si pulsamos sobre el botón OK los cambios no surtirán efecto. Para comprobar que todo ha salido correctamente, sobre la representación virtual de la placa, debería poder verse un contorno amarillo que rodea la placa entera excepto por los cuatro puntos que hemos indicado que no deben ser cortados.\nIndicar cortes internos El tercer icono correspondiente al tercer paso es utilizado para seleccionar cortes internos. Lo más probable es que el diseño que se esté realizando no tenga cortes internos, por lo que es posible que incluso el icono esté desactivado. En ese caso este paso simplemente será ignorado y se pasará al siguiente.\nRealizar limpieza de cobre El cuarto paso, está destinado a la limpieza total de cobre en algunas zonas de la placa. Si el usuario lo desea, puede seleccionar ciertas zonas de la placa a las que se les quitará todo el cobre de la superficie. La realización de este paso se deja a elección del usuario puesto que este paso no es obligatorio y puede ser omitido.\nAislamiento de las capas El quinto paso se encarga de aislar las capas que forman nuestra placa. Este paso es tan sencillo como pulsar sobre el quinto icono y esperar a que la tarea finalice.\nExportar los archivos Llegamos al último paso del proceso con CircuitCam. Al igual que los últimos pasos, este es un paso muy sencillo ya que nos encargaremos simplemenre de exportar el proyecto y generar el archivo .LMD, con el que podrá operar BoardMaster y podremos imprimir nuestro diseño. Debemos indicar una ruta y un nombre del archivo para que el archivo LMD sea almacenado.\nInterfaz de BoardMaster BoardMaster es el software que nos permitirá comunicarnos y manejar la máquina LPKF durante todo el proceso de impresión. Antes de comenzar a explicar este proceso, es necesario explicar algunas de las funcionalidades más importantes de este software, así como familiarizar al usuario con la interfaz que presenta.\nCuando BoardMaster se ejecuta presenta la interfaz que se puede ver a continuación.\nInterfaz BoardMaster\nComo se puede observar en la imagen, en la interfaz hay algunos números señalando partes de la misma para simplificar su explicación. La utilidad de cada elemento es la siguiente.\nLista desplegable con los distintos pasos que se deben llevar a cabo para realizar la impresión. Lista desplegable con los nombres y medidas de las herramientas que van a ser usadas para la impresión. Las herramientas son unas brocas que deben ser situadas al norte de la mesa de trabajo. Esta lista muestra el orden de las herramientas de izquierda a derecha, la herramienta con el número 1 será la que está situada más a la izquierda desde el punto de vista del usuario. Representación virtual de las herramientas en la mesa de trabajo, a cada tipo de herramienta se la asigna un color que se corresponde con el color que posee la herramienta real. Botón de la cruceta que permite situar el cabezal de la máquina en una coordenada concreta de la mesa. Botón de Parking que nos permitirá extraer la mesa de trabajo para realizar cualquier operación, como por ejemplo colocar la placa de cobre o colocar las herramientas. Estas flechas de desplazamiento permiten al usuario mover el cabezal con precisión. Las flechas de la izquierda permiten mover el cabezal en los ejes X e Y, las flechas de la derecha permiten moverlo en el eje Z. El número situado en el centro de las flechas permite indicar el número de unidades que se va a desplazar el cabezal por una pulsación. Estos cuatro botones forman un conjunto que permite manipular el footprint de la placa. El botón de abajo a la izquierda permite mover la representación virtual de la placa a lo largo de la mesa de trabajo para elegir donde realizar la impresión, el botón de la derecha realiza copias de la placa lo cual será muy útil si por ejemplo se quiere realizar la impresión de más de una placa de nuestro diseño. Los botones de arriba son botones de selección de pads y pistas, el de la izquierda solo seleccionará aquellos pads y pistas que se le indiquen, el de la derecha en cambio seleccionará los pads y pistas que hemos indicado así como todo a lo que esté conectado. Botón Start/Stop. Cuando se realice el proceso de impresión este botón se utilizará para iniciar o detener cualquiera de las etapas del proceso. Representación virtual de la mesa de trabajo. Botones de adición y eliminación de pads y pistas. Para el software BoardMaster, cuando queremos seleccionar algún componente del footprint no basta con los botones de selección, estos componentes despueś deben ser añadidos. Por ejemplo si quisiesemos imprimir un pad de la placa y simplemente lo seleccionamos el pad no será impreso dado que debe ser añadido. Los botones + y - añaden o eliminan una selección respectivamente, los botones Alt+ y Alt- añaden o eliminan todos los componentes del footprint. Si el uso de algún elemento de la interfaz no se ha entendido muy bien en esta explicación, durante el proceso de impresión todo quedará más claro.\nPreparación de la impresión Una vez generado el archivo LMD con CircuitCam se deben preparar tanto la máquina como el software BoardMaster para realizar el proceso de impresión.\nLo primero de todo es encender la máquina LPKF, muy importante, el programa BoardMaster no debe ejecutarse hasta que la máquina esté encendida con el objetivo de evitar cualquier problema de comunicación entre el software y la máquina. Una vez que nos aseguremos de que la máquina está encendida se procede a abrir BoardMaster.\nPreparación de la máquina LPKF La primera tarea será la de colocar las herramientas, con cuidado de no dañarlas, en su lugar correspondiente. Para ello, primero debemos observar las herramientas que el software considera que tiene la máquina (la lista desplegable de las herramientas nos será de gran ayuda) y colocar cada una en su lugar correspondiente. Si nuestro diseño requiere de alguna herramienta que no aparezca en la interfaz de BoardMaster tenemos dos alternativas, antes de iniciar el proceso de impresión añadir esa herramienta en el software y después la colocamos en la máquina, o también, en el momento que lleguemos a una etapa que requiera de esa herramienta, BoardMaster nos avisará y podremos realizar el cambio tanto en el software como en la máquina. Lo más importante es que en este paso las herramientas de la mesa virtual de BoardMaster y de la máquina LPKF sean las mismas y estén colocadas en su lugar correspondiente.\nUna vez tengamos las herramientas colocadas hay que colocar la placa de cobre en la mesa, muy importante siempre que manejemos la placa el uso de guantes, ya que la grasa de las manos ensuciaría la placa provocando su oxidación. Si observamos detenidamente la placa de cobre, en los laterales tiene dos pequeños orificios que encajan perfectamente con la mesa, cogemos la placa con los guantes y la encajamos bien en la mesa comprobando que no se mueva. La colocación es muy importante, ya que si para la capa top y para la capa bottom, la placa no fuese colocada de la misma forma, las pistas no coincidirían con los pads produciendo un mal proceso de fabricación que puede concluir en una placa defectuosa.\nEjemplo de herramientas colocadas en la máquina LPKF\nPreparación de BoardMaster Para empezar hay que señalar el área de trabajo en la mesa virtual de la interfaz de BoardMaster, es decir vamos a indicar la zona en la que se va a mover el cabezal de la máquina durante la impresión. Para ello, en la lista desplegable señalamos una herramienta (preferentemente una de marcado, se suelen identificar con el color naranja) y esperamos a que el cabezal de la máquina la coja. A continuación, ya sea con el botón de la cruceta o con las flechas de desplazamiento, desplazamos el cabezal de la máquina a la zona de la mesa donde queramos situar la esquina inferior izquierda de nuestro área de trabajo. Una vez llegado al punto deseado confirmamos seleccionando Configuración-\u0026gt;Material-\u0026gt;Seleccionar Esquina Inferior. Hacemos lo mismo para seleccionar la esquina superior derecha pero en este caso confirmamos con Configuración-\u0026gt;Material-\u0026gt;Seleccionar Esquina Superior.\nEl área de trabajo se representará como un recuadro gris sobre la mesa virtual, este área representa ahora la zona de la placa donde podemos realizar impresiones. Muy importante que a la hora de hacer la selección observemos la zona que recorre el cabezal real de la máquina, ya que por ejemplo si seleccionamos una zona en la que ya se ha realizado el corte de una placa, la impresión fallaría.\nEjemplo de área de trabajo\nPor último hay que importar el proyecto que se ha generado anteriormente con CircuitCam, seleccionamos Archivo-\u0026gt;Importar-\u0026gt;LMD e introducimos la ruta donde almacenamos el archivo con extensión .LMD. Una vez importado en la mesa de trabajo aparecerá un footprint de nuestro PCB, el cual podremos desplazar a nuestro gusto por la zona de trabajo.\nProceso de impresión Todo el entorno, tanto la máquina LPKF como BoardMaster está preparado para iniciar la impresión, ahora debemos ejecutar, en orden, cada una de las étapas de la lista desplegable para realizar el proceso. Nosotros solo debemos preocuparnos en seleccionar la étapa de la lista, añadir los componentes del footprint, pulsar el botón Start y esperar a que finalice, el software sabrá que herramienta elegir y como operar para cada una de las tareas. Si no entiendes alguna indicación del proceso ve a Interfaz de BoardMaster. El procedimiento de la impresión será el siguiente:\nMarkingDrills: Primer paso del proceso de impresión, su objetivo es el de marcar los taladros, para que llegada la hora de taladrar el taladro no resbale y los pads y pistas se desplacen. Se seleccionan todos los pads y pistas con el botón Alt+ y se pulsa el botón Start. Una vez termine el proceso, se puede observar si el proceso se ha realizado correctamente pulsando sobre el botón parking y viendo el resultado en la placa. DrillingPlated: En este paso la broca atravesará la placa y creará los pads. Dado que tampoco es una tarea muy delicada podemos añadir todos los pads con el botón Alt+ e iniciar el proceso con Start. Info! Galvanic Plating: Esta es una etapa informativa, lo que nos está diciendo es que si vamos a hacer el galvanizado de los pads es el momento idóneo para hacerlo. Este paso puede ser omitido. Read_Bottom: Esta de nuevo es otra etapa informativa que nos está diciendo que preparemos la capa bottom para ser impresa. MillingBottom: Aquí la máquina realizará las pistas que conectan los pads de la capa bottom. Para comprobar que la máquina está calibrada, con el botón de selección de la derecha (el que seleccionaba unicamente aquello que indicabamos) se deben indicar pequeñas pistas aisladas que rodean los pads de la placa, añadirlos con el botón + y realizar la impresión. Si tras la impresión se ve que la pista está limpia se pulsa sobre el botón Alt+ para realizar la impresión completa de las pistas, en caso contrario habrá que calibrar la máquina. Read_Top: En esta etapa se nos está indicando que debemos darle la vuelta a la placa, puesto que vamos a imprimir la capa top. Para ello no hay que olvidarse de ponerse los guantes, darle la vuelta a la placa y encajarla bien de nuevo en la mesa para que el proceso sea lo más preciso posible. MillingTop: Con este paso se llega a los momentos más críticos de la impresión. Al girar la placa se puede dar el caso de que las pistas no correspondan a la perfección con las de la capa bottom, pudiendo ocasionar malas conexiones. Para solventar este problema, se puede proceder como en el caso anterior imprimiendo pequeñas pistas aisladas que rodean pads y comprobando su impresión. Si las pistas no coinciden bien con el pad se debe calibrar la máquina modificarndo el origen de la misma. Con este fin se pulsa sobre Configuración-\u0026gt;Selecciones de Máquina, en la ventana que aparece pulsamos sobre Desbloquear, y mediante prueba y error, modificamos las coordenadas del Home a la vez que imprimimos pequeñas pistas aisladas hasta que las pistas coincidan correctamente con los pads. DrillingUnplated: Etapa omitida. CuttingInside: En esta etapa la máquina realizará los cortes internos de la placa. Lo más probable es que la placa a fabricar no tenga cortes internos por lo que esta etapa puede omitirse. CuttingOutside: Última etapa en el software. En esta etapa la máquina realizará el corte externo, dejando los espacios sin cortar que indicamos en CircuitCam. Ventana de modificación del home\nCuando los pasos hayan finalizado, pulsamos el botón parking para extraer la mesa de trabajo, y con los guantes, coger la lámina de PCB y extraer la/s placa/s impresa/s de la placa original. Una vez extraídas se las aplica una laca protectora y se liman los bordes para poder cogerla con las manos desnudas.\nPara finalizar, se debe dejar el entorno lo más limpio posible por lo que es importante extraer las herramientas y apagar la máquina. El proceso de impresión ha finalizado.\n","ref":"https://arcogroup.bitbucket.io/recipes/print_pcb_with_lpkf/","summary":"En esta sección se verá como imprimir con una máquina LPKF un PCB previamente diseñado","title":"Cómo imprimir un PCB con una máquina LPKF"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/equipment/","summary":"","title":"equipment"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/hardware/","summary":"","title":"hardware"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/lpkf/","summary":"","title":"lpkf"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/pcb/","summary":"","title":"pcb"},{"body":"Mijia Smart Home API reference class index: LumiGateway CubeAqgl01 Magnet Motion Plug Switch WeatherV1 mijia API Documentation Class LumiGateway This class is used to access a Lumi Gateway device (provided by Xiaomi / Aqara). It will automatically try to discover devices on your network, using the Lumi LAN protocol. It installs a thread to listen for incomming events, and also allows you to change the device light color.\nillumination [read only property] Stores the Gateway\u0026rsquo;s provided value for its light sensor. It\u0026rsquo;s updated when a change event arrives, and on start-up.\nstatus A property that provides the status of this device, which could be:\nunknown: there is not enough information to set a valid status. It receives this state when the object is first created. online: some message arrived recently from the device, so it is up and running correctly. offline: the device has not sent a heartbeat during its heartbeat period (which is variable, but usually 10 minutes on battery devices). __init__ def __init__(self, passwd=None, mcast_member=None) LumiGateway constructor. This is the starting point of the library. You will need to create an instance of this class, and use it access other devices.\nParameters:\npassword, [optional] the Wireless communication protocol password, needed to change the state of any device (which performs a write on the device). mcast_member, [optional] the IP address of the interface that will be used as the multicast member. This address is used to receive events from the Lumi Gateway. If not provided, it will try to automatically determine it. get_devices def get_devices(self, filter=None) Returns a dictionary (pairs of sid and Device objects) with the currently linked devices.\nParameters:\nfilter: used to retrieve only devices of the given type. You could use the class of a Device, its type property or even the type value. For instance, to get only the Switch objects, you may use any of the following options: gw.get_devices(filter=Switch) gw.get_devices(filter=Switch.type) gw.get_devices(filter=\u0026quot;switch\u0026quot;)\nlight_off def light_off(self) Switch off Gateway\u0026rsquo;s light. The same result could be achieved providing 0 as intensity on set_light_color().\non_change def on_change(self, cb) Stablish a callback to be called when the Gateway\u0026rsquo;s light sensor changes. The callback signature should be as follows:\ndef callback([self,] event, device)\nWhere event is the name of this event (change), and device is the Gateway device. To retrieve the current value of the light sensor, use the property illumination.\non_heartbeat def on_heartbeat(self, cb) Installs an event handler that will be called when this device sends a heartbeat message.\nParameters:\ncb: callback function to be called. It must accept two parameters: the event name (a string), and the device that produces the event (a Device or derived object). set_light_color def set_light_color(self, r=0, g=0, b=0, i=0) Changes the Gateway\u0026rsquo;s light to the given color (by RGB components) and intensity. Note that the provided values are relative to each other, so a value of (1, 0, 0) will produce the same result as (255, 0, 0).\nParameters:\nr: int in range [0, 255]; red component of the light\u0026rsquo;s color. g: int in range [0, 255]; green component of the light\u0026rsquo;s color. b: int in range [0, 255]; blue component of the light\u0026rsquo;s color. i: int in range [0, 100]; light intensity. start def start(self) Start the discovery process. It will search for an active gateway, and then list its linked devices. It also starts the listening thread to receive messages from the Gateway. You usually will need to call this method.\nwait_until_break def wait_until_break(self) Blocks the calling thread execution until it receives a KeyboardInterrupt (Ctrl+C) Use it (or use instead an event loop) to keep the application running while you wait for incoming events.\nmijia.devices API Documentation Class CubeAqgl01 Mi Cube Controller, which is a device that detects different movements or actions performed on itself: tap, double tap, flips of 90 and 180 degrees, rotation, move, shake, swing and free fall. It also works as a motion sensor: when you touches it after a while, it will send an alert event.\nExcept for the rotate event, all the attached callbacks should be in the form:\ndef callback([self,], event, device)\nWhere event is the name of this event, and device is the device object that produced the event.\nstatus A property that provides the status of this device, which could be:\nunknown: there is not enough information to set a valid status. It receives this state when the object is first created. online: some message arrived recently from the device, so it is up and running correctly. offline: the device has not sent a heartbeat during its heartbeat period (which is variable, but usually 10 minutes on battery devices). on_alert def on_alert(self, cb) Attach a callback handler for alert events. These events are sent when the cube is moved after some time of inactivity (no movement).\non_flip180 def on_flip180(self, cb) Attach a callback handler for flip180 events. These events are produced when the cube is flipped 180 degrees (i.e rotated on X and Y axis).\non_flip90 def on_flip90(self, cb) Attach a callback handler for flip90 events. These events are produced when the cube is flipped 90 degrees (i.e rotated on X and Y axis).\non_free_fall def on_free_fall(self, cb) Attach a callback handler for free_fall events, which are sent when you drop the cube (or it experiments some acceleration on the vertical axis).\non_heartbeat def on_heartbeat(self, cb) Installs an event handler that will be called when this device sends a heartbeat message.\nParameters:\ncb: callback function to be called. It must accept two parameters: the event name (a string), and the device that produces the event (a Device or derived object). on_move def on_move(self, cb) Attach a callback handler for move events, generated when you displace the cube some small distance on the horizontal plane.\non_rotate def on_rotate(self, cb) Attach a callback handler for rotate events, which are sent when you rotate the cube along the vertical axis (Z axis).\nThe attached callbacks should be in the form:\ndef callback([self,], event, device, degrees, time)\nWhere event is the name of this event, device is the device object that produced the event, degrees is the angle of the rotation, and time is the time that took the sensor to measure the angle.\non_shake_air def on_shake_air(self, cb) Attach a callback handler for shake_air events, which are sent when you take the cube and shake it on the air.\non_swing def on_swing(self, cb) Attach a callback handler for swing events. These events are signaled when the cube makes a fast swing movement (one single shot is enough). Note that it is also a trigger for the iam event (used to link the cube to a Gateway).\non_tap_twice def on_tap_twice(self, cb) Attach a callback handler for tap_twice events. These events are sent when the cube is knocked twice on an horizontal hard surface.\nClass Magnet Aqara reed sensor (magnet) for detection of opening and close of doors and windows. It emits an open event when the two components of the sensor are separated, and a close event when they are joined together. Also, no_close events are sent when the elapsed time from the last open event reaches some pre-established thresholds (in the sensor firmware or in the Gateway).\nelapsed_no_close Gives the amount of elapsed time (in seconds) from the last open event. It is provided with the no_close event from the sensor (not computed locally).\nopen True if the sensor detects that the door/window is open (or the two components of the sensore are not together). False otherwise.\nstatus A property that provides the status of this device, which could be:\nunknown: there is not enough information to set a valid status. It receives this state when the object is first created. online: some message arrived recently from the device, so it is up and running correctly. offline: the device has not sent a heartbeat during its heartbeat period (which is variable, but usually 10 minutes on battery devices). on_close def on_close(self, cb) Attach a callback handler for close events, which are sent when the sensor detects the closing of the door/window. The attached callback should be in the form:\ndef callback([self,], event, device)\nWhere event is the name of this event, and device is the device object that produced the event.\non_heartbeat def on_heartbeat(self, cb) Installs an event handler that will be called when this device sends a heartbeat message.\nParameters:\ncb: callback function to be called. It must accept two parameters: the event name (a string), and the device that produces the event (a Device or derived object). on_no_close def on_no_close(self, cb) Attach a callback handler for no_close events, which are sent when the elapsed time from the last open event reaches certain thresholds (120 seconds, 300, 600, and so on). The elapsed time is provided in the event, so the attached callback should be in the form:\ndef callback([self,], event, device, elapsed)\nWhere event is the name of this event, and device is the device object that produced the event.\non_open def on_open(self, cb) Attach callback handler for open events, which are sent when the sensor detects the opening of the door/window. The attached callback should be in the form:\ndef callback([self,], event, device)\nWhere event is the name of this event, and device is the device object that produced the event.\nClass Motion This is the Aqara PIR (or motion) sensor. It emits an event when a moving object is detected, and some other events when the elapsed time from the last motion detected reaches some pre-established thresholds (in the sensor firmware or in the Gateway).\nelapsed_no_motion Gives the amount of elapsed time (in seconds) between the last motion event and the last no_motion event. It is provided with the no_motion event from the sensor (not computed locally).\nstatus A property that provides the status of this device, which could be:\nunknown: there is not enough information to set a valid status. It receives this state when the object is first created. online: some message arrived recently from the device, so it is up and running correctly. offline: the device has not sent a heartbeat during its heartbeat period (which is variable, but usually 10 minutes on battery devices). on_heartbeat def on_heartbeat(self, cb) Installs an event handler that will be called when this device sends a heartbeat message.\nParameters:\ncb: callback function to be called. It must accept two parameters: the event name (a string), and the device that produces the event (a Device or derived object). on_motion def on_motion(self, cb) Attach a callback handler for motion events, which are sent when a moving object passes through the field of view of the sensor. The attached callback should be in the form:\ndef callback([self,], event, device)\nWhere event is the name of this event, and device is the device object that produced the event.\non_no_motion def on_no_motion(self, cb) Attach a callback handler for no_motion events, which are sent when the elapsed time from the last detected motion reaches certain thresholds (120 seconds, 300, 600, and so on). The elapsed time is provided in the event, so the attached callback should be in the form:\ndef callback([self,], event, device, elapsed)\nWhere event is the name of this event, and device is the device object that produced the event.\nClass Plug This is the Aqara plug controller. Emits 2 different events, to notify if someone enabled or disabled the plug. You can also change the state of the plug: enable or disable it. The properties enabled and inuse will give you information about the plug usage.\nAll event callbacks should met the following signature:\ndef callback([self,], event, device)\nWhere event is the name of the event, and device is the device object that produced this event.\nenabled True if the plug is enabled (provides energy to the connected appliance), False otherwise.\ninuse True if there are a connected appliance, and it is currently consuming energy. False otherwise.\nload_power Connected appliance (load) power usage, in watts (W).\npower_consumed The cumulative load power consumption since the product was used, in Wh.\nstatus A property that provides the status of this device, which could be:\nunknown: there is not enough information to set a valid status. It receives this state when the object is first created. online: some message arrived recently from the device, so it is up and running correctly. offline: the device has not sent a heartbeat during its heartbeat period (which is variable, but usually 10 minutes on battery devices). on_heartbeat def on_heartbeat(self, cb) Installs an event handler that will be called when this device sends a heartbeat message.\nParameters:\ncb: callback function to be called. It must accept two parameters: the event name (a string), and the device that produces the event (a Device or derived object). on_switch_off def on_switch_off(self, cb) Attach callback handler for switch_off events, which are sent when someone presses the built-in button of the plug (or otherwise changes the enabled state), and the state goes from on to off.\non_switch_on def on_switch_on(self, cb) Attach callback handler for switch_on events, which are sent when someone presses the built-in button of the plug (or otherwise changes the enabled state), and the state goes from off to on.\nswitch_off def switch_off(self) Call this method to disable the plug (does not provide energy to connected appliance). Remember that you must provide a password on the Gateway construction for this method to work.\nswitch_on def switch_on(self) Call this method to enable the plug (provide energy to connected appliance). Remember that you must provide a password on the Gateway construction for this method to work.\nClass Switch This is the Aqara push button that emits 4 different events. All event callbacks should met the following signature:\ndef callback([self,], event, device)\nWhere event is the name of the event, and device is the device object that produced this event.\npressed Property to retrieve the current state of the push button: true is pressed, false is not pressed.\nstatus A property that provides the status of this device, which could be:\nunknown: there is not enough information to set a valid status. It receives this state when the object is first created. online: some message arrived recently from the device, so it is up and running correctly. offline: the device has not sent a heartbeat during its heartbeat period (which is variable, but usually 10 minutes on battery devices). on_click def on_click(self, cb) Attach callback handler for click events, which is a single press on the button.\non_double_click def on_double_click(self, cb) Attach callback handler for double_click events, which is a double press on the button.\non_heartbeat def on_heartbeat(self, cb) Installs an event handler that will be called when this device sends a heartbeat message.\nParameters:\ncb: callback function to be called. It must accept two parameters: the event name (a string), and the device that produces the event (a Device or derived object). on_long_click_press def on_long_click_press(self, cb) Attach callback handler for long_click_press events. These events are produced when the button is pressed and maintained for more that 1 second. It is paired with the long_click_release event.\non_long_click_release def on_long_click_release(self, cb) Attach callback handler for long_click_release events. These events are produced when the button is pressed, maintained for more that 1 second and then released. It is paired with the long_click_press event.\nClass WeatherV1 Aqara weather sensor (version 1.0). It provides readings for temperature, humidity and air pressure. The latest provided values for these magnitudes could be read any time from the corresponding properties.\nAll event callbacks should met the following signature:\ndef callback([self,], event, device, value)\nWhere event is the name of the event, device is the device object that produced this event, and value is the provided value (as integer) for that magnitude.\nhumidity Last value provided on a humidity event (or retrieved on startup).\npressure Last value provided on a pressure event (or retrieved on startup).\nstatus A property that provides the status of this device, which could be:\nunknown: there is not enough information to set a valid status. It receives this state when the object is first created. online: some message arrived recently from the device, so it is up and running correctly. offline: the device has not sent a heartbeat during its heartbeat period (which is variable, but usually 10 minutes on battery devices). temperature Last value provided on a temperature event (or retrieved on startup).\non_heartbeat def on_heartbeat(self, cb) Installs an event handler that will be called when this device sends a heartbeat message.\nParameters:\ncb: callback function to be called. It must accept two parameters: the event name (a string), and the device that produces the event (a Device or derived object). on_humidity def on_humidity(self, cb) Attach callback handler for humidity events. Only sent when value changes from previous one.\non_pressure def on_pressure(self, cb) Attach callback handler for pressure events. Only sent when value changes from previous one.\non_temperature def on_temperature(self, cb) Attach callback handler for temperature events. Only sent when value changes from previous one.\n","ref":"https://arcogroup.bitbucket.io/api/mijia_smart_home/","summary":"Mijia Smart Home module to control Xiaomi devices","title":"Mijia Smart Home API reference"},{"body":"Overview Xiaomi (or Aqara or Mijia) has a family of products intended to be part of a Smart Home environment that everyone can install and use. They are beautiful and simple, but the associated cloud app sometimes does not fullfill our requirements (for instance, cloudless control).\nThus, here we present a Python library that you could use to interact with those devices, changing its state and receiving events from their sensors. Let\u0026rsquo;s review what you will need to follow up.\nXiaomi/Mijia smart home kit\nIngredients For this recipe, you will need the following items:\nA Mijia Gateway, and the devices you want to control (PIR, push button, magnet sensor, \u0026hellip;) A WiFi access point (you may also use your own Smartphone AP) An updated Debian/Ubuntu Linux box Python 3 installed and ready Pike\u0026rsquo;s repository for library package For this last requirement, and if you already have Pike\u0026rsquo;s repository configured on your system, just install the library:\nconsole $ sudo apt install mijia-smart-home Setting up the environment The library you just installed uses what is called the Lan Communication API, which is an open API that allows third-party applications to control Mijia devices. It should be enabled on the Gateway, as it\u0026rsquo;s disabled by default.\nSo, your first step should be to install the \u0026ldquo;Mi Home\u0026rdquo; application on your smartphone. Now, plug the Mijia Gateway and wait for a chinese message. It should now blink on yellow. If not, press the button for 5 seconds and release. It should beep, say the message and blink on yellow. This state is the \u0026ldquo;binding\u0026rdquo; state. Now, open the \u0026ldquo;Mi Home\u0026rdquo; app on your smartphone.\nIf required, login or register. Then, press the upper-right plus button. It will show an scan procedure, and find the connected Mijia Gateway. Press it when found.\nAdding Gateway to Mi Home\nNow, it will show you a dialog to enter the WiFi credentials. The Mi Home app wil send this information to the Gateway, so it can also connect to your AP. Just select your SSID and provide the AP password. Note that this AP must have Internet conectivity for this step to be completed. But just for this step, once the Gateway is configured, you can let the AP isolated from the Internet.\nPress \u0026ldquo;next\u0026rdquo;, and the app will finish the setup procedure. You may also hear a message from the Gateway saying that the connection is ready.\nNext, select the Gateway in the app, and open its \u0026ldquo;Settings\u0026rdquo;, and press on \u0026ldquo;About\u0026rdquo;.\nGateway \u0026#39;settings\u0026#39; \u0026gt; \u0026#39;about\u0026#39; It will show you an screen with a link to the forum, and the \u0026ldquo;Plug-in version\u0026rdquo; at the bottom. Press that label (\u0026ldquo;Plug-in version\u0026rdquo;) until you see a message, and more options appear.\nEnabling the hidden settings\nThere, you should see now the option \u0026ldquo;Wireless communication protocol\u0026rdquo;. Tap it. Here you can enable the protocol, and regenerate a new password. Do both of them, and store the password, as you will need it later. Press OK when finish.\nEnabling the \u0026#39;Wireles Communication Protocol\u0026#39;\nNow, you can disconnect your WiFi from the internet (if you want), and close the \u0026ldquo;Mi Home\u0026rdquo; app. You are now ready to use the library.\nConnection to the Gateway The mijia-smart-home library provides some objects that represents the real-life devices. You can call these object\u0026rsquo;s methods to attach handlers that will be called when an event arrives, or call directly a method to change the state of some device.\nSo, the first object you should create is the Gateway itself. Import the library and create a LumiGateway() instance. Then, call the method start(), which does the following:\nStarts a discovery stage, to find the Gateway. Setups a listener thread, to process incoming events from it. snippet.py from mijia import LumiGateway gw = LumiGateway() gw.start() Warning! The LumiGateway\u0026rsquo;s constructor has some parameters but we will review them later. When the start() returns, you can inspect the list of linked devices to the Gateway (both online and offline).\nsnippet.py for dev in gw.get_devices().values(): print(dev) Each device is of an specific class, having its specific methods, but all of them have an sid, which is the device identifier, and a status, which indicates if the device is online and the Gateway acknowledges it or not. The rest of the interface will be analyzed in following sections.\nSo, the full code for this part would be:\nlist-devices.py #!/usr/bin/python3 from mijia import LumiGateway class ListDevices: def __init__(self): # create the gateway gw = LumiGateway() gw.start() for dev in gw.get_devices().values(): print(dev) if __name__ == \u0026#34;__main__\u0026#34;: ListDevices() Running that script in a well configured environment, should yield something like this:\nconsole $ ./list-devices.py INFO:root:using 192.168.8.157 for multicast membership WARNING:root:using not ciphered key (password not provided) \u0026lt;Plug, sid: 158d0002699f12, status: unknown, enabled: False, inuse: False\u0026gt; \u0026lt;Magnet, sid: 158d0002520b26, status: unknown, open: None\u0026gt; \u0026lt;Switch, sid: 158d00027b8eff, status: unknown, pressed: False\u0026gt; \u0026lt;WeatherV1, sid: 158d0002c8f54e, status: unknown, temp: 10000, humi: 0, press: 0\u0026gt; \u0026lt;Unsupported Device, sid: d0cf5efffeccf1dc, status: unknown\u0026gt; \u0026lt;Motion, sid: 158d0002567d32, status: unknown, elapsed: None\u0026gt; \u0026lt;SensorCube, sid: 158d00029a8fa6, status: unknown\u0026gt; Receiving events On the above listing you can see a number of defices, each one of an specific class. For instance, there are a Switch and a Motion, and both emit events when somenthing happens. You can configure event listeners to be run when one of such events arrives. There is also an Unsupported Device, which is a device that the Gateway sees but can not control (yet).\nSo, let\u0026rsquo;s subscribe to some event. To do that, each device has one method per event type, usually called on_[event_name](). For example, the Switch object will emit click events when single pressed, double_click when double tapped, and so on. If you want to know the supported devices and their events, please see the Mijia Smart Home API. For now, we are interested only in the click event, so we use the on_click() method to register our listener. A listener is just a function (or method) that will be called upon event arrival, so let\u0026rsquo;s define it:\nsnippet.py def on_event(self, event, dev): print(\u0026#34;- event: {}, dev: {}\u0026#34;.format(event, dev)) As you can see, the method receives two arguments:\nevent: which is the name of the event (so you can reuse this method for multiple event types) dev: which is the device object that produced this event These arguments are common to all event types. This event does not have any specific argument, but other events do (so please, review the correct signature for your event type!).\nNow, it\u0026rsquo;s time to register the listener. So the first step is to get the Switch object. The Gateway\u0026rsquo;s get_devices() method has an optional argument, filter, used to return only devices of an specific type. In my case there is only one Switch, so the code would be:\nsnippet.py devices = gw.get_devices(filter=\u0026#34;switch\u0026#34;) switch = list(devices.values())[0] And now, to subscribe the listener method to the click event, you need to use the on_click() method:\nsnippet.py switch.on_click(self.on_click_event) One last step: as all of the previous methods are non-blocking, if you run the script, it will finish before any event may arrive. To solve this, we could wait somehow (for instance using a sleep, or with an event loop). Also, we could use the Gateway\u0026rsquo;s waiting function, wait_until_break(), which does that very thing: sleeps until a Ctrl+C is received, and then returns. So, the whole script would be:\nswitch-listener.py #!/usr/bin/python3 from mijia import LumiGateway class SwitchListener: def __init__(self): # create the gateway gw = LumiGateway() gw.start() # get the switch device devices = gw.get_devices(filter=\u0026#34;switch\u0026#34;) switch = list(devices.values())[0] switch.on_click(self.on_click_event) # wait for incomming events gw.wait_until_break() def on_click_event(self, event, dev): print(\u0026#34;- event: {}, dev: {}\u0026#34;.format(event, dev)) if __name__ == \u0026#34;__main__\u0026#34;: SwitchListener() If you run it, and then press the button once, you may obtain a result like this:\nconsole $ ./switch-listener.py INFO:root:using 192.168.8.157 for multicast membership WARNING:root:using not ciphered key (password not provided) - event: click, dev: \u0026lt;Switch, sid: 158d00027b8eff, status: online, pressed: False\u0026gt; Sending actions Some devices allow you to change its state. For instance, you are able to switch on and off the Plug, or change the light status of the LumiGateway.\nIn order to gain permission to modify the state of any device, you must provide the password that the Gateway is using (remember the number that you got when activating the Lan Protocol?). So, you need to change the LumiGateway instantiation like this:\nsnippet.py gw = LumiGateway(passwd=\u0026#34;thePassword\u0026#34;) Of course, you don\u0026rsquo;t want to store the password inside the script, so it may be a variable obtained elsewhere (in our example, we will use sys.argv[1], but it is not a good place either).\nNow, to change some parameter of your device, just call the appropiate method with the required arguments. For instance, to change the light of the Gateway, we use set_light_color(), which accepts the values for Red, Green and Blue components (in relative range of 0-255), and also the Intensity (from 0 to 100). If you want a bright red color, your call will be:\nsnippet.py gw.set_light_color(255, 0, 0, 100) Or, if you want to smoothly range from green to red:\nsnippet.py for i in range(0, 255): gw.set_light_color(i, 255-i, 0, 100) time.sleep(0.05) To review the devices that you can control, and what methods they have, please go to the Mijia Smart Home API section.\n","ref":"https://arcogroup.bitbucket.io/recipes/integrating_xiaomi_devices/","summary":"This recipe will show you how to actuate and receive events from the Mijia Smart Home kit (using \u003cb\u003ePython\u003c/b\u003e and the \u003ccode\u003emijia-smart-home\u003c/code\u003e library).","title":"Integrating Xiaomi Devices"},{"body":"Introducción La documentación del grupo Arco se genera mediante una herramienta de gestion de sitios estáticos, llamada Hugo, y se mantiene en una serie de repositorios. Para añadir o editar cualquier contenido, se ha de conocer esta estructura, y se deben utilizar las herramientas adecuadas.\nEl objetivo de esta receta es proporcionar una visión global, y algunos mecanismos para llevar a cabo la tarea de documentar. Si necesitas más información, puedes visitar la página de documentación de Hugo.\nConfiguración del entorno Antes de nada, para poder llevar a cabo esta receta necesitarás:\nAcceso al repositorio de documentación Una instalación actualizada de GNU/Linux Manejo básico de la terminal Para empezar, instala los paquetes necesarios:\nconsole $ sudo apt install git hugo make inotify-tools Recuerda que la versión de Hugo que instales ha de ser \u0026gt;= 0.40, lo que puedes comprobar con el siguiente comando:\nconsole $ hugo version Para poder trabajar con la documentación de Arco, en primer lugar descárgate el repositorio:\nconsole $ git clone git@bitbucket.org:arco_group/doc.git En la raíz del repositorio encontrarás diferentes directorios, cada uno específico de una sección de la documentación. Así, por ejemplo, el directorio idm contiene la documentación del proyecto IDM, y será accesible desde la URL https://site.url/idm/. Para más información sobre esta estructura, ve a Como crear una nueva sección.\nTambién deberías ver el directorio hugo-site, que contiene la configuración del sitio, algunos ficheros estáticos comunes, plantillas genéricas, etc. Para poder generar y desplegar la documentación, es necesario sincronizar el contenido de los directorios de cada sección con el proyecto Hugo. Para ello, se proporciona un Makefile con algunas reglas:\nhugo-clean: elimina el contenido del proyecto Hugo, y reestablece la estructura mínima necesaria. hugo-sync: copia el contenido de las secciones al proyecto Hugo. keep-sync: lanza un servicio que monitoriza los ficheros de las secciones, y sincroniza el proyecto Hugo si hay algún cambio. sync: regla que ejecuta las tres anteriores. run-server: lanza el servidor de desarrollo de Hugo, que permite previsualizar el contenido generado mientras realizas cambios. Dado esto, el workflow general que deberías seguir para modificar la documentación es el siguiente:\nActualizar el repositorio: git pull Lanzar el servicio para sincronizar el proyecto Hugo: make sync En otra terminal, ejecutar el servidor de desarrollo: make run-server Abrir en el navegador la URL: http://localhost:1313 Editar, y revisar que el resultado es el esperado. Hacer un commit con todos los cambios. Cómo editar un documento existente Lo primero que debes hacer es determinar qué fichero debes editar. Podemos distinguir principalmente dos tipos de contenido: plantillas (en formato .html) y documentos en Markdown (con la extensión .md). Cada fichero .md se renderizará con una plantilla determinada.\nIgualmente, cada sección (o subsección) del sitio se renderiza con una plantilla específica. El fichero de cabecera de cada sección (el _index.md) puede especificar con qué plantilla se renderizará (con el parámetro layout). Por ejemplo, la sección idm, tiene layout: \u0026quot;home\u0026quot; en su fichero de cabecera, por lo que esa sección se renderizará con la plantilla home.html, situada en el directorio idm/layouts.\nY si lo que quieres es editar contenido específico de una entrada (no de una sección), sólo debes buscar el fichero Markdown correspondiente. Por ejemplo, el contenido de la entrada http://site.url/idm/examples/install/ lo proporciona el fichero /idm/content/examples/install.md.\nImágenes Si necesitas incluir imágenes, crea un directorio con el mismo nombre que tu entrada (pero sin extensión), dentro de [SECTION]/static/images. Por ejemplo, supongamos que estás editando el fichero recipes/content/integrating_xiaomi_devices.md. En este caso, el directorio a crear sería:\nconsole $ mkdir recipes/static/images/integrating_xiaomi_devices A continuación, incluye tus imágenes dentro de ese directorio, y haz referencia a ellas en tu contenido usando el Shortcode image, tal que así:\nsnippet.md {{\u0026lt; image \u0026#34;filename.png\u0026#34; \u0026gt;}} También puedes añadir una imagen que esté directamente en el directorio raiz, static/images, de la sección correspondiente. Para ellos, especifica en el shortcode el parámetro relative=\u0026quot;false\u0026quot;. Por ejemplo:\nsnippet.md {{\u0026lt; image src=\u0026#34;filename.png\u0026#34; relative=\u0026#34;false\u0026#34; \u0026gt;}} Código y ficheros de texto El proceso para incluir el contenido de un fichero estático (quizá un script en Python, o un fichero de configuración de Ice) es muy similar al de las imágenes. Antes de nada, crea el directorio específico de tu entrada que contendrá estos ficheros en el directorio static/code de tu sección. Debe llamarse de la misma forma que tu página (pero sin la extension .md).\nPor ejemplo, si quiero incluir un snippet en Python dentro del post idm/content/hello_world.md, crearé el directorio idm/static/code/hello_world/:\nconsole $ mkdir idm/static/code/hello_world A continuación, mete en ese directorio todo el código o ficheros estáticos que necesites. Para incluirlos en la entrada, puedes usar el Shortcode staticCode. Acepta dos parámetros: el nombre del fichero, y el lenguage (opcional, para cuestiones de marcado de sintáxis). Por ejemplo:\nsnippet.md {{\u0026lt; staticCode \u0026#34;hello_world.py\u0026#34; \u0026gt;}} Lo que debe producir un resultado similar a esto:\nhello-world.py #!/usr/bin/python3 # example of a hello world print(\u0026#34;Hello world!\u0026#34;) Si lo que necesitas es incluir in fragmento de código inline (como los que hay en esta sección), usa el Shortcode code, indicándole el lenguaje del snippet que estás incluyendo. Por ejemplo, para mostrar código en C#, sería:\nsnippet.md {{\u0026lt; code cs \u0026gt;}} class Service { private static int id = 123; public string name = \u0026#34;service-123\u0026#34;; } {{\u0026lt;/ code \u0026gt;}} Lo que mostraría el siguiente resultado:\nY si lo que quieres es mostrar un comando por la terminal, el Shortcode a usar el shell:\nsnippet.md {{\u0026lt; shell \u0026gt;}} $ cat file.txt ... {{\u0026lt;/ shell \u0026gt;}} Cómo crear una nueva sección Para crear una nueva sección, se han creado algunas herramientas específicas que se encargan de copiar las plantillas y configurar el proyecto adecuadamente.\nAntes de nada, para hacerlas disponibles, carga el fichero arco-doc.sh de la siguiente forma:\nconsole $ . arco-doc.sh A continuación, ejecuta el script ad-create-section, indicándole el nombre de la nueva sección, en minúsculas. Te pedirá más información, como el tipo de sección que estás creando, a elegir entre las siguientes:\nproject: un proyecto oficial del grupo, como \u0026ldquo;citisim\u0026rdquo; o \u0026ldquo;hesperia\u0026rdquo;. internal-project: proyectos no financiados directamente, en los que se desarrolla algún producto software horizontal. recipes: documentación genérica sobre procesos y otros usos. La herramienta también te pedirá una breve descripción, que se mostrará en la portada de la página de documentación. Un ejemplo de ejecución sería este:\nconsole $ ad-create-section arco-talks Please, provide the type of section. Choose from [project, internal-project, recipes, ...] - recipes And a brief description? (intro when finish) - Posts with information about the Arco Talks: topic, date and maybe even the slides! Creating section dirs... Populating with initial content... Adding section to automated tools... Done. - You can modify the given parameters in \u0026#39;arco-talks/content/_index.md\u0026#39; - Also, don\u0026#39;t forget to provide an icon (arco-talks/images/arco-talks-logo.png). Al finalizar, habrá creado un nuevo directorio con el nombre de la sección (arco-talks en el ejemplo), con la siguiente estructura:\nconsole $ tree arco-talks/ arco-talks/ ├── content │ └── _index.md ├── layouts │ └── partials │ └── header.html └── static ├── code └── images 5 directories, 2 files El directorio content se encargará de almacenar todo el contenido de la nueva sección, ficheros Markdown y subsecciones. En layouts se guardarán las plantillas específicas para tu contenido. Si no añades ninguna plantilla, se utilizará alguna genérica del sitio. Por último, en static estarán todos los ficheros estáticos (css, js, imágenes, código, ejemplos, etc.) de esta sección.\nAl crear la nueva sección, también se habrá modificado el Makefile para que las reglas de sincronización tengan en cuenta la nueva entrada. Si tenías el servicio de sync o el servidor de desarrollo de Hugo en marcha, necesitas reiniciarlos para aplicar los cambios. Simplemente páralos y vuelve a ejecutar make sync y make run-server. Si vuelves a abrir la página principal de la documentación, deberías poder ver tu nueva sección. Por ejemplo, algo así:\nComo ves, es necesario hacer algunos retoques. Por ejemplo, quizá quieras cambiar el nombre que se muestra aquí (parámetro title), o la ruta del icono (parámetro icon). Para ello, edita el fichero _index.md de tu sección ([SECTION]/content/_index.md). El fichero generado para el ejemplo tiene este contenido:\narco-talks/content/_index.md --- type: \u0026#34;recipes\u0026#34; title: \u0026#34;Arco-talks\u0026#34; description: \u0026#34;Posts with information about the Arco Talks: topic, date and maybe even the slides!\u0026#34; icon: \u0026#34;arco-talks/images/arco-talks-logo.png\u0026#34; --- En este ejemplo, cambiaré el título, y reemplazaré el icono por defecto con uno específico de esta sección. Quedaría algo como esto:\nSi pulsas sobre el botón \u0026ldquo;View\u0026rdquo; (es decir, si navegas con el browser a la sección que acabas de crear), verás un documento vacío, con un mensaje animándote a modificar la cabecera de tu sección. La cabecera es un fichero HTML (un partial de Hugo) que se incluirá al renderizar todas las páginas de esta sección. Se encuentra en [SECTION]/layouts/partials/header.html. El contenido será algo similar a esto:\nheader.html {{ partial \u0026#34;header.html\u0026#34; . }} This is your section ({{ .Section }}) header. Customize it. La primera línea se usa para incluir la cabecera del sitio, no la modifiques. El resto puedes cambiarlo según tus necesidades. Este es el lugar adecuado para cargar un fichero CSS específico de esta sección, añadir un título, etc. Por ejemplo, la cabecera de la sección arco-talks sería similar a esta:\narco-talks/layouts/partials/header.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;/{{ .Section }}/css/style.css\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;box\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;/{{ .Section }}/images/arco-talks-logo.png\u0026#34; style=\u0026#34;padding: 0 25px; width: 110px\u0026#34; /\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;h1 style=\u0026#34;display: inline; font-weight: bold; font-size: 2.3em; padding-top: 10px\u0026#34;\u0026gt; Arco Talks\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;hr style=\u0026#34;border: 1px solid #333333\u0026#34;\u0026gt; \u0026lt;br\u0026gt; Por otro lado, es importante notar que la plantilla por defecto para una sección recién creada será la que hay definida en el tema del site. Es decir, si no creas una plantilla nueva o se lo indicas en el Front Matter de tu sección, la plantilla que usará será hugo-site/themes/default/layouts/_default/section.html. Esta plantilla creará una lista con las páginas de tu sección, lo cual, combinado con la cabecera antes descrita, debería ser suficiente.\nSin embago, es posible que necesites más flexibilidad, o simplemente quieras crear la página de sección directamente en HTML. Para ello, será necesario que crees una plantilla específica. De entre las diversas formas de hacerlo, la más sencilla es indicar explícitamente en el fichero _index.md de la sección qué plantilla usará (parámetro layout). Por ejemplo:\narco-talks/content/_index.md --- [...] icon: \u0026#34;arco-talks/images/arco-talks-logo.png\u0026#34; layout: \u0026#34;home\u0026#34; --- Ahora deberás crear la plantilla, con el nombre indicado y extensión .html, en el directorio layouts de la sección. Es importante notar que esta plantilla no forma parte de una de nivel superior, por lo que tendrás que incluir los partials header.html, y footer.html del sitio. Puedes utilizar el siguiente fragmento como punto de partida:\narco-talks/layouts/home.html {{ partial (print .Section \u0026#34;/header.html\u0026#34;) . }} {{ .Content }} {{ range .Data.Pages }} {{ .Render \u0026#34;card\u0026#34; }} {{ end }} {{ partial \u0026#34;footer.html\u0026#34; . }} Cómo añadir una nueva entrada Hugo tiene un comando para crear nuevo contenido, y en función del tipo, copiará una plantilla u otra. Estas plantillas se llaman Archetypes, y se encuentran en el directorio hugo-site/archetypes. Así, para crear una nueva entrada, desde la raíz del repositorio, ejecuta el comando:\nconsole $ hugo new recipes/content/integrating_xiaomi_devices.md Esto creará un nuevo archivo, llamado integrating_xiaomi_devices.md dentro de la sección recipes, en el directorio de contenido (es decir en la ruta especificada). Hugo usará la primera palabra de la ruta para determinar el Archetype que empleará a modo de plantilla para la nueva entrada. Si abres el fichero recién creado, verás algo como esto:\nrecipes/content/integrating_xiaomi_devices.md --- title: \u0026#34;Integrating Xiaomi Devices\u0026#34; date: 2019-02-20T08:05:08\u0026#43;01:00 tags: - recipe draft: true # image: \u0026#34;your-recipe-image.png\u0026#34; description: \u0026#34;Some description, used in recipe summary\u0026#34; --- ** Insert your recipe here! ** Como puedes ver, es una plantilla específica de esta sección, de la cual partir para crear tu contenido, en Markdown. Si abres la página principal de la sección recipes, deberás ver una entrada como esta:\nAhora, simplemente abre ese archivo con tu editor favorito, y añade el contenido que necesites. Happy editing! :D\nCómo publicar los cambios Para publicar (o desplegar) los cambios que hagas, simplemente haz commit en el repositorio y sube los cambios al servidor. No debes preocuparte de nada más, un servicio automático se encargará de compilar y actualizar la página de documentación.\nEs muy recomendable, sin embargo, que sólo subas versiones estables, que se puedan compilar con una versión reciente de Hugo. Para verificar si hay o no errores, ejecuta make en la raíz del repositorio y comprueba los mensajes de log. Deberías obtener una salida similar a esta:\nconsole $ make make -C hugo-site all make[1]: se entra en el directorio \u0026#39;/home/oscar/repos/doc/hugo-site\u0026#39; hugo -v --theme=default INFO 2019/02/20 11:49:11 Using config file: /home/oscar/repos/doc/hugo-site/config.toml Building sites … INFO 2019/02/20 11:49:11 syncing static files to /home/oscar/repos/doc/hugo-site/public/ INFO 2019/02/20 11:49:11 found taxonomies: map[string]string{\u0026#34;tag\u0026#34;:\u0026#34;tags\u0026#34;, \u0026#34;category\u0026#34;:\u0026#34;categories\u0026#34;} | EN \u0026#43;------------------\u0026#43;----\u0026#43; Pages | 31 Paginator pages | 0 Non-page files | 0 Static files | 65 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Total in 25 ms make[1]: se sale del directorio \u0026#39;/home/oscar/repos/doc/hugo-site\u0026#39; Si no hay errores, es una versión candidata para ser publicada. Por último, ten en cuenta que en la versión pública no se incluyen los borradores, por lo que asegúrate de cambiar el parámetro draft a false en el Front Matter de tus páginas.\nCómo generar la referencia (API) de una librería en Python Se han creado algunas herramientas específicas para generar la documentación de referencia de cualquier librería en Python. Esto incluye la descripción de las clases, las propiedades y los métodos (públicos o privados).\nPara entender mejor como funciona, supongamos que tenemos una librería, llamada mypylib en el directorio /tmp/mypylib. El fichero __init__.py de ese módulo contiene lo siguiente:\nmypylib/__init__.py import sys class MyUtils: \u0026#34;\u0026#34;\u0026#34; This is a basic configurable calculator. It enables or disables the `sum` operation in yor math space. \u0026#34;\u0026#34;\u0026#34; def __init__(self, args): \u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34;\u0026#34; # private constructor, avoid documenting it self.args = args def division(self, a, b): # work in progress pass def sum(self, a, b): \u0026#34;\u0026#34;\u0026#34; Returns the addition of both arguments, if addition is a valid operation. **Parameters:** * `a`: first operand for the addition. It must be an **integer** or a **float**. * `b`: second operand for the addition, of same type that `a` param. \u0026#34;\u0026#34;\u0026#34; assert self.can_sum return a \u0026#43; b @property def can_sum(self): \u0026#34;\u0026#34;\u0026#34; *[read only]* Property to check whether the `sum` operation could be done. \u0026#34;\u0026#34;\u0026#34; return \u0026#34;sum\u0026#34; in self.args class MyTools: def print(self, msg): # work in progress pass Para generar la documentación automáticamente, primero debes cargar las herramientas de arco-doc:\nconsole $ . arco-doc.sh Una vez hecho esto, tendrás disponible la utilidad ad-gen-py-api, que acepta como argumentos una lista de los módulos que quieres documentar, junto con algunas opciones más. En particular, nos interesan estas dos:\n-c MODULE_DIR, que sirve para indicarle la ruta donde está el módulo que quieres documentar (útil en caso de que la librería no esté instalada en el sistema). --title TITLE, para añadir un título a la documentación. También es importante notar que ad-gen-py-api genera la documentación por la salida estándar, por lo que es necesario que guardes esa salida en el fichero .md correspondiente.\nAsí pues, el comando que usaríamos para documentar la librería de este ejemplo sería el siguiente:\nconsole $ ad-gen-py-api -c /tmp --title \u0026#34;A Configurable Calculator\u0026#34; mypylib \u0026gt; api/content/mypylib.md Lo que genera un documento Markdown con la referencia de la librería mypylib. Observa que la referencia se ha guardado en la sección api de la documentación. Si abres esa página con el navegador, deberías ver algo similar a esto:\nComo puedes observar, la herramienta ha incluido sólo aquellas clases o métodos que tienen documentación. Si una clase no incluye su docstring (por ejemplo, la clase MyTools), esa clase se ignorará completamente. Lo mismo sucede con los métodos, no se incluirán aquellos cuya docstring está vacía o es nula.\nPor otro lado, si la clase hereda de otra, y quieres evitar que se incluya el docstring de un método en concreto, puedes sobreescribirlo e incluir una cadena vacía (algo similar a lo que ocurre en el ejemplo con el método __init__).\nPor último, resaltar que la documentación generará un índice al principio con todas las clases documentadas, con enlaces a sus respectivas subsecciones.\n","ref":"https://arcogroup.bitbucket.io/recipes/creating_arco_docs/","summary":"En esta receta se describen los pasos necesarios para usar el \u003cb\u003erepositorio de documentación\u003c/b\u003e de Arco.","title":"Cómo crear documentación"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/docs/","summary":"","title":"docs"},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/hugo/","summary":"","title":"hugo"},{"body":"","ref":"https://arcogroup.bitbucket.io/symbiot/","summary":"Provides a set of new techniques that automates the creation of new services based on emerging situations.","title":"Symbiot"},{"body":"","ref":"https://arcogroup.bitbucket.io/idm/news/","summary":"","title":""},{"body":"","ref":"https://arcogroup.bitbucket.io/tags/event/","summary":"","title":"event"},{"body":"","ref":"https://arcogroup.bitbucket.io/idm/","summary":"Inter Domain Messaging system, a protocol for communicating devices across different technologies.","title":"IDM"},{"body":"At last, we have joined efforts and made a new site for maintaining the IDM documentation. Of course, feel free to contact us if you have any questions. Hope you enjoy it!\n","ref":"https://arcogroup.bitbucket.io/idm/news/new-documentation-site/","summary":"","title":"New IDM documentation site!"},{"body":" IDM installation Note: this post will show you how to install IDM on a Debian based machine (like Ubuntu). The following instructions were tested on Debian 9.0 (stretch) y Ubuntu 17.04 (zesty). If you use Vagrant, you can download our VagrantFile for both setups.\nSetting up Debian repositories IDM is located on ARCO\u0026rsquo;s pike repository. To install it, run the following command:\nconsole $ wget -qO- \u0026#39;http://pike.esi.uclm.es/add-pike-repo.sh\u0026#39; | sudo sh Installing ZeroC Ice for Python3 With a fresh installation of Debian 9.0 or Ubuntu 17.04, you can install ZeroC Ice for Python 3 directly:\nconsole $ sudo apt-get install python3-zeroc-ice For older versions or another systems, you can follow the official installation instructions: Ice Distributions.\nInstalling IDM Also, very straightforward:\nconsole $ sudo apt-get install idm Check that it works To check if your setup is working, download the following configuration file: router.config Router.Table.Path = router.table Router.Adapter.Endpoints = tcp -h 127.0.0.1 -p 6140 Router.Ids = 10AA01 And launch the IDM router using:\nconsole $ idm-router --Ice.Config=router.config WARNING:root: router controller not defined WARNING:root: DUO advertisements disabled INFO:root: queue Size: 10 INFO:root: router at \u0026#39;10AA01 -t -e 1.1:tcp -h 127.0.0.1 -p 6140 -t 60000\u0026#39; INFO:root: waiting events... If you see the same (or similar) result, then it works!\nUsing Vagrant The provided VagrantFile has two configured boxes, and comes bundled with a bootstrap script to install the required packages. Download it, and launch one of the included configurations (debian or ubuntu):\nconsole $ vagrant up debian When finished, you can enter the box and check if IDM is correctly installed (using the above instructions):\nconsole $ vagrant ssh debian [...] ","ref":"https://arcogroup.bitbucket.io/idm/examples/install/","summary":"","title":"How-to install IDM"},{"body":"","ref":"https://arcogroup.bitbucket.io/idm/examples/","summary":"Getting started examples of using IDM.","title":"IDM examples"},{"body":"","ref":"https://arcogroup.bitbucket.io/citisim/","summary":"Design and implementation of a new generation platform for the Smart City ecosystem.","title":"Citisim"},{"body":"","ref":"https://arcogroup.bitbucket.io/icec/","summary":"IceP compatible implementation of an object oriented middleware for contrainded devices (in C).","title":"IceC"},{"body":"","ref":"https://arcogroup.bitbucket.io/hesperia/","summary":"Homeland Security: tecnologías para la seguridad integral en espacios públicos e infraestructuras.","title":"Hesperia"},{"body":"¡Bienvenido a ARCO! Si estas viendo esta información ya has contactado con alguien de nuestro grupo y te ha pasado este enlace. Esa persona será la que te sirva de guía, pueda resolver tus dudas y será tu mentor. El objetivo de esta página es que todos ahorremos tiempo y puedas ser productivo lo antes posible.\nPuedes revisar las secciones en el orden que quieras, pero te recomendamos empezar por el principio. Y si tienes alguna duda, ¡pregunta! . Estamos disponibles en los laboratorios / despachos del grupo, así como en los diferentes canales de Slack. ¡Happy hacking!\nLa mayoría de los equipos de desarrollo y producción del grupo ARCO son máquinas con GNU/Linux y es donde más comodo nos sentimos, si no estás familiarizado con este entorno será una de tus primeras cosas que deberías aprender. Hay muchos recursos en la web para aprender este tipo de entornos, ponemos algunos para que te hagas una idea de las herramientas que usamos:\nEntre las distribuciones que mas nos gustan están Debian y Ubuntu, en este enlace puedes encontrar una buena introducción con ejercicios para que te sientas cómodo gestionando paquetes en estas distribuciones (secciones 8.1.1 y 8.1.2). Esa documentación precisamente pertenece a otra distribución (KALI) por lo que la parte de los repositorios está centrada en esa distribución, aunque vale como concepto, los repositorios de debian y ubuntu son diferentes. Asegurate de sentirte cómodo instalando/desinstalando paquetes, manejando la consola de comandos, gestionando usuarios de forma básica, moviendote en los directorios/archivos, los permisos de usuarios/archivos, etc.\nTodo lo que generes con nosotros debe estar bajo un sistema de control de versiones (actualimente git). Si no estas familiarizado con git y quieres ver de forma simple el flujo de trabajo, [aquí puedes verlo] (https://rogerdudler.github.io/git-guide/).\nCreamos repositorios en bitbucket y github, Crea una cuenta en esas web con tu dirección de correo UCLm (@uclm.es o @alu.uclm.es).\nEl usuario que te asignemos será, por defecto, una máquina GNU/Linux. Tu mentor se ocupará de crearte un usuario en esa máquina.\nDejamos libre elección en la herramienta de edición de los archivos, si no tienes ninguna preferida, Visual Studio Code es una buena elección.\nUsamos slack para discusiones/coordinarnos. Crea una cuenta en Slack con tu dirección de correo UCLM (@uclm.es o @alu.uclm.es.\nEscribe un correo al responsable de sistemas (David.Villa@uclm.es) indicando:\nTus usuarios en bitbucket, github y slack. Las cuentas de correo que has usado para crear esas cuentas (aunque aconsejamos usar la de la UCLM). Quién es tu mentor y ponle en copia del correo. En qué proyecto vas a trabajar. Aunque hay variaciones por distintos motivos (normalmente el proyecto específico), intentamos seguir unas pautas a la hora de codificar, estructurar los repositorios y generar documentación. Salvo indicación contraria del director del proyecto:\nPython es uno de nuestros lenguajes favoritos, si te integras en un proyecto python seguramente ya tiene un repositorio, sigue trabajando con esa estructura. Si vas a crear un módulo esta es una estructura clásica de un proyecto python que te puede servir como referencia. Como guía de estilo de programación, y siempre usando el sentido común, una buena guía es Pep 8\nNunca subas repositorio un fichero generado a partir de otro (binarios, .o , ejecutables, pngs, etc.). Sobre todo al principio ten especial cuidado con esto.\nLo que subas al repositorio, al menos debe compilar.\nHaz commits/push frecuentes (al menos uno diario).\nPara escribir recetas concretas sobre procedimientos de configuración, herramientas, documentación técnica, etc sigue estas instrucciones\nOtros procedimientos muy importantes a tener en cuenta:\nTu mentor debe dar tu nombre completo al conserje para que puedas recoger la llave del laboratorio. No te olvides de cerrar la puerta del laboratorio con llave siempre que se quede vacío. Ten tu puesto de trabajo limpio y ordenado. Usa auriculares (con un volumen razonable) si te pones música, para no molestar a tus compañeros. Si necesitas algún tipo de equipamiento adicional, habla con tu mentor. No cojas nada de otro equipo/proyecto, ni siquiera temporalmente, salvo que tu mentor te de permiso. Consulta el manual de procedimientos del grupo para información más detallada.\n","ref":"https://arcogroup.bitbucket.io/arco101/","summary":"Getting started documentation for \u003cb\u003enewcomers\u003c/b\u003e to the ARCO Research Group!","title":"ARCO 101"},{"body":"Introduction CittaVR for Unity is a Unity framework that allows you to create virtual twins for real environments, to manipulate or sense the state and status of your sensors and actuators.\nIt is available for (and compatible with) both Linux and Windows platforms, but this manual focuses only on the Unity Editor for GNU/Linux systems. On the other hand, as always with Unity, you can create your models using the same editor (with ProBuilder), or an external tool, like Blender.\nLet\u0026rsquo;s begin with the installation.\nInstallation of Unity on Linux Note: To download the most recent version of the Editor (or any older one), you should use the Unity Hub, which could be found here:\nDownload Unity Mark it as executable and run it (you could, of course, do it from the GUI):\nconsole chmod a\u0026#43;x UnityHub.AppImage ./UnityHub.AppImage Once you start the Hub, you could go to the Installs tab and add the version you prefer:\nUnity Hub: adding a Unity version\nAlso, you will need an external tool for editing the source code (in C#). You may use whatever you want (like Emacs or Atom), but the recommended IDE (and what has better integration) is Visual Studio Code. You can just grab the .deb package and install it like any other Debian package.\nNow, you are ready to start a new Unity project and add support for CittaVR to it. Let\u0026rsquo;s see how.\nCreating a new project This step is very straightforward: just open the Unity Hub (if you closed it) and go to the Projects tab. Click on the New button. Select the project type, its location and also the name:\nUnity Hub: creating a new project\nPress on Create and wait. It will take some time to compile the minimum assets included by default on every new project. Just wait until it finishes and shows the Unity Editor with your newly created project.\nSetting .NET framework First thing you should do is to change your project settings, because by default, it uses .NET Standard 2.0, which is not yet fully compatible with ZeroC Ice and other libraries.\nSo, go to Edit \u0026gt; Project Settings \u0026gt; Player. This will open a new inspector window with some of your project settings. Open the section called Other Settings and find the API Compatibility Level, as seen in the following picture:\nUnity: change the .NET version\nSet it to .NET 4.x and close the inspector. It will compile again all your assets using the new framework version, so please wait.\nOpen C# project Now is a good time to check your IDE integration (if you want to use an IDE, anyway). To configure the recommended settings, got to Edit \u0026gt; Preferences. In the tab External Tools, change the property External Script Editor to \u0026ldquo;Visual Studio Code Insiders\u0026rdquo; (click on Browse if needed). You can see this dialog in the following screenshot:\nUnity: external source editor setting\nAnd to open the C# project, just right click on an empty space of your Aassets folder, and click on Open C# Project:\nUnity: open C# project with your IDE\nIf you now see VS Code open, and loaded with your Unity project, well done! You are ready for rock \u0026rsquo;n roll! ;)\nSetup of IDM router CittaVR may use IDM as its internal middleware for communicating with external objects and to receive incoming invocations. So, if you want to enable the IDM support, first thing you should do is to setup an IDM Router (in case you don\u0026rsquo;t have already one). Don\u0026rsquo;t worry, its pretty easy. Just make sure you installed the IDM Debian package (or do it now!) and create a new configuration file for it (call it router.config). I usually put it on the Assets folder:\nVS Code: create the IDM router config file\nNext, add the following properties (and adjust them to suit your needs):\nrouter.config Router.Adapter.Endpoints = tcp -h 127.0.0.1 -p 6140 Router.Table.Path = router.table Router.Ids = 0A01C17400000001 To launch the router, just use this configuration file:\nconsole idm-router --Ice.Config=router.config IDM: running the router with your settings\nAdd CittaVR config file CittaVR uses a configuration file to store some of its settings (in particular, where is the IDM router you want to use, if any). So, let\u0026rsquo;s create a CittaVR configuration file. The file must be located in the Assets/StreamingAssets folder (we use this location in order to ensure that Unity will bundle them when building), and be called cittavr.config. You may specify the following properties:\nIDM.Router.Proxy, which is clearly the proxy of your IDM router. If you used the previous configuration file, this value would be 0A01C17400000001 -t -e 1.0:tcp -h 127.0.0.1 -p 6140. Note: if you are contacting with IceC devices, remember that you must add -e 1.0 here.\nCittaVR.Adapter.Endpoints, as CittaVR will expose virtual objects to the real world, you will need to specify the endpoints where they should be contacted. These virtual objects will be registered on the given IDM router, so these endpoints must be reachable by the router.\nCittaVR.Id, as we will see later, CittaVR provides an object to dynamically instantiate new objects. This is the identity of this object. You should put here something in the same domain as your router (in this example, something like 0A01C17400000001), and make sure that there are enough free addresses starting from this one, as the new dynamic objects will be using them.\nThe following is a full configuration file, which uses the previously defined router:\ncittavr.config IDM.Router.Proxy = 0A01C17400000001 -t -e 1.0:tcp -h 127.0.0.1 -p 6140 CittaVR.Adapter.Endpoints = tcp -h 127.0.0.1 -p 9001 CittaVR.Id = 0A01C17400000002 Installing ZeroC Ice for Unity As you may already know, CittaVR uses ZeroC Ice, so before anything else, you must install it. ZeroC provides its libraries and tools for C# using the NuGet repositories, and there is a plugin to add support for NuGet to Unity, so this is what you need to install.\nFirst, download the plugin in the following site (is an asset in the form of .unitypackage):\nNuGetForUnity github releases In my case, I downloaded the 2.0 version. Then, you can import it: right click on Assets folder and select Import Package \u0026gt; Custom Package...:\nUnity: import custom package\nSelect the file you downloaded and click on Open. It will show you a window with the assets to import. Select all and press Import. It may take a while, so be patient. Once finished, you should see a new menu entry:\nUnity: NuGet correctly installed and compiled\nOk. Go to NuGet \u0026gt; Manage NuGet Packages. It will open a new tab that allows you to install new packages. It may open as a floating window, so feel free to drag it where the other tabs are, to expand it. Now, press on Search and find the package called zeroc.ice.net:\nNuGet: installing ZeroC package\nPress on install and let it finish. When done, you are ready to include the CittaVR packages!\nAdding CittaVR assets Until now, we only set up the environment to work with CittaVR, but we didn\u0026rsquo;t install it. Now is the time. CittaVR has many components: a Debian package with scripts to communicate with the internal manager to instantiate new objects and two Unity packages, one for the core and another with common assets.\nThe Debian package could be installed using the Pike repository (visit the link for instructions), just like any other package:\nconsole sudo apt install cittavr-unity The Unity packages could be downloaded from CittaVR repository, under the Downloads section. For convenience, I put here the links to the latest versions, just click to download and save them.\nCittaVR Core - Unity package CittaVR Assets - Unity package Now, you can import both packages into your project, using the same procedure as before, ot just drag and drop the package inside the Assets folder of your Unity Editor. Add first the CittaVR Core package:\nUnity: installing CittaVR Core\nOn drop, it will show a new window to select what components you want to include. Confirm that all is selected and press on import. Repeat the same process to include the package \u0026lsquo;CittaVR Assets\u0026rsquo; you downloaded earlier.\nThe final step to conclude your setup is adding to your scene an instance of the CittaVRApp prefab, which is needed to initialize all the internal runtime of CittaVR. Go to Assets \u0026gt; CittaVR folder inside your project tab on the Unity Editor. There you should find a prefab called CittaVRApp (the blue 3D box), drag it and drop into your Hierarchy tab (and save the scene!).\nUnity: adding the CittaVR prefab\nNow, you can press play on Unity, and (if everything went ok), your CittaVR manager should have registered correctly on the given IDM router. You may see messages like these:\nIDM: CittaVR manager advertisement\nUnity: starting messages of CittaVR\nPress stop, we are going to add some objects to the scene!\nStatic instantiating In the folder Assets \u0026gt; Resources \u0026gt; CittaVR you will find some more prefabs. These are the objects that come bundled with CittaVR. Of course, you are not limited to use only these, later we will see how to create new ones, but as a starting point they will do.\nDepending on the version you downloaded, there will be more or less available assets. As of writing this document, there were: a street lamp, a traffic lights and a proximity sensor. Let\u0026rsquo;s add the street lamp first.\nThe process is exactly the same used to add the CittaVRApp: just grab the StreetLamp prefab and put it on your scene, wherever you want. After that, you may see something like this:\nUnity: adding a new StreetLamp asset\nWell, OK. Maybe I\u0026rsquo;ve changed some little things (I\u0026rsquo;ve added a plane as a floor, reduced the light intensity and switched on the Street Lamp), but you get the idea, there is a street lamp on my scene, as should be in yours! Now, you can press play again and your street lamp will be ready for action. You may see a warning message like this:\nUnity: running without IDM address\nThe reason is that you didn\u0026rsquo;t set a valid IDM address for this object. In order to be reachable, or to know the source of a virtual event, every object in CittaVR should have an IDM address. If you don\u0026rsquo;t set it, CittaVR will automatically choose one from the address pool (in this case, the next free address: 0A01C17400000003. If you want to set it, go to the object inspector (right side in the above picture), and find the Street Lamp (Script) component. There, you should see two properties:\nIDM Address, which is the field we need to change, the address of this object for everyone else (including the outside world!) Citisim ID, this is an internal identifier, used for dynamic instantiating, don\u0026rsquo;t to worry about it now. So, to change the IDM Address field, add a valid address, in the same domain that your router, like 0A01C17400101234. Then, if you press play again, you will get rid of that warning. One way or the other, the object should have been advertised on your IDM router, like this:\nIDM: advertisement of the new Street Lamp\nNow, you can change its status from inside and outside Unity. From inside, just click on the street lamp head, and it should toggle the lamp status. To change it from outside, you must know that this lamp implements the Smart Transducer interface st::IBool, so you need a client that uses this interface. For instance, the Debian package smart-transducer provides a command line tool that could be used for this purpose. To reach the object directly, use the proxy that you saw on the advertisement:\nconsole st-client -t bool -p \u0026#34;0A01C17400101234 -t:tcp -h 127.0.0.1 -p 9001\u0026#34; 1 st-client -t bool -p \u0026#34;0A01C17400101234 -t:tcp -h 127.0.0.1 -p 9001\u0026#34; 0 But, if you want to use IDM (which is, by the way, the proper method), you will use the router\u0026rsquo;s endpoints and the object\u0026rsquo;s address, like this:\nconsole st-client -t bool -p \u0026#34;0A01C17400101234 -t:tcp -h 127.0.0.1 -p 6140\u0026#34; 1 st-client -t bool -p \u0026#34;0A01C17400101234 -t:tcp -h 127.0.0.1 -p 6140\u0026#34; 0 In this case, you will also see how the router forwards these messages to the proper destination:\nIDM: forwarding messages to the lamp\nWhat is more, this object also implements the interface st::Linkable, so you can set another object as its observer (or link), and when the lamp changes its state, it will notify the new state to this observer. Also, this lamp could be the observer of some other object. Let\u0026rsquo;s do it!\nDynamic instantation We will use one of the other prefabs that come with CittaVR Assets: the proximity sensor. The scene could be the following:\nSo, we need a proximity sensor that detects a new car, and then switches on the street lamp. This sensor implements the interface st::Linkable, and sends events using st::IBool, which is the same interface implemented by the StreetLamp object. We can just connect one to the other.\nIf you go to the Assets \u0026gt; Resources \u0026gt; CittaVR, there should be a ProximitySensor prefab. You could just add it to your scene, as we did earlier, but we want to do it differently, in a dynamic way (i.e: having the whole scene running).\nNote: To achieve this, first thing you need to do is to press play on the Unity editor, as we need the CittaVRApp running. Then, open a terminal window, and use the cittavr companion tool to add the asset. You need to specify your project folder (the Unity project root folder), the name of the asset and the position in the scene to place it. For instance:\nconsole cittavr . --add-asset CittaVR/ProximitySensor --pos 0 3.5 0 90 Note: Of course, you can use the supplied Ice interface directly from your program to achieve the same result. In any case, the proximity sensor should be displayed like a red box (you will see it in the game tab and also in the scene editor tab):\nProximity Sensor: normal appearance\nYou may don\u0026rsquo;t want to see this red box, so just uncheck the Mesh Renderer option of the proximity sensor prefab (remember, if you want to make this change permanent, add the prefab to the scene statically):\nProximity Sensor: disabling the red box\nNow, when some object collides with this one, it will emit a message to its observer (if no observer is set, nothing will be done). Note that the colliding object should be a Rigid Body, and also have some type of collider on it. So its time to add the object that will activate this sensor, just to test if everything works. Press Stop on your player, and add a cube or something that will collide with your sensor. Now, you can press play, and add the proximity sensor again, using the cittavr tool.\nBut, if you expect the light to switch on, you will get disappointed! :) Why? Because we didn\u0026rsquo;t connect both objects (yet). Its a simple step. Open a new terminal window, and use a tool called st-link-to, available on the smart-transducer Debian package. It needs two parameters: first, the proxy of your observable object (which is the source of the events), and second, the IDM address of the observer (that is the object that will receive the events). In this example, the observable object is the proximity sensor (IDM: 0A01C17400000003) and the observer is your street lamp (IDM: 0A01C17400101234).\nSo, to connect both elements using the router endpoints, run the following command:\nconsole st-link-to \u0026#39;0A01C17400000003 -t:tcp -h 127.0.0.1 -p 6140\u0026#39; 0A01C17400101234 Now, when the sensor detects some collision, it will invoke the street lamp, which will power on. Later, if the sensor stops detecting the collider, it will also notify the lamp, but with a delay of 8 seconds (which is the default value of the prefab).\nTest: the yellow cube just collided with the sensor\nThe next big thing to achieve is the task of creating new CittaVR compliant assets. This will be analyzed in the following section.\nCreating a new CittaVR Asset A CittaVR asset is usually a prefab which provides the mesh, materials, textures, scripts and other components in a single unit. Thus, you just need to add it to your scene, and maybe change some settings.\nIn this section, we will create a new asset, step by step. It will be a level crossing, and will provide an interface to set its state (up or down).\nModel in Blender Well, this part is very \u0026lsquo;simple\u0026rsquo;: just open Blender and create the model you wish. Take note that you will import it into Unity, so you must know the limitations and caveats it has (which will not be enumerated here). If your model is simple, you won\u0026rsquo;t have any problem. Even some modifiers work out of the box, like basic textures do (the UV mappings are also imported correctly). Specific shaders or node materials are not supported, you must create them inside Unity.\nNote: For this example, I\u0026rsquo;ve created a simple barrier, like this:\nBlender: a basic train barrier\nCreating the prefab Add your model to the assets folder of your Unity project, and then to your scene, to check if everything is correct. Adjust your model materials, and textures, and add whatever component you need.\nNow, to create your prefab, just drag the model you added in your scene (which you have modified, adding components, etc.), drop it on your assets folder and select Original Prefab:\nUnity: populating the prefab with your instance\nNow, you have an object which will be the template used to instantiate new objects. Note that you still need to preserve the original model imported from blender, and also the textures and associated scripts.\nCompanion C# script Up to now, your asset is not integrated into CittaVR in any way. So, create a new empty C# script, and open it with your editor.\nFirst of all, you will need to create a new class, that inherits from CittaVR.AssetManager (which is a subclass of MonoBehaviour ). Among other tasks, this class will be used to instantiate your Ice servant, so you must implement a method called create_servant, which has the following signature:\nsnippet.cs public abstract Ice.Object create_servant( GameObject obj, ConcurrentQueue\u0026lt;GUITask\u0026gt; task_queue, Ice.ObjectPrx router); The given parameters are:\nGameObject obj: the actual GameObject to which this script is attached to. ConcurrentQueue\u0026lt;GUITask\u0026gt; task_queue: this is a queue of GUITask, used to execute drawing functions on the GUI thread. If your servant wants to modify the scene on runtime, this is the way to achieve it. Ice.ObjectPrx router: the IDM router that you have to use. So, on this method, you should create and return an instance of your servant. For example, this asset will use the st::IBool interface, so I create my servant as this:\nsnippet.cs class LevelCrossingServant : st.IBoolDisp_ { private static ILogger logger = Debug.unityLogger; private GameObject _obj; private ConcurrentQueue\u0026lt;GUITask\u0026gt; _task_queue; private Ice.ObjectPrx _router; private string _observer; private string _source; private Transform _barrier; private Quaternion _originalRot; public LevelCrossingServant( GameObject obj, ConcurrentQueue\u0026lt;GUITask\u0026gt; task_queue, Ice.ObjectPrx router, string source) { this._obj = obj; this._task_queue = task_queue; this._router = router; this._source = source; this._barrier = _obj.transform.GetChild(0); this._originalRot = _barrier.rotation; } public override void set(bool value, string source, Current current=null) { // enqueue UI update _task_queue.Enqueue((mb) =\u0026gt; { _barrier.rotation = _originalRot * Quaternion.AngleAxis(value ? 90: 0, Vector3. up); }); } } Here, I\u0026rsquo;ve implemented the set() method. What it does is to enqueue an operation of update inside the GUI thread, to just rotate the barrier to one side or the other.\nWith this class, I\u0026rsquo;ve implemented the create_servant() method as follows:\nsnippet.cs public class LevelCrossing : CittaVR.AssetManager { private LevelCrossingServant _servant; public override Ice.Object create_servant( GameObject obj, ConcurrentQueue\u0026lt;GUITask\u0026gt; task_queue, Ice.ObjectPrx router) { _servant = new LevelCrossingServant( obj, task_queue, router, IDMAddress); return _servant; } } Now, of course, don\u0026rsquo;t forget to add this script as a component of your prefab. And, if you go to the inspector tab, you will see some familiar properties on this script: the IDM Address and the less used Citisim ID.\nTo test the script, create an instance of your prefab, set the IDM address, and press play. You should see the advertisement of this object in the router output, and also should be able to change its state using the st-client again.\nExporting your asset If you want to use this asset on another project, you should export it as a Unity Package. There are two ways of doing this: using the contextual menu of the editor, or by command line. I prefer the later, because you can put it inside a Makefile, and generate the package very easily, but this is a matter of preference.\nAnyway, if you want to export your asset, I recommend you to make an specific folder for it, and then put every dependency that you want to include in your package inside that folder. In our example, it should be like this:\nUnity: folders for our asset\nThen, to export it using the editor, just click over that folder, and select Export Package. Unity automatically will select all dependencies of this asset, which includes CittaVR and many other things that you don\u0026rsquo;t want to provide, so in the window that appears, uncheck the box Include dependencies (at the bottom):\nUnity: on export, don\u0026#39;t include dependencies\nClick on Export and save the package wherever you want. It\u0026rsquo;s also a good idea to create a new Unity project and check that everything worked fine.\nIf you want to do this process but from the command line interface, open a terminal and go to your project folder. If you are using the new unity-hub (like me), then you may need to create an alias for the binary unity to the location you specified on setup. For example, I have all the unity installs in the folder ~/use/share/unity, so in my example, I will create the following alias:\nconsole alias unity=\u0026#39;~/usr/share/unity/2019.3.0f6/Editor/Unity\u0026#39; Then, to create the package, execute the following command:\nNote: console unity -quit -nographics -batchmode \\ -projectPath \u0026#34;$(pwd)\u0026#34; \\ -exportPackage Assets/LevelCrossing \\ \u0026#34;$(pwd)/cittavr-level-crossing.unitypackage\u0026#34; Compiling Slices with Unity In many cases, you will need to use your own Slice interfaces, to provide a custom servant or to implement two or more interfaces in the same class. Then, you will need to compile the slices to C#, using the Slice translators. Again, you have two options: install the Ice 3.7 compilers on your system, and compile directly from the command line interface, or use the Slice4Unity package. Here we will see the later method.\nSo, go ahead and download the Slice4Unity package:\nSlice4Unity package from bitbucket And add it to your project, as any other asset.\nNote: After the installation, you can now select any .ice file, and the inspector will show you a button to compile the slice, like this:\nSlice4Unity: inspecting an slice file\nWhen you press the button, it will create a folder called Generated with the generated code for that slice interface. Nowadays, there is no other option, like support for including other directories, but I\u0026rsquo;m working on it :D\n","ref":"https://arcogroup.bitbucket.io/cittavr/","summary":"CittaVR for Unity is a framework that allows you to create \u003cb\u003evirtual twins\u003c/b\u003e for real environments","title":"CittaVR"},{"body":"","ref":"https://arcogroup.bitbucket.io/icec/examples/","summary":"Getting started examples of using IceC.","title":"IceC examples"},{"body":"Manual de usuario del espejo inteligente Despliegue e instalación Servicios Programación ","ref":"https://arcogroup.bitbucket.io/miratar/","summary":"Predictive, personalized, preventive, and participatory (4P) management of frailty and multimorbidity for the digital transition of the care economy","title":"Miratar"},{"body":"","ref":"https://arcogroup.bitbucket.io/arco-talks/","summary":"Posts with information about the Arco Talks: topic, date and maybe even the slides!","title":"Talks"}]