

<!DOCTYPE html>
<html>

<head>
  <title>Arco Research &amp; Documentation</title>
  <meta charset='utf-8'>

  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/font-awesome.css">
  <link rel="stylesheet" href="/css/highlight-default.css">
  <link rel="stylesheet" href="/css/style.css">

  <link rel="icon" type="image/png"  href="/images/icon.png">
</head>

<body>
  <div id="content" class="container" style="padding-top: 15px">


<link rel="stylesheet" href="/shapes/css/style.css">

<div class="box">
  <a href="/">
    <img src="/shapes/images/shapes-logo.png"
         style="padding: 0 25px" />
  </a>
  <h1 style="display: inline; font-weight: bold; font-size: 2.3em; padding-top: 10px">
    <a href="/shapes" style="color: inherit">Shapes</a>
  </h1>
</div>

<hr style="border: 1px solid #333333">


<h1>Speech Recognition with DeepSpeech</h1>
<nav id="TableOfContents">
  <ul>
    <li><a href="#overview">Overview</a></li>
    <li><a href="#ingredients">Ingredients</a></li>
    <li><a href="#setup">Setup</a></li>
    <li><a href="#set-the-commands">Set the commands</a></li>
    <li><a href="#usage-to-train-a-model">Usage to train a model</a></li>
    <li><a href="#making-a-mmap-able-model">Making a mmap-able model</a></li>
    <li><a href="#run-the-model">Run the model</a></li>
  </ul>
</nav>

<div class="recipe-content">
  <h2 id="overview">Overview</h2>
<p>DeepSpeech is a speech recognition engine developed by Mozilla that uses a model trained by machine learning techniques. This recipe will explain how to make use of this tool for speech recognition.</p>
<h2 id="ingredients">Ingredients</h2>
<ul>
<li>Linux environment (tested on Ubuntu 20.04 LTS x64)</li>
<li>APT dependencies: docker (&gt;= 20.10.2), python3 (&gt;= 3.7), venv</li>
<li>The speech recognition repository available in <a href="https://bitbucket.org/cristibp11/shapes-mm-speechrecognition/src/master/">this link</a></li>
</ul>
<h2 id="setup">Setup</h2>
<p>First, clone the repository provided in the <strong>Ingredients</strong> section. Once cloned you must copy the audios, in .wav format, that you want to convert from speech to text into a folder called <strong>original</strong>. In the repository you can find some sample audios in the <strong>Downloads</strong> section.</p>
<div class="app-skin shell">
  <div class="filename">console</div>
  
  <pre class=shell>user@pc:~/$ tree original
original
└── es
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_1_001.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_1_002.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_1_003.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_1_004.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_2_001.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_2_002.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_2_003.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_3_001.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_3_002.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_3_003.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_4_001.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_4_002.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_4_003.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_5_001.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_5_002.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_6_001.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_6_002.wav
    ├── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_7_001.wav
    └── shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_7_002.wav

1 directory, 19 files</pre>
</div>
<h2 id="set-the-commands">Set the commands</h2>
<p>In the name of the audios an id is specified. This id has to match the content of the <code>commands.csv</code> file, which specifies the audio id and the audio transcript. With the example audios our <code>commands.csv</code> file would look like this:
<div class="app-skin shell">
  <div class="filename">console</div>
  
  <pre class=shell>user@pc:~/$ cat commands.csv
id,es
1,ok espejo
2,hola espejo
3,espejo
4,vale espejo
5,llama a mi hija
6,espejo llama a mi hijo
7,espejo llama a mi nuera</pre>
</div></p>
<h2 id="usage-to-train-a-model">Usage to train a model</h2>
<p>In order to install all dependencies, you must execute from the repository root directory <code>make dependencies</code>. Then, execute <code>make volumes</code> and finally <code>make</code>, which executes <code>build</code> and <code>run</code> targets. The audio will be processed and stored in a new folder calles <strong>data</strong> (this folder will be copied into a docker volume to be used by the container), and will be generated as <code>data/train.csv</code>, <code>registry.csv</code>, <code>Dockerfile</code>, and so on. Also, then container will start running detached. For checking its status execute <code>make log</code>.</p>
<p>If you conciusly want to stop the container, use <code>make stop</code>. This stopping the container and stores last finished DeepSpeech epoch so, if you start again the container continues from there. If you want to restart the container just execute <code>make run</code>.</p>
<p>In case the training of the model has finished and you want to restart it anyway, change in the file <code>configuration.env</code> the variables <code>CURRENT_EPOCH</code> to 0 or <code>MAX_EPOCH</code> to a value greater than the current one.</p>
<p>If you manually added or changed something keep in mind that DeepSpeech could no longer work, as it tries to restore from a checkpoint that has invalid values. To start a new training, rebuild the trainer image executing <code>make rebuild NEW_DATA=yes LOAD_FROM_CHECKPOINT=no</code>. This copies the new and modified data contents to docker volume and tells DeepSpeech to not load from any checkpoint, so there is no errors.</p>
<h2 id="making-a-mmap-able-model">Making a mmap-able model</h2>
<p>The <code>shapes-mm-speechrecognition.pb</code> model file generated in the above step will be loaded in memory to be dealt with when running inference. This will results in extra loading time and memory consumption. One way to avoid this is to directly read data from the disk.</p>
<p>Tensorflow has tooling to achieve this, so clone DeepSpeech repository with the command <code>git clone --branch v0.9.3 https://github.com/mozilla/DeepSpeech</code>. To use the tools in this repository you must have python version 3.6. To do this you can create a virtual environment with that version with the command <code>python3 -m virtualenv -p /usr/bin/python3.6 &lt;name_virtual_environment&gt;</code>. Then use <code>util/taskcluster.py</code> tool to download:
<div class="app-skin shell">
  <div class="filename">console</div>
  
  <pre class=shell>user@user-pc:~$ python3 util/taskcluster.py --source tensorflow --artifact convert_graphdef_memmapped_format --branch r1.15 --target .</pre>
</div></p>
<p>Then, from the directory where you will use the model execute the next command:
<div class="app-skin shell">
  <div class="filename">console</div>
  
  <pre class=shell>user@user-pc:~$ ./convert_graphdef_memmapped_format --in_graph=/var/lib/docker/volumes/ds_model/_data/shapes-mm-speechrecognition.pb --out_graph=shapes-mm-speechrecognition.pbmm</pre>
</div></p>
<h2 id="run-the-model">Run the model</h2>
<p>First, you will have to install the DeepSpeech wheel. To perform the installation, just use pip3 as such <code>pip3 install deepspeech</code>. After installation has finished, you should be able to call deepspeech from the next command-line:</p>
<div class="app-skin shell">
  <div class="filename">console</div>
  
  <pre class=shell>user@user-pc:~$ deepspeech --model shapes-mm-speechrecognition.pbmm --audio data/es/&lt;name_file_audio&gt;</pre>
</div>
<p>The output of this command with one of the example audios would look like this:
<div class="app-skin shell">
  <div class="filename">console</div>
  
  <pre class=shell>user@user-pc:~$ deepspeech --model shapes-mm-speechrecognition.pbmm --audio data/es/shapes_06d3eec8a7f1add24c69dcf04125c06b_2_3_000000_1_001.wav
Loading model from file shapes-mm-speechrecognition.pbmm
TensorFlow: v2.3.0-6-g23ad988
DeepSpeech: v0.9.3-0-gf2e9c85
2021-04-28 09:32:21.749436: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loaded model in 0.0949s.
Running inference.
ok espejo
Inference took 1.570s for 2.400s audio file.</pre>
</div></p>

</div>



    </div> 

    <footer class="footer">
      <div class="container text-center">
        <a href="http://www.arcoresearch.com">ARCO Research Group</a> |
        <a href="/">Docs</a>&nbsp; - &nbsp;made with
        <a href="https://gohugo.io">Hugo</a> ·
        <a href="http://getbootstrap.com">Bootstrap</a>
      </div>
    </footer>

    <script src="/js/jquery.min.js"></script>
    <script src="/js/bootstrap.bundle.min.js"></script>
    <script src="/js/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>


